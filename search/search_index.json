{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TAP Reference Service Packages \u00b6 This repository is a collection of reference packages for Kubernetes, aimed at demonstrating how to consume public cloud services and expose the generated secrets to applications via service bindings. They are fully compatible with Services Toolkit for VMware Tanzu Application Platform . Either Carvel packages or Crosspplane configurations have been used, in order to provide examples for both methods. Important Such packages are intended to be for reference only and haven't been tested nor are they supported for production use.","title":"HOME"},{"location":"#tap-reference-service-packages","text":"This repository is a collection of reference packages for Kubernetes, aimed at demonstrating how to consume public cloud services and expose the generated secrets to applications via service bindings. They are fully compatible with Services Toolkit for VMware Tanzu Application Platform . Either Carvel packages or Crosspplane configurations have been used, in order to provide examples for both methods. Important Such packages are intended to be for reference only and haven't been tested nor are they supported for production use.","title":"TAP Reference Service Packages"},{"location":"crossplane/","text":"Install Crossplane via Upbound CLI \u00b6 Download the up cli curl -sL \"https://cli.upbound.io\" | sh sudo mv up /usr/local/bin/ Check the installed version: up --version Switch to the proper Kubernetes context and run the following command in order to install Upbound Universal Crossplane (UXP): up uxp install Verify all UXP pods are Running with kubectl get pods -n upbound-system. This may take up to five minutes depending on your Kubernetes cluster. $ k get pods -n upbound-system NAME READY STATUS RESTARTS AGE crossplane-65444df64-7wcb2 1 /1 Running 0 92s crossplane-rbac-manager-69498f955b-2npkl 1 /1 Running 0 92s upbound-bootstrapper-5c9864b546-lngkw 1 /1 Running 0 92s xgql-6485cf5748-src2w 1 /1 Running 3 ( 70s ago ) 92s Note RESTARTS for the xgql pod are normal during initial installation.","title":"Install Crossplane"},{"location":"crossplane/#install-crossplane-via-upbound-cli","text":"Download the up cli curl -sL \"https://cli.upbound.io\" | sh sudo mv up /usr/local/bin/ Check the installed version: up --version Switch to the proper Kubernetes context and run the following command in order to install Upbound Universal Crossplane (UXP): up uxp install Verify all UXP pods are Running with kubectl get pods -n upbound-system. This may take up to five minutes depending on your Kubernetes cluster. $ k get pods -n upbound-system NAME READY STATUS RESTARTS AGE crossplane-65444df64-7wcb2 1 /1 Running 0 92s crossplane-rbac-manager-69498f955b-2npkl 1 /1 Running 0 92s upbound-bootstrapper-5c9864b546-lngkw 1 /1 Running 0 92s xgql-6485cf5748-src2w 1 /1 Running 3 ( 70s ago ) 92s Note RESTARTS for the xgql pod are normal during initial installation.","title":"Install Crossplane via Upbound CLI"},{"location":"crossplane/providers/aws/","text":"Upbound Universal Crossplane (UXP) AWS provider is a provider for Amazon Web Services developed and supported by Upbound. It can be deployed on top of a Kubernetes cluster with Crossplane, by using either the Upbound CLI (see here for details about installation) or a YAML manifest. Installation \u00b6 You can check available releases on project's GitHub repository or using gh like gh release list --repo upbound/provider-aws and store the desired release into the PROVIDER_AWS_RELEASE variable. Upbound CLI YAML manifest Do make sure you have installed the up CLI as described here and execute up controlplane provider install xpkg.upbound.io/upbound/provider-aws: ${ PROVIDER_AWS_RELEASE } --name provider-aws kubectl apply -f - <<EOF apiVersion: pkg.crossplane.io/v1 kind: Provider metadata: name: provider-aws spec: package: xpkg.upbound.io/upbound/provider-aws:${PROVIDER_AWS_RELEASE} EOF It is now necessary to configure the provider's authentication to the AWS API endpoints. The authentication method can vary based on your company's policies: for example, you might be allowed to use long-term credentials such as access key and secret access key pairs, however, if your Kubernetes platform is AWS EKS, it's much more secure to use IAM roles for service accounts (IRSA) . Please make sure you do create the OIDC provider as described in the EKS set-up guide before reading on. The following paragraphs explain how to configure IRSA for the Crossplane AWS provider. Create IAM role and policy \u00b6 You must create a proper role for the provider to assume, for granting the necessary and sufficient permissions to manage the AWS infrastructure. The least-privilege principle applies, therefore it's important to understand the actual needs and create the permission policy accordingly. For example, the following snippet creates a policy that allows the role it's attached to to execute actions only on the S3 service. First of all, set your the AWS region you're operating in and your EKS cluster name. # AWS region you're operating in export AWS_REGION = \"eu-central-1\" # EKS cluster name CLUSTER_NAME = \"my-eks-cluster\" # define variables for IAM ACCOUNT_ID = $( aws sts get-caller-identity --query Account --output text ) OIDC_ID = $( aws eks describe-cluster --name ${ CLUSTER_NAME } --output text --query \"cluster.identity.oidc.issuer\" | cut -d/ -f3- ) CROSSPLANE_ROLE = \"crossplane-for- ${ CLUSTER_NAME } \" ROLE_TRUST_POLICY = $( mktemp ) ROLE_PERMISSION_POLICY = $( mktemp ) You can then define the role's trust policy, in order to allow Crossplane AWS provider's service account to assume it as WebIdentity, and then create the role. # prepare the trust policy document cat > ${ ROLE_TRUST_POLICY } <<EOF { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_ID}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringLike\": { \"${OIDC_ID}:sub\": \"system:serviceaccount:upbound-system:provider-aws-*\" } } }] } EOF # create the role with the proper trust policy aws iam create-role --role-name ${ CROSSPLANE_ROLE } --assume-role-policy-document file:// ${ ROLE_TRUST_POLICY } Now, the permission policies, that define which permissions are granted to the role, have to be created and attached to the role. For this example you will need just one policy, with a number of statements declaring what the role can or cannot do. # create the permission policy document cat > ${ ROLE_PERMISSION_POLICY } <<EOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"s3:*\", \"Effect\": \"Allow\", \"Resource\": \"*\" } ] } EOF # create the permission policy PERMISSION_POLICY_ARN = $( aws iam create-policy --policy-name ${ CROSSPLANE_ROLE } --policy-document file:// ${ ROLE_PERMISSION_POLICY } --query Policy.Arn --output text ) # attach the policy to the role aws iam attach-role-policy --policy-arn ${ PERMISSION_POLICY_ARN } --role-name ${ CROSSPLANE_ROLE } # clean up temporary files rm ${ ROLE_TRUST_POLICY } rm ${ ROLE_PERMISSION_POLICY } Create Kubernetes resources \u00b6 Create a ProviderConfig resource to specify IRSA as authentication method. kubectl apply -f - <<EOF apiVersion: aws.upbound.io/v1beta1 kind: ProviderConfig metadata: name: default spec: credentials: source: IRSA EOF Create a ControllerConfig resource to specify the AWS provider's settings, including the IRSA role to assume: kubectl apply -f - <<EOF apiVersion: pkg.crossplane.io/v1alpha1 kind: ControllerConfig metadata: annotations: eks.amazonaws.com/role-arn: arn:aws:iam::${ACCOUNT_ID}:role/${CROSSPLANE_ROLE} name: aws-irsa EOF and patch the AWS provider to use it: kubectl patch providers.pkg.crossplane.io provider-aws --type = 'merge' --patch '{\"spec\": { \"controllerConfigRef\": { \"name\": \"aws-irsa\" } } }' Destroy the existing pods to make sure that new ones will be created: kubectl -n upbound-system delete pods -l pkg.crossplane.io/provider = provider-aws Now you can test the effectiveness of the configuration by creating a simple S3 bucket: BUCKET_NAME = $( kubectl create -o yaml -f - <<EOF | yq '.metadata.name' apiVersion: s3.aws.upbound.io/v1beta1 kind: Bucket metadata: generateName: crossplane-test-bucket- spec: forProvider: region: ${AWS_REGION} EOF ) and verify its status $ kubectl get buckets.s3.aws.upbound.io ${ BUCKET_NAME } NAME READY SYNCED EXTERNAL-NAME AGE crossplane-test-bucket-cxr9g True True crossplane-test-bucket-cxr9g 80s As the bucket is marked as synced, it's worth checking the status of the AWS resource: $ aws s3api list-buckets | jq '.Buckets[]|select(.Name == \"' ${ BUCKET_NAME } '\")' { \"Name\" : \"crossplane-test-bucket-cxr9g\" , \"CreationDate\" : \"2022-11-05T00:36:49+00:00\" } This proves that the provider is configured correctly and you can safely delete the test bucket: kubectl delete buckets.s3.aws.upbound.io ${ BUCKET_NAME }","title":"UXP AWS provider"},{"location":"crossplane/providers/aws/#installation","text":"You can check available releases on project's GitHub repository or using gh like gh release list --repo upbound/provider-aws and store the desired release into the PROVIDER_AWS_RELEASE variable. Upbound CLI YAML manifest Do make sure you have installed the up CLI as described here and execute up controlplane provider install xpkg.upbound.io/upbound/provider-aws: ${ PROVIDER_AWS_RELEASE } --name provider-aws kubectl apply -f - <<EOF apiVersion: pkg.crossplane.io/v1 kind: Provider metadata: name: provider-aws spec: package: xpkg.upbound.io/upbound/provider-aws:${PROVIDER_AWS_RELEASE} EOF It is now necessary to configure the provider's authentication to the AWS API endpoints. The authentication method can vary based on your company's policies: for example, you might be allowed to use long-term credentials such as access key and secret access key pairs, however, if your Kubernetes platform is AWS EKS, it's much more secure to use IAM roles for service accounts (IRSA) . Please make sure you do create the OIDC provider as described in the EKS set-up guide before reading on. The following paragraphs explain how to configure IRSA for the Crossplane AWS provider.","title":"Installation"},{"location":"crossplane/providers/aws/#create-iam-role-and-policy","text":"You must create a proper role for the provider to assume, for granting the necessary and sufficient permissions to manage the AWS infrastructure. The least-privilege principle applies, therefore it's important to understand the actual needs and create the permission policy accordingly. For example, the following snippet creates a policy that allows the role it's attached to to execute actions only on the S3 service. First of all, set your the AWS region you're operating in and your EKS cluster name. # AWS region you're operating in export AWS_REGION = \"eu-central-1\" # EKS cluster name CLUSTER_NAME = \"my-eks-cluster\" # define variables for IAM ACCOUNT_ID = $( aws sts get-caller-identity --query Account --output text ) OIDC_ID = $( aws eks describe-cluster --name ${ CLUSTER_NAME } --output text --query \"cluster.identity.oidc.issuer\" | cut -d/ -f3- ) CROSSPLANE_ROLE = \"crossplane-for- ${ CLUSTER_NAME } \" ROLE_TRUST_POLICY = $( mktemp ) ROLE_PERMISSION_POLICY = $( mktemp ) You can then define the role's trust policy, in order to allow Crossplane AWS provider's service account to assume it as WebIdentity, and then create the role. # prepare the trust policy document cat > ${ ROLE_TRUST_POLICY } <<EOF { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_ID}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringLike\": { \"${OIDC_ID}:sub\": \"system:serviceaccount:upbound-system:provider-aws-*\" } } }] } EOF # create the role with the proper trust policy aws iam create-role --role-name ${ CROSSPLANE_ROLE } --assume-role-policy-document file:// ${ ROLE_TRUST_POLICY } Now, the permission policies, that define which permissions are granted to the role, have to be created and attached to the role. For this example you will need just one policy, with a number of statements declaring what the role can or cannot do. # create the permission policy document cat > ${ ROLE_PERMISSION_POLICY } <<EOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"s3:*\", \"Effect\": \"Allow\", \"Resource\": \"*\" } ] } EOF # create the permission policy PERMISSION_POLICY_ARN = $( aws iam create-policy --policy-name ${ CROSSPLANE_ROLE } --policy-document file:// ${ ROLE_PERMISSION_POLICY } --query Policy.Arn --output text ) # attach the policy to the role aws iam attach-role-policy --policy-arn ${ PERMISSION_POLICY_ARN } --role-name ${ CROSSPLANE_ROLE } # clean up temporary files rm ${ ROLE_TRUST_POLICY } rm ${ ROLE_PERMISSION_POLICY }","title":"Create IAM role and policy"},{"location":"crossplane/providers/aws/#create-kubernetes-resources","text":"Create a ProviderConfig resource to specify IRSA as authentication method. kubectl apply -f - <<EOF apiVersion: aws.upbound.io/v1beta1 kind: ProviderConfig metadata: name: default spec: credentials: source: IRSA EOF Create a ControllerConfig resource to specify the AWS provider's settings, including the IRSA role to assume: kubectl apply -f - <<EOF apiVersion: pkg.crossplane.io/v1alpha1 kind: ControllerConfig metadata: annotations: eks.amazonaws.com/role-arn: arn:aws:iam::${ACCOUNT_ID}:role/${CROSSPLANE_ROLE} name: aws-irsa EOF and patch the AWS provider to use it: kubectl patch providers.pkg.crossplane.io provider-aws --type = 'merge' --patch '{\"spec\": { \"controllerConfigRef\": { \"name\": \"aws-irsa\" } } }' Destroy the existing pods to make sure that new ones will be created: kubectl -n upbound-system delete pods -l pkg.crossplane.io/provider = provider-aws Now you can test the effectiveness of the configuration by creating a simple S3 bucket: BUCKET_NAME = $( kubectl create -o yaml -f - <<EOF | yq '.metadata.name' apiVersion: s3.aws.upbound.io/v1beta1 kind: Bucket metadata: generateName: crossplane-test-bucket- spec: forProvider: region: ${AWS_REGION} EOF ) and verify its status $ kubectl get buckets.s3.aws.upbound.io ${ BUCKET_NAME } NAME READY SYNCED EXTERNAL-NAME AGE crossplane-test-bucket-cxr9g True True crossplane-test-bucket-cxr9g 80s As the bucket is marked as synced, it's worth checking the status of the AWS resource: $ aws s3api list-buckets | jq '.Buckets[]|select(.Name == \"' ${ BUCKET_NAME } '\")' { \"Name\" : \"crossplane-test-bucket-cxr9g\" , \"CreationDate\" : \"2022-11-05T00:36:49+00:00\" } This proves that the provider is configured correctly and you can safely delete the test bucket: kubectl delete buckets.s3.aws.upbound.io ${ BUCKET_NAME }","title":"Create Kubernetes resources"},{"location":"crossplane/providers/azure/","text":"Upbound's Azure Provider is an Azure provider for Crossplane that is developed and supported by Upbound. It can be deployed on top of a Kubernetes cluster with Crossplane using the Upbound CLI (see here for details about installation) or a YAML manifest. Installation \u00b6 You can check available releases on project's GitHub repository or using gh like gh release list --repo upbound/provider-azure Store the desired release in the PROVIDER_AZURE_RELEASE variable. Note Upbound CLI YAML manifest Do make sure you have installed the up CLI, as described here , and execute up controlplane provider install xpkg.upbound.io/upbound/provider-azure: ${ PROVIDER_AZURE_RELEASE } kubectl apply -f - <<EOF apiVersion: pkg.crossplane.io/v1 kind: Provider metadata: name: provider-azure spec: package: xpkg.upbound.io/upbound/provider-azure:${PROVIDER_AZURE_RELEASE} EOF Ensure the provider is installed and healthy by running the following: kubectl get provider Which should yield something like the following: NAME INSTALLED HEALTHY PACKAGE AGE provider-azure True True xpkg.upbound.io/upbound/provider-azure:v0.17.0 2m35s Before we can use the provider, we need to supply it with credentials. We can use a Service Provider or a Managed Service Identity . Verify Provider \u00b6 The installation of the Crossplane Azure provider results in the availability of new Kubernetes APIs for interacting with Azure resources from within the TAP cluster. The total number of available resources is relatively high, so let us focus on the resources related to CosmosDB. kubectl api-resources --api-group cosmosdb.azure.upbound.io Running this command prints the following APIs: NAME SHORTNAMES APIVERSION NAMESPACED KIND accounts cosmosdb.azure.upbound.io/v1beta1 false Account cassandraclusters cosmosdb.azure.upbound.io/v1beta1 false CassandraCluster cassandradatacenters cosmosdb.azure.upbound.io/v1beta1 false CassandraDatacenter cassandrakeyspaces cosmosdb.azure.upbound.io/v1beta1 false CassandraKeySpace cassandratables cosmosdb.azure.upbound.io/v1beta1 false CassandraTable gremlindatabases cosmosdb.azure.upbound.io/v1beta1 false GremlinDatabase gremlingraphs cosmosdb.azure.upbound.io/v1beta1 false GremlinGraph mongocollections cosmosdb.azure.upbound.io/v1beta1 false MongoCollection mongodatabases cosmosdb.azure.upbound.io/v1beta1 false MongoDatabase sqlcontainers cosmosdb.azure.upbound.io/v1beta1 false SQLContainer sqldatabases cosmosdb.azure.upbound.io/v1beta1 false SQLDatabase sqlfunctions cosmosdb.azure.upbound.io/v1beta1 false SQLFunction sqlroleassignments cosmosdb.azure.upbound.io/v1beta1 false SQLRoleAssignment sqlroledefinitions cosmosdb.azure.upbound.io/v1beta1 false SQLRoleDefinition sqlstoredprocedures cosmosdb.azure.upbound.io/v1beta1 false SQLStoredProcedure sqltriggers cosmosdb.azure.upbound.io/v1beta1 false SQLTrigger tables cosmosdb.azure.upbound.io/v1beta1 false Table Create & Configure Service Provider \u00b6 Before proceeding, we need an Azure subscription, an Azure account with sufficient privileges, and the az (Azure) CLI. You can find how to install the CLI here . Login to Azure via its CLI ( az ). az login If you're not sure about your subscription id, you can run the following command: az account show --query \"{subscriptionId:id, tenantId:tenantId}\" SUBSCRIPTION_ID = Then create a Service Principle ( sp ), which has sufficient permissions to create all the necessary resources in Azure. For example: az ad sp create-for-rbac \\ --sdk-auth \\ --role Owner \\ --scopes \"/subscriptions/ ${ SUBSCRIPTION_ID } \" Warning You probably do not want to give it the role Owner if this is a production account. Save the output as azure-credentials.json and create a Kubernetes secret in the upbound-system (assuming you use uxp ). kubectl create secret generic azure-secret \\ -n upbound-system \\ --from-file = creds = ./azure-credentials.json We then create a ProviderConfig , pointing to this credential. kubectl apply -f - <<EOF apiVersion: azure.upbound.io/v1beta1 kind: ProviderConfig metadata: name: default spec: credentials: source: Secret secretRef: namespace: upbound-system name: azure-secret key: creds EOF Create & Configure Managed Service Identity \u00b6 TBD","title":"UXP Azure provider"},{"location":"crossplane/providers/azure/#installation","text":"You can check available releases on project's GitHub repository or using gh like gh release list --repo upbound/provider-azure Store the desired release in the PROVIDER_AZURE_RELEASE variable. Note Upbound CLI YAML manifest Do make sure you have installed the up CLI, as described here , and execute up controlplane provider install xpkg.upbound.io/upbound/provider-azure: ${ PROVIDER_AZURE_RELEASE } kubectl apply -f - <<EOF apiVersion: pkg.crossplane.io/v1 kind: Provider metadata: name: provider-azure spec: package: xpkg.upbound.io/upbound/provider-azure:${PROVIDER_AZURE_RELEASE} EOF Ensure the provider is installed and healthy by running the following: kubectl get provider Which should yield something like the following: NAME INSTALLED HEALTHY PACKAGE AGE provider-azure True True xpkg.upbound.io/upbound/provider-azure:v0.17.0 2m35s Before we can use the provider, we need to supply it with credentials. We can use a Service Provider or a Managed Service Identity .","title":"Installation"},{"location":"crossplane/providers/azure/#verify-provider","text":"The installation of the Crossplane Azure provider results in the availability of new Kubernetes APIs for interacting with Azure resources from within the TAP cluster. The total number of available resources is relatively high, so let us focus on the resources related to CosmosDB. kubectl api-resources --api-group cosmosdb.azure.upbound.io Running this command prints the following APIs: NAME SHORTNAMES APIVERSION NAMESPACED KIND accounts cosmosdb.azure.upbound.io/v1beta1 false Account cassandraclusters cosmosdb.azure.upbound.io/v1beta1 false CassandraCluster cassandradatacenters cosmosdb.azure.upbound.io/v1beta1 false CassandraDatacenter cassandrakeyspaces cosmosdb.azure.upbound.io/v1beta1 false CassandraKeySpace cassandratables cosmosdb.azure.upbound.io/v1beta1 false CassandraTable gremlindatabases cosmosdb.azure.upbound.io/v1beta1 false GremlinDatabase gremlingraphs cosmosdb.azure.upbound.io/v1beta1 false GremlinGraph mongocollections cosmosdb.azure.upbound.io/v1beta1 false MongoCollection mongodatabases cosmosdb.azure.upbound.io/v1beta1 false MongoDatabase sqlcontainers cosmosdb.azure.upbound.io/v1beta1 false SQLContainer sqldatabases cosmosdb.azure.upbound.io/v1beta1 false SQLDatabase sqlfunctions cosmosdb.azure.upbound.io/v1beta1 false SQLFunction sqlroleassignments cosmosdb.azure.upbound.io/v1beta1 false SQLRoleAssignment sqlroledefinitions cosmosdb.azure.upbound.io/v1beta1 false SQLRoleDefinition sqlstoredprocedures cosmosdb.azure.upbound.io/v1beta1 false SQLStoredProcedure sqltriggers cosmosdb.azure.upbound.io/v1beta1 false SQLTrigger tables cosmosdb.azure.upbound.io/v1beta1 false Table","title":"Verify Provider"},{"location":"crossplane/providers/azure/#create-configure-service-provider","text":"Before proceeding, we need an Azure subscription, an Azure account with sufficient privileges, and the az (Azure) CLI. You can find how to install the CLI here . Login to Azure via its CLI ( az ). az login If you're not sure about your subscription id, you can run the following command: az account show --query \"{subscriptionId:id, tenantId:tenantId}\" SUBSCRIPTION_ID = Then create a Service Principle ( sp ), which has sufficient permissions to create all the necessary resources in Azure. For example: az ad sp create-for-rbac \\ --sdk-auth \\ --role Owner \\ --scopes \"/subscriptions/ ${ SUBSCRIPTION_ID } \" Warning You probably do not want to give it the role Owner if this is a production account. Save the output as azure-credentials.json and create a Kubernetes secret in the upbound-system (assuming you use uxp ). kubectl create secret generic azure-secret \\ -n upbound-system \\ --from-file = creds = ./azure-credentials.json We then create a ProviderConfig , pointing to this credential. kubectl apply -f - <<EOF apiVersion: azure.upbound.io/v1beta1 kind: ProviderConfig metadata: name: default spec: credentials: source: Secret secretRef: namespace: upbound-system name: azure-secret key: creds EOF","title":"Create &amp; Configure Service Provider"},{"location":"crossplane/providers/azure/#create-configure-managed-service-identity","text":"TBD","title":"Create &amp; Configure Managed Service Identity"},{"location":"usecases/aws/packages/elasticache/ack/","text":"This guide describes using the Services Toolkit to allow Tanzu Application Platform workloads to consume AWS Elasticache. This particular topic makes use of AWS Controller for Kubernetes (ACK) to manage AWS Elasticache instances. Note This usecase is not currently compatible with TAP air-gapped installations. Prerequisites \u00b6 You need to meet a number of prerequisites before being able to effectively follow this guide. Create service instances that are compatible with Tanzu Application Platform \u00b6 The installation of the AWS Elasticache Controller for Kubernetes results in the availability of new Kubernetes APIs for interacting with Elasticache resources from within the TAP cluster. kubectl api-resources --api-group elasticache.services.k8s.aws NAME SHORTNAMES APIVERSION NAMESPACED KIND cacheparametergroups elasticache.services.k8s.aws/v1alpha1 true CacheParameterGroup cachesubnetgroups elasticache.services.k8s.aws/v1alpha1 true CacheSubnetGroup replicationgroups elasticache.services.k8s.aws/v1alpha1 true ReplicationGroup snapshots elasticache.services.k8s.aws/v1alpha1 true Snapshot usergroups elasticache.services.k8s.aws/v1alpha1 true UserGroup users elasticache.services.k8s.aws/v1alpha1 true User To create an AWS Elasticache service instance for consumption by Tanzu Application Platform, you can use a ready-made, reference Carvel Package. The Service Operator typically performs this step. Follow the steps in Creating an AWS Elasticache service instance using a Carvel Package . Alternatively, if you are interested in authoring your own Reference Package and want to learn about the underlying APIs and how they come together to produce a useable service instance for the Tanzu Application Platform, you can achieve the same outcome using the more advanced Creating an AWS Elasticache service instance manually . Once you have completed either of these steps and have a running AWS Elasticache service instance, please return here to continue with the rest of the use case. Create a service instance class for AWS Elasticache \u00b6 Now that you know how to create AWS Elasticache instances, it's time to learn how to make those instances discoverable to Application Operators. Again, this step is typically performed by the service operator persona. You can use Services Toolkit's ClusterInstanceClass API to create a service instance class to represent Elasticache service instances within the cluster. The existence of such classes makes these logical service instances discoverable to application operators, thus allowing them to create Resource Claims for such instances and to then bind them to application workloads. Create the following Kubernetes resource on your AKS cluster: clusterinstanceclass.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 --- apiVersion : services.apps.tanzu.vmware.com/v1alpha1 kind : ClusterInstanceClass metadata : name : aws-elasticache spec : description : short : AWS Elasticache instances pool : kind : Secret labelSelector : matchLabels : services.apps.tanzu.vmware.com/class : aws-elasticache kubectl apply -f clusterinstanceclass.yaml In this particular example, the class represents claimable instances of Postgresql by a Secret object with the label services.apps.tanzu.vmware.com/class set to aws-elasticache . In addition, you need to grant sufficient RBAC permissions to Services Toolkit to be able to read the secrets specified by the class. Create the following RBAC on your AKS cluster: clusterrole.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : stk-secret-reader labels : servicebinding.io/controller : \"true\" rules : - apiGroups : - \"\" resources : - secrets verbs : - get - list - watch kubectl apply -f clusterrole.yaml If you want to claim resources across namespace boundaries, you will have to create a corresponding ResourceClaimPolicy . For example, if the provisioned AWS Elasticache instance named redis exists in namespace service-instances and you want to allow App Operators to claim them for workloads residing in the default namespace, you would have to create the following ResourceClaimPolicy : resourceclaimpolicy.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 --- apiVersion : services.apps.tanzu.vmware.com/v1alpha1 kind : ResourceClaimPolicy metadata : name : default-can-claim-aws-elasticache namespace : service-instances spec : subject : kind : Secret group : \"\" selector : matchLabels : services.apps.tanzu.vmware.com/class : aws-elasticache consumingNamespaces : [ \"default\" ] kubectl apply -f resourceclaimpolicy.yaml Discover, Claim, and Bind to an AWS Elasticache for Redis instance \u00b6 The act of creating the ClusterInstanceClass and the corresponding RBAC essentially advertises to application operators that AWS Elasticache for Redis is available to use with their application workloads on Tanzu Application Platform. In this section, you learn how to discover, claim, and bind to the AWS Elasticache service instance previously created. Discovery and claiming service instances is typically the responsibility of the application operator persona. Binding is typically a step for Application Developers. To discover what service instances are available to them, application operators can run: $ tanzu services classes list NAME DESCRIPTION aws-elasticache AWS Elasticache instances You can see information about the ClusterInstanceClass created in the previous step. Each ClusterInstanceClass created will be added to the list of classes returned here. The next step is to \"claim\" an instance of the desired class, but to do that, the application operators must first discover the list of currently claimable instances for the class. The capacity to claim instances is affected by many variables (including namespace boundaries, claim policies, and the exclusivity of claims) and so Services Toolkit provides a CLI command to help inform application operators of the instances that can result in successful claims. This command is the tanzu service claimable list command. $ tanzu services claimable list --class aws-elasticache -n default NAME NAMESPACE KIND APIVERSION redis-reader-creds-bindable service-instances Secret v1 redis-writer-creds-bindable service-instances Secret v1 Create a claim for the newly created secret by running: tanzu services claim create redis-writer-claim \\ --namespace default \\ --resource-namespace service-instances \\ --resource-name redis-writer-creds-bindable \\ --resource-kind Secret \\ --resource-api-version v1 Obtain the claim reference of the claim by running: $ tanzu services claim list -o wide NAME READY REASON CLAIM REF redis-writer-claim True Ready services.apps.tanzu.vmware.com/v1alpha1:ResourceClaim:redis-writer-claim Test Claim With TAP Workload \u00b6 Create an application workload that consumes the claimed AWS Elasticache by running: Example: tanzu apps workload create my-workload \\ --git-repo <a-git-repo> \\ --git-tag <a-tag-to-checkout> \\ --type web \\ --annotation autoscaling.knative.dev/minScale = 1 \\ --service-ref db = services.apps.tanzu.vmware.com/v1alpha1:ResourceClaim:redis-writer-claim --service-ref is set to the claim reference obtained previously. Your application workload starts and gets automatically the credentials to the AWS Elasticache instance via service bindings. Delete an AWS Elasticache service instance resources \u00b6 To delete the AWS Elasticache service instance, you can run the appropriate cleanup commands for how you created the service. Delete an AWS Elasticache instance via Carvel Package \u00b6 tanzu package installed delete redis-instance Delete an AWS Elasticache instance via Kubectl \u00b6 Delete the AWS Elasticache instance by running: kubectl delete -n service-instances replicationgroup.elasticache.services.k8s.aws redis kubectl delete -n service-instances usergroup.elasticache.services.k8s.aws redis kubectl delete -n service-instances user.elasticache.services.k8s.aws redis-default kubectl delete -n service-instances user.elasticache.services.k8s.aws redis-reader kubectl delete -n service-instances user.elasticache.services.k8s.aws redis-writer kubectl delete -n service-instances cachesubnetgroups.elasticache.services.k8s.aws ack-elasticache kubectl delete -n service-instances secrettemplate.secretgen.carvel.dev redis-reader-creds-bindable kubectl delete -n service-instances secrettemplate.secretgen.carvel.dev redis-writer-creds-bindable kubectl delete -n service-instances password.secretgen.carvel.dev redis-reader-creds kubectl delete -n service-instances password.secretgen.carvel.dev redis-writer-creds kubectl delete -n service-instances serviceaccounts redis-elasticache-reader kubectl delete -n service-instances role redis-elasticache-reader kubectl delete -n service-instances rolebinding redis-elasticache-reader","title":"Consuming AWS Elasticache with ACK"},{"location":"usecases/aws/packages/elasticache/ack/#prerequisites","text":"You need to meet a number of prerequisites before being able to effectively follow this guide.","title":"Prerequisites"},{"location":"usecases/aws/packages/elasticache/ack/#create-service-instances-that-are-compatible-with-tanzu-application-platform","text":"The installation of the AWS Elasticache Controller for Kubernetes results in the availability of new Kubernetes APIs for interacting with Elasticache resources from within the TAP cluster. kubectl api-resources --api-group elasticache.services.k8s.aws NAME SHORTNAMES APIVERSION NAMESPACED KIND cacheparametergroups elasticache.services.k8s.aws/v1alpha1 true CacheParameterGroup cachesubnetgroups elasticache.services.k8s.aws/v1alpha1 true CacheSubnetGroup replicationgroups elasticache.services.k8s.aws/v1alpha1 true ReplicationGroup snapshots elasticache.services.k8s.aws/v1alpha1 true Snapshot usergroups elasticache.services.k8s.aws/v1alpha1 true UserGroup users elasticache.services.k8s.aws/v1alpha1 true User To create an AWS Elasticache service instance for consumption by Tanzu Application Platform, you can use a ready-made, reference Carvel Package. The Service Operator typically performs this step. Follow the steps in Creating an AWS Elasticache service instance using a Carvel Package . Alternatively, if you are interested in authoring your own Reference Package and want to learn about the underlying APIs and how they come together to produce a useable service instance for the Tanzu Application Platform, you can achieve the same outcome using the more advanced Creating an AWS Elasticache service instance manually . Once you have completed either of these steps and have a running AWS Elasticache service instance, please return here to continue with the rest of the use case.","title":"Create service instances that are compatible with Tanzu Application Platform"},{"location":"usecases/aws/packages/elasticache/ack/#create-a-service-instance-class-for-aws-elasticache","text":"Now that you know how to create AWS Elasticache instances, it's time to learn how to make those instances discoverable to Application Operators. Again, this step is typically performed by the service operator persona. You can use Services Toolkit's ClusterInstanceClass API to create a service instance class to represent Elasticache service instances within the cluster. The existence of such classes makes these logical service instances discoverable to application operators, thus allowing them to create Resource Claims for such instances and to then bind them to application workloads. Create the following Kubernetes resource on your AKS cluster: clusterinstanceclass.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 --- apiVersion : services.apps.tanzu.vmware.com/v1alpha1 kind : ClusterInstanceClass metadata : name : aws-elasticache spec : description : short : AWS Elasticache instances pool : kind : Secret labelSelector : matchLabels : services.apps.tanzu.vmware.com/class : aws-elasticache kubectl apply -f clusterinstanceclass.yaml In this particular example, the class represents claimable instances of Postgresql by a Secret object with the label services.apps.tanzu.vmware.com/class set to aws-elasticache . In addition, you need to grant sufficient RBAC permissions to Services Toolkit to be able to read the secrets specified by the class. Create the following RBAC on your AKS cluster: clusterrole.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : stk-secret-reader labels : servicebinding.io/controller : \"true\" rules : - apiGroups : - \"\" resources : - secrets verbs : - get - list - watch kubectl apply -f clusterrole.yaml If you want to claim resources across namespace boundaries, you will have to create a corresponding ResourceClaimPolicy . For example, if the provisioned AWS Elasticache instance named redis exists in namespace service-instances and you want to allow App Operators to claim them for workloads residing in the default namespace, you would have to create the following ResourceClaimPolicy : resourceclaimpolicy.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 --- apiVersion : services.apps.tanzu.vmware.com/v1alpha1 kind : ResourceClaimPolicy metadata : name : default-can-claim-aws-elasticache namespace : service-instances spec : subject : kind : Secret group : \"\" selector : matchLabels : services.apps.tanzu.vmware.com/class : aws-elasticache consumingNamespaces : [ \"default\" ] kubectl apply -f resourceclaimpolicy.yaml","title":"Create a service instance class for AWS Elasticache"},{"location":"usecases/aws/packages/elasticache/ack/#discover-claim-and-bind-to-an-aws-elasticache-for-redis-instance","text":"The act of creating the ClusterInstanceClass and the corresponding RBAC essentially advertises to application operators that AWS Elasticache for Redis is available to use with their application workloads on Tanzu Application Platform. In this section, you learn how to discover, claim, and bind to the AWS Elasticache service instance previously created. Discovery and claiming service instances is typically the responsibility of the application operator persona. Binding is typically a step for Application Developers. To discover what service instances are available to them, application operators can run: $ tanzu services classes list NAME DESCRIPTION aws-elasticache AWS Elasticache instances You can see information about the ClusterInstanceClass created in the previous step. Each ClusterInstanceClass created will be added to the list of classes returned here. The next step is to \"claim\" an instance of the desired class, but to do that, the application operators must first discover the list of currently claimable instances for the class. The capacity to claim instances is affected by many variables (including namespace boundaries, claim policies, and the exclusivity of claims) and so Services Toolkit provides a CLI command to help inform application operators of the instances that can result in successful claims. This command is the tanzu service claimable list command. $ tanzu services claimable list --class aws-elasticache -n default NAME NAMESPACE KIND APIVERSION redis-reader-creds-bindable service-instances Secret v1 redis-writer-creds-bindable service-instances Secret v1 Create a claim for the newly created secret by running: tanzu services claim create redis-writer-claim \\ --namespace default \\ --resource-namespace service-instances \\ --resource-name redis-writer-creds-bindable \\ --resource-kind Secret \\ --resource-api-version v1 Obtain the claim reference of the claim by running: $ tanzu services claim list -o wide NAME READY REASON CLAIM REF redis-writer-claim True Ready services.apps.tanzu.vmware.com/v1alpha1:ResourceClaim:redis-writer-claim","title":"Discover, Claim, and Bind to an AWS Elasticache for Redis instance"},{"location":"usecases/aws/packages/elasticache/ack/#test-claim-with-tap-workload","text":"Create an application workload that consumes the claimed AWS Elasticache by running: Example: tanzu apps workload create my-workload \\ --git-repo <a-git-repo> \\ --git-tag <a-tag-to-checkout> \\ --type web \\ --annotation autoscaling.knative.dev/minScale = 1 \\ --service-ref db = services.apps.tanzu.vmware.com/v1alpha1:ResourceClaim:redis-writer-claim --service-ref is set to the claim reference obtained previously. Your application workload starts and gets automatically the credentials to the AWS Elasticache instance via service bindings.","title":"Test Claim With TAP Workload"},{"location":"usecases/aws/packages/elasticache/ack/#delete-an-aws-elasticache-service-instance-resources","text":"To delete the AWS Elasticache service instance, you can run the appropriate cleanup commands for how you created the service.","title":"Delete an AWS Elasticache service instance resources"},{"location":"usecases/aws/packages/elasticache/ack/#delete-an-aws-elasticache-instance-via-carvel-package","text":"tanzu package installed delete redis-instance","title":"Delete an AWS Elasticache instance via Carvel Package"},{"location":"usecases/aws/packages/elasticache/ack/#delete-an-aws-elasticache-instance-via-kubectl","text":"Delete the AWS Elasticache instance by running: kubectl delete -n service-instances replicationgroup.elasticache.services.k8s.aws redis kubectl delete -n service-instances usergroup.elasticache.services.k8s.aws redis kubectl delete -n service-instances user.elasticache.services.k8s.aws redis-default kubectl delete -n service-instances user.elasticache.services.k8s.aws redis-reader kubectl delete -n service-instances user.elasticache.services.k8s.aws redis-writer kubectl delete -n service-instances cachesubnetgroups.elasticache.services.k8s.aws ack-elasticache kubectl delete -n service-instances secrettemplate.secretgen.carvel.dev redis-reader-creds-bindable kubectl delete -n service-instances secrettemplate.secretgen.carvel.dev redis-writer-creds-bindable kubectl delete -n service-instances password.secretgen.carvel.dev redis-reader-creds kubectl delete -n service-instances password.secretgen.carvel.dev redis-writer-creds kubectl delete -n service-instances serviceaccounts redis-elasticache-reader kubectl delete -n service-instances role redis-elasticache-reader kubectl delete -n service-instances rolebinding redis-elasticache-reader","title":"Delete an AWS Elasticache instance via Kubectl"},{"location":"usecases/aws/packages/elasticache/ack/manual/","text":"This topic describes how to use Services Toolkit to allow Tanzu Application Platform workloads to consume AWS Elasticache for Redis. This particular topic makes use of AWS Controller for Kubernetes (ACK) to manage AWS Elasticache resources. Following this guide, you will be creating all the resources in the service-instances namespace. It's important to make sure it exists before reading on. Create a cache subnet group \u00b6 The Elasticache instances must be created in a Cache Subnet Group, that can be created in a number of ways, here you will leverage ACK to create it. First of all, you have to get the IDs of the subnets you want to build the Cache Subnet Group for. Example You could have a list of all the subnets in a given VPC and then choose amongst them. VPC_NAME = \"my-vpc-name\" VPC_ID = $( aws ec2 describe-vpcs --filter \"Name=tag:Name,Values= ${ VPC_NAME } \" --query \"Vpcs[0].VpcId\" --output text ) SUBNET_IDS = $( aws ec2 describe-subnets --filters \"Name=vpc-id,Values= ${ VPC_ID } \" --query \"Subnets[].SubnetId\" ) This is just an example. Make sure you do choose your subnets carefully. When you have a list of subnetIDs stored in the SUBNET_IDS shell variable, you can create your CacheSubnetGroup . Create the following ytt template cache-subnet-group.ytt.yaml 1 2 3 4 5 6 7 8 9 10 11 #@ load(\"@ytt:data\", \"data\") --- apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : CacheSubnetGroup metadata : name : ack-elasticache namespace : service-instances spec : cacheSubnetGroupDescription : A subnet group for Elasticache cacheSubnetGroupName : #@ data.values.cacheSubnetGroupName subnetIDs : #@ data.values.subnetIDs Now you can use ytt to add the proper values and pipe it to kubectl apply CACHE_SUBNET_GROUP_NAME = \"ack-elasticache\" ytt \\ -v cacheSubnetGroupName = \" ${ CACHE_SUBNET_GROUP_NAME } \" \\ -v subnetIDs = \" ${ SUBNET_IDS } \" \\ -f cache-subnet-group.ytt.yaml \\ | kubectl apply -f - Create the users to log into Elasticache \u00b6 You need at least one usergroup with at least one member user to associate to the Elasticache instance. Every group must have one user with name default and up to 100 total users (more on Elasticache quotas ). However, there can be only one user with id default per Elasticache instance , which is automatically made available by AWS, but there can be more users with name default and different id. This is useful to know in order to create proper default users for each group. For the sake of this example, you will create just one user group and a default user in it with all the permissions. In order to generate a random password, you will make use of Secretgen Controller , which is provided out of the box by TAP, thus if you went through the prerequisites it should have already been installed. The following snippet declares the default user along with its auto-generated password and the usergroup: elasticache-user.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 --- apiVersion : secretgen.k14s.io/v1alpha1 kind : Password metadata : name : ack-elasticache-default-creds namespace : service-instances spec : length : 128 secretTemplate : type : Opaque stringData : password : $(value) --- apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : User metadata : name : ack-elasticache-default namespace : service-instances spec : accessString : on ~* +@all engine : redis passwords : - name : ack-elasticache-default-creds key : password namespace : service-instances userID : ack-elasticache-default userName : default --- apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : UserGroup metadata : name : ack-elasticache namespace : service-instances spec : engine : redis userGroupID : ack-elasticache userIDs : - ack-elasticache-default Store it into the elasticache-user.yaml file and apply it: kubectl apply -f elasticache-user.yaml Create the ReplicationGroup \u00b6 Before going ahead and create the ReplicationGroup resource, which maps to the actual instance that can be consumed, you need to create a proper security group for filtering the incoming and outgoing traffic. Assuming that you want to be able to connect to Elasticache from the EKS instance created previously , you can use the EKS security group as source for the new security group which will actually filter the Elasticache traffic. # AWS region you're operating in export AWS_REGION = \"eu-west-1\" # EKS cluster name CLUSTER_NAME = \"my-eks-cluster\" EKS_SECURITY_GROUP_ID = $( aws eks describe-cluster --name ${ CLUSTER_NAME } --query 'cluster.resourcesVpcConfig.clusterSecurityGroupId' --output text ) # VPC_ID has been defined above in \"Create a cache subnet group\" ELASTICACHE_SECURITY_GROUP_ID = $( aws ec2 create-security-group --group-name \"Elasticache\" --description \"Elasticache security group\" --vpc-id ${ VPC_ID } --output text --query GroupId ) REDIS_PORT = 6379 aws ec2 authorize-security-group-ingress --group-id ${ ELASTICACHE_SECURITY_GROUP_ID } --source-group ${ EKS_SECURITY_GROUP_ID } --protocol tcp --port ${ REDIS_PORT } Now you can define the ReplicationGroup using the following ytt template replication-group.ytt.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #@ load(\"@ytt:data\", \"data\") --- apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : ReplicationGroup metadata : name : ack-elasticache namespace : service-instances spec : description : A redis service instance engine : redis replicationGroupID : ack-elasticache cacheNodeType : cache.t2.micro cacheSubnetGroupName : #@ data.values.cacheSubnetGroupName securityGroupIDs : - #@ data.values.securityGroupID userGroupIDs : - ack-elasticache and apply it ytt \\ -v cacheSubnetGroupName = \" ${ CACHE_SUBNET_GROUP_NAME } \" \\ -v securityGroupID = \" ${ ELASTICACHE_SECURITY_GROUP_ID } \" \\ -f replication-group.ytt.yaml \\ | kubectl apply -f - It will take 5 to 10 minutes to create. You can wait for the resource to be ready running the command kubectl -n service-instances wait --for = condition = ACK.ResourceSynced = True replicationgroups.elasticache.services.k8s.aws ack-elasticache or you can take a closer look at the new resource kubectl -n service-instances get replicationgroups.elasticache.services.k8s.aws ack-elasticache -o yaml particularly at the status field, which eventually will display something like status : ... conditions : - status : \"True\" type : ACK.ResourceSynced ... status : available The status object also contains the details of the provisioned AWS resource, along with the nodegroups and their endpoints. Create a Binding Specification Compatible Secret \u00b6 As mentioned in Creating service instances that are compatible with Tanzu Application Platform , in order for Tanzu Application Platform workloads to be able to claim and bind to services such as AWS Elasticache, a resource compatible with Service Binding Specification must exist in the cluster. This can take the form of either a ProvisionedService , as defined by the specification, or a Kubernetes Secret with some known keys, also as defined in the specification. In this guide, you create a Kubernetes secret in the necessary format using the secretgen-controller tooling. You do so by using the SecretTemplate API to extract values from the ACK resources and populate a new spec-compatible secret with the values. Create a ServiceAccount for Secret Templating \u00b6 As part of using the SecretTemplate API, a Kubernetes ServiceAccount must be provided. The ServiceAccount is used for reading the ReplicationGroup resource and the Secret created from the Password resource above. Create the following Kubernetes resources on your EKS cluster: rbac.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 --- apiVersion : v1 kind : ServiceAccount metadata : name : ack-elasticache-reader namespace : service-instances --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : ack-elasticache-reader namespace : service-instances rules : - apiGroups : - \"\" resources : - secrets verbs : - get - list - watch resourceNames : - ack-elasticache-default-creds - apiGroups : - elasticache.services.k8s.aws resources : - replicationgroups verbs : - get - list - watch resourceNames : - ack-elasticache --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : ack-elasticache-reader namespace : service-instances roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : ack-elasticache-reader subjects : - kind : ServiceAccount name : ack-elasticache-reader namespace : service-instances kubectl apply -f rbac.yaml Create a SecretTemplate \u00b6 In combination with the ServiceAccount just created, a SecretTemplate can be used to declaratively create a secret that is compatible with the service binding specification. For more information on this API see the Secret Template Documentation . Create the following Kubernetes resource on your EKS cluster: secrettemplate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 apiVersion : secretgen.carvel.dev/v1alpha1 kind : SecretTemplate metadata : name : ack-elasticache-default-creds-bindable namespace : service-instances spec : serviceAccountName : ack-elasticache-reader inputResources : - name : replicationGroup ref : apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : ReplicationGroup name : ack-elasticache - name : creds ref : apiVersion : v1 kind : Secret name : ack-elasticache-default-creds template : metadata : labels : services.apps.tanzu.vmware.com/class : aws-elasticache type : servicebinding.io/redis stringData : type : redis username : $(.creds.data.username) ssl : \"true\" host : $(.replicationGroup.status.nodeGroups[0].primaryEndpoint.address) port : $(.replicationGroup.status.nodeGroups[0].primaryEndpoint.port) data : password : $(.creds.data.password) kubectl apply -f secrettemplate.yaml Verify the Service Instance \u00b6 Wait until the ReplicationGroup instance is ready as described before . Next, ensure a bindable Secret was produced by the SecretTemplate . To do so, run: kubectl -n service-instances wait --for = condition = ReconcileSucceeded = True secrettemplates.secretgen.carvel.dev ack-elasticache-default-creds-bindable kubectl -n service-instances get secret ack-elasticache-default-creds-bindable","title":"Creating AWS Elasticache Instances manually using kubectl (experimental)"},{"location":"usecases/aws/packages/elasticache/ack/manual/#create-a-cache-subnet-group","text":"The Elasticache instances must be created in a Cache Subnet Group, that can be created in a number of ways, here you will leverage ACK to create it. First of all, you have to get the IDs of the subnets you want to build the Cache Subnet Group for. Example You could have a list of all the subnets in a given VPC and then choose amongst them. VPC_NAME = \"my-vpc-name\" VPC_ID = $( aws ec2 describe-vpcs --filter \"Name=tag:Name,Values= ${ VPC_NAME } \" --query \"Vpcs[0].VpcId\" --output text ) SUBNET_IDS = $( aws ec2 describe-subnets --filters \"Name=vpc-id,Values= ${ VPC_ID } \" --query \"Subnets[].SubnetId\" ) This is just an example. Make sure you do choose your subnets carefully. When you have a list of subnetIDs stored in the SUBNET_IDS shell variable, you can create your CacheSubnetGroup . Create the following ytt template cache-subnet-group.ytt.yaml 1 2 3 4 5 6 7 8 9 10 11 #@ load(\"@ytt:data\", \"data\") --- apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : CacheSubnetGroup metadata : name : ack-elasticache namespace : service-instances spec : cacheSubnetGroupDescription : A subnet group for Elasticache cacheSubnetGroupName : #@ data.values.cacheSubnetGroupName subnetIDs : #@ data.values.subnetIDs Now you can use ytt to add the proper values and pipe it to kubectl apply CACHE_SUBNET_GROUP_NAME = \"ack-elasticache\" ytt \\ -v cacheSubnetGroupName = \" ${ CACHE_SUBNET_GROUP_NAME } \" \\ -v subnetIDs = \" ${ SUBNET_IDS } \" \\ -f cache-subnet-group.ytt.yaml \\ | kubectl apply -f -","title":"Create a cache subnet group"},{"location":"usecases/aws/packages/elasticache/ack/manual/#create-the-users-to-log-into-elasticache","text":"You need at least one usergroup with at least one member user to associate to the Elasticache instance. Every group must have one user with name default and up to 100 total users (more on Elasticache quotas ). However, there can be only one user with id default per Elasticache instance , which is automatically made available by AWS, but there can be more users with name default and different id. This is useful to know in order to create proper default users for each group. For the sake of this example, you will create just one user group and a default user in it with all the permissions. In order to generate a random password, you will make use of Secretgen Controller , which is provided out of the box by TAP, thus if you went through the prerequisites it should have already been installed. The following snippet declares the default user along with its auto-generated password and the usergroup: elasticache-user.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 --- apiVersion : secretgen.k14s.io/v1alpha1 kind : Password metadata : name : ack-elasticache-default-creds namespace : service-instances spec : length : 128 secretTemplate : type : Opaque stringData : password : $(value) --- apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : User metadata : name : ack-elasticache-default namespace : service-instances spec : accessString : on ~* +@all engine : redis passwords : - name : ack-elasticache-default-creds key : password namespace : service-instances userID : ack-elasticache-default userName : default --- apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : UserGroup metadata : name : ack-elasticache namespace : service-instances spec : engine : redis userGroupID : ack-elasticache userIDs : - ack-elasticache-default Store it into the elasticache-user.yaml file and apply it: kubectl apply -f elasticache-user.yaml","title":"Create the users to log into Elasticache"},{"location":"usecases/aws/packages/elasticache/ack/manual/#create-the-replicationgroup","text":"Before going ahead and create the ReplicationGroup resource, which maps to the actual instance that can be consumed, you need to create a proper security group for filtering the incoming and outgoing traffic. Assuming that you want to be able to connect to Elasticache from the EKS instance created previously , you can use the EKS security group as source for the new security group which will actually filter the Elasticache traffic. # AWS region you're operating in export AWS_REGION = \"eu-west-1\" # EKS cluster name CLUSTER_NAME = \"my-eks-cluster\" EKS_SECURITY_GROUP_ID = $( aws eks describe-cluster --name ${ CLUSTER_NAME } --query 'cluster.resourcesVpcConfig.clusterSecurityGroupId' --output text ) # VPC_ID has been defined above in \"Create a cache subnet group\" ELASTICACHE_SECURITY_GROUP_ID = $( aws ec2 create-security-group --group-name \"Elasticache\" --description \"Elasticache security group\" --vpc-id ${ VPC_ID } --output text --query GroupId ) REDIS_PORT = 6379 aws ec2 authorize-security-group-ingress --group-id ${ ELASTICACHE_SECURITY_GROUP_ID } --source-group ${ EKS_SECURITY_GROUP_ID } --protocol tcp --port ${ REDIS_PORT } Now you can define the ReplicationGroup using the following ytt template replication-group.ytt.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #@ load(\"@ytt:data\", \"data\") --- apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : ReplicationGroup metadata : name : ack-elasticache namespace : service-instances spec : description : A redis service instance engine : redis replicationGroupID : ack-elasticache cacheNodeType : cache.t2.micro cacheSubnetGroupName : #@ data.values.cacheSubnetGroupName securityGroupIDs : - #@ data.values.securityGroupID userGroupIDs : - ack-elasticache and apply it ytt \\ -v cacheSubnetGroupName = \" ${ CACHE_SUBNET_GROUP_NAME } \" \\ -v securityGroupID = \" ${ ELASTICACHE_SECURITY_GROUP_ID } \" \\ -f replication-group.ytt.yaml \\ | kubectl apply -f - It will take 5 to 10 minutes to create. You can wait for the resource to be ready running the command kubectl -n service-instances wait --for = condition = ACK.ResourceSynced = True replicationgroups.elasticache.services.k8s.aws ack-elasticache or you can take a closer look at the new resource kubectl -n service-instances get replicationgroups.elasticache.services.k8s.aws ack-elasticache -o yaml particularly at the status field, which eventually will display something like status : ... conditions : - status : \"True\" type : ACK.ResourceSynced ... status : available The status object also contains the details of the provisioned AWS resource, along with the nodegroups and their endpoints.","title":"Create the ReplicationGroup"},{"location":"usecases/aws/packages/elasticache/ack/manual/#create-a-binding-specification-compatible-secret","text":"As mentioned in Creating service instances that are compatible with Tanzu Application Platform , in order for Tanzu Application Platform workloads to be able to claim and bind to services such as AWS Elasticache, a resource compatible with Service Binding Specification must exist in the cluster. This can take the form of either a ProvisionedService , as defined by the specification, or a Kubernetes Secret with some known keys, also as defined in the specification. In this guide, you create a Kubernetes secret in the necessary format using the secretgen-controller tooling. You do so by using the SecretTemplate API to extract values from the ACK resources and populate a new spec-compatible secret with the values.","title":"Create a Binding Specification Compatible Secret"},{"location":"usecases/aws/packages/elasticache/ack/manual/#create-a-serviceaccount-for-secret-templating","text":"As part of using the SecretTemplate API, a Kubernetes ServiceAccount must be provided. The ServiceAccount is used for reading the ReplicationGroup resource and the Secret created from the Password resource above. Create the following Kubernetes resources on your EKS cluster: rbac.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 --- apiVersion : v1 kind : ServiceAccount metadata : name : ack-elasticache-reader namespace : service-instances --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : ack-elasticache-reader namespace : service-instances rules : - apiGroups : - \"\" resources : - secrets verbs : - get - list - watch resourceNames : - ack-elasticache-default-creds - apiGroups : - elasticache.services.k8s.aws resources : - replicationgroups verbs : - get - list - watch resourceNames : - ack-elasticache --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : ack-elasticache-reader namespace : service-instances roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : ack-elasticache-reader subjects : - kind : ServiceAccount name : ack-elasticache-reader namespace : service-instances kubectl apply -f rbac.yaml","title":"Create a ServiceAccount for Secret Templating"},{"location":"usecases/aws/packages/elasticache/ack/manual/#create-a-secrettemplate","text":"In combination with the ServiceAccount just created, a SecretTemplate can be used to declaratively create a secret that is compatible with the service binding specification. For more information on this API see the Secret Template Documentation . Create the following Kubernetes resource on your EKS cluster: secrettemplate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 apiVersion : secretgen.carvel.dev/v1alpha1 kind : SecretTemplate metadata : name : ack-elasticache-default-creds-bindable namespace : service-instances spec : serviceAccountName : ack-elasticache-reader inputResources : - name : replicationGroup ref : apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : ReplicationGroup name : ack-elasticache - name : creds ref : apiVersion : v1 kind : Secret name : ack-elasticache-default-creds template : metadata : labels : services.apps.tanzu.vmware.com/class : aws-elasticache type : servicebinding.io/redis stringData : type : redis username : $(.creds.data.username) ssl : \"true\" host : $(.replicationGroup.status.nodeGroups[0].primaryEndpoint.address) port : $(.replicationGroup.status.nodeGroups[0].primaryEndpoint.port) data : password : $(.creds.data.password) kubectl apply -f secrettemplate.yaml","title":"Create a SecretTemplate"},{"location":"usecases/aws/packages/elasticache/ack/manual/#verify-the-service-instance","text":"Wait until the ReplicationGroup instance is ready as described before . Next, ensure a bindable Secret was produced by the SecretTemplate . To do so, run: kubectl -n service-instances wait --for = condition = ReconcileSucceeded = True secrettemplates.secretgen.carvel.dev ack-elasticache-default-creds-bindable kubectl -n service-instances get secret ack-elasticache-default-creds-bindable","title":"Verify the Service Instance"},{"location":"usecases/aws/packages/elasticache/ack/package/","text":"This topic describes creating, updating, and deleting AWS Elasticache instances using a Carvel package. For a more detailed and low-level alternative procedure, see Creating Service Instances that are compatible with Tanzu Application Platform . Add a reference package repository to the cluster \u00b6 The namespace tanzu-package-repo-global has a special significance. The kapp-controller defines a Global Packaging namespace. In this namespace, any package that is made available through a Package Repository is available in every namespace. When the kapp-controller is installed via Tanzu Application Platform, the namespace is tanzu-package-repo-global . If you install the controller in another way, verify which namespace is considered the Global Packaging namespace. You can use the following command to get the global namespace: GLOBAL_NAMESPACE = $( kubectl -n kapp-controller get deployment kapp-controller -o json | jq -r '.spec.template.spec.containers[]|select(.name==\"kapp-controller\").args[]|select(.|startswith(\"-packaging-global-namespace\"))|split(\"=\")[1]' ) To add a reference package repository to the cluster: Use the Tanzu CLI to add the new Service Reference packages repository: tanzu package repository add tap-reference-service-packages \\ --url ghcr.io/vmware-tanzu/tanzu-application-platform-reference-service-packages:0.0.3 \\ -n ${ GLOBAL_NAMESPACE } Create a ServiceAccount to provision PackageInstall resources by using the following example. The namespace of this ServiceAccount must match the namespace of the tanzu package install command in the next step. rbac.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 --- apiVersion : v1 kind : ServiceAccount metadata : name : elasticache-install --- kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : name : elasticache-install rules : - apiGroups : [ \"elasticache.services.k8s.aws\" ] resources : [ \"*\" ] verbs : [ \"*\" ] - apiGroups : [ \"secretgen.carvel.dev\" , \"secretgen.k14s.io\" ] resources : [ \"secrettemplates\" , \"passwords\" ] verbs : [ \"*\" ] - apiGroups : [ \"\" ] resources : [ \"serviceaccounts\" , \"configmaps\" ] verbs : [ \"*\" ] - apiGroups : [ \"\" ] resources : [ \"namespaces\" ] verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"rbac.authorization.k8s.io\" ] resources : [ \"roles\" , \"rolebindings\" ] verbs : [ \"*\" ] --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : elasticache-install subjects : - kind : ServiceAccount name : elasticache-install roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : elasticache-install kubectl apply -f rbac.yaml Create an AWS Elasticache instance through the Tanzu CLI \u00b6 In order to configure the package installation, you must provide a values file. Here are some values highlighted: namespace is the namespace where to deploy the AWS resources to, it may differ from the one dedicated to the package(s). The following example uses the service-instances namespace, do make sure it exists or set createNamespace: true . cacheSubnetGroupName is the name of the AWS CacheSubnetGroup to use for deploying the Elasticache instance. If it doesn't exist it can be created as part of the package setting the createCacheSubnetGroup flag to true and providing the subnetIDs list. vpcSecurityGroupIDs is a mandatory list of security group IDs that will be associated to the Elasticache instances and will filter network traffic to/from them. Warning Because of the ephemeral nature of such IDs, if the security groups are destroyed and re-created, the package will need to be updated with the new values, otherwise Elasticache instances will become unreachable. It is recommended to set the value of the name field below from redis to something unique, using only lowercase letters, digits and hyphens. Do make sure you also change the commands below using a redis value, such as the redis-writer-creds-bindable from the SecretTemplate, and replace redis with the actual name . Create a values file holding the configuration of the AWS Elasticache service instance: redis-instance-values.yml 1 2 3 4 5 6 7 8 9 10 --- name : redis namespace : service-instances cacheSubnetGroupName : redis-subnets replicasPerNodeGroup : 1 vpcSecurityGroupIDs : - sg-0a4ddae4fbf426cc8 tags : - key : Generator value : Carvel package Tip To understand which settings are available for this package you can run: tanzu package available get \\ --values-schema elasticache.aws.references.services.apps.tanzu.vmware.com/0.0.1-alpha This shows a list of all configuration options you can use in the redis-instance-values.yml file. Use the Tanzu CLI to install an instance of the reference service instance package. tanzu package install redis-instance \\ --package-name elasticache.aws.references.services.apps.tanzu.vmware.com \\ --version 0 .0.1-alpha \\ --service-account-name elasticache-install \\ --values-file redis-instance-values.yml \\ --wait You can install the elasticache.aws.references.services.apps.tanzu.vmware.com package multiple times to produce various AWS Elasticache instances. You create a separate <INSTANCE-NAME>-values.yml for each instance, set a different name value, and then install the package with the instance-specific data values file. Verify the AWS Resources \u00b6 Verify the creation status for the AWS Elasticache instance by inspecting the conditions in the Kubernetes API. To do so, run: kubectl -n service-instances get replicationgroups.elasticache.services.k8s.aws redis -o yaml After a few minutes, even up to 10 or more depending on how many replicas have been requested, you will be able to find the binding-compliant secrets produced by PackageInstall . Currently the package creates a reader and a writer user, each one with its own bindable secret. To view them, run: kubectl -n service-instances get secrettemplate redis-reader-creds-bindable -o jsonpath = \"{.status.secret.name}\" kubectl -n service-instances get secrettemplate redis-writer-creds-bindable -o jsonpath = \"{.status.secret.name}\" Verify the Service Instance \u00b6 First, wait until the Elasticache instance is ready. kubectl -n service-instances wait --for = condition = ACK.ResourceSynced = True replicationgroups.elasticache.services.k8s.aws ack-elasticache Next, ensure a bindable Secret was produced by the SecretTemplate . To do so, run: kubectl -n service-instances wait --for = condition = ReconcileSucceeded = True --timeout = 5m secrettemplate redis-reader-creds-bindable kubectl -n service-instances get secret -n default redis-reader-creds-bindable The same applies to the redis-writer-creds-bindable resources. Summary \u00b6 You have learnt to use Carvel's Package and PackageInstall APIs to create an AWS Elasticache instance. If you want to learn more about the pieces that comprise this service instance package, see Creating AWS Elasticache Instances manually using kubectl . Now that you have this available in the cluster, you can learn how to make use of it by continuing where you left off in Consuming AWS Elasticache with ACK .","title":"Creating AWS Elasticache instances by using a Carvel package (experimental)"},{"location":"usecases/aws/packages/elasticache/ack/package/#add-a-reference-package-repository-to-the-cluster","text":"The namespace tanzu-package-repo-global has a special significance. The kapp-controller defines a Global Packaging namespace. In this namespace, any package that is made available through a Package Repository is available in every namespace. When the kapp-controller is installed via Tanzu Application Platform, the namespace is tanzu-package-repo-global . If you install the controller in another way, verify which namespace is considered the Global Packaging namespace. You can use the following command to get the global namespace: GLOBAL_NAMESPACE = $( kubectl -n kapp-controller get deployment kapp-controller -o json | jq -r '.spec.template.spec.containers[]|select(.name==\"kapp-controller\").args[]|select(.|startswith(\"-packaging-global-namespace\"))|split(\"=\")[1]' ) To add a reference package repository to the cluster: Use the Tanzu CLI to add the new Service Reference packages repository: tanzu package repository add tap-reference-service-packages \\ --url ghcr.io/vmware-tanzu/tanzu-application-platform-reference-service-packages:0.0.3 \\ -n ${ GLOBAL_NAMESPACE } Create a ServiceAccount to provision PackageInstall resources by using the following example. The namespace of this ServiceAccount must match the namespace of the tanzu package install command in the next step. rbac.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 --- apiVersion : v1 kind : ServiceAccount metadata : name : elasticache-install --- kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : name : elasticache-install rules : - apiGroups : [ \"elasticache.services.k8s.aws\" ] resources : [ \"*\" ] verbs : [ \"*\" ] - apiGroups : [ \"secretgen.carvel.dev\" , \"secretgen.k14s.io\" ] resources : [ \"secrettemplates\" , \"passwords\" ] verbs : [ \"*\" ] - apiGroups : [ \"\" ] resources : [ \"serviceaccounts\" , \"configmaps\" ] verbs : [ \"*\" ] - apiGroups : [ \"\" ] resources : [ \"namespaces\" ] verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"rbac.authorization.k8s.io\" ] resources : [ \"roles\" , \"rolebindings\" ] verbs : [ \"*\" ] --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : elasticache-install subjects : - kind : ServiceAccount name : elasticache-install roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : elasticache-install kubectl apply -f rbac.yaml","title":"Add a reference package repository to the cluster"},{"location":"usecases/aws/packages/elasticache/ack/package/#create-an-aws-elasticache-instance-through-the-tanzu-cli","text":"In order to configure the package installation, you must provide a values file. Here are some values highlighted: namespace is the namespace where to deploy the AWS resources to, it may differ from the one dedicated to the package(s). The following example uses the service-instances namespace, do make sure it exists or set createNamespace: true . cacheSubnetGroupName is the name of the AWS CacheSubnetGroup to use for deploying the Elasticache instance. If it doesn't exist it can be created as part of the package setting the createCacheSubnetGroup flag to true and providing the subnetIDs list. vpcSecurityGroupIDs is a mandatory list of security group IDs that will be associated to the Elasticache instances and will filter network traffic to/from them. Warning Because of the ephemeral nature of such IDs, if the security groups are destroyed and re-created, the package will need to be updated with the new values, otherwise Elasticache instances will become unreachable. It is recommended to set the value of the name field below from redis to something unique, using only lowercase letters, digits and hyphens. Do make sure you also change the commands below using a redis value, such as the redis-writer-creds-bindable from the SecretTemplate, and replace redis with the actual name . Create a values file holding the configuration of the AWS Elasticache service instance: redis-instance-values.yml 1 2 3 4 5 6 7 8 9 10 --- name : redis namespace : service-instances cacheSubnetGroupName : redis-subnets replicasPerNodeGroup : 1 vpcSecurityGroupIDs : - sg-0a4ddae4fbf426cc8 tags : - key : Generator value : Carvel package Tip To understand which settings are available for this package you can run: tanzu package available get \\ --values-schema elasticache.aws.references.services.apps.tanzu.vmware.com/0.0.1-alpha This shows a list of all configuration options you can use in the redis-instance-values.yml file. Use the Tanzu CLI to install an instance of the reference service instance package. tanzu package install redis-instance \\ --package-name elasticache.aws.references.services.apps.tanzu.vmware.com \\ --version 0 .0.1-alpha \\ --service-account-name elasticache-install \\ --values-file redis-instance-values.yml \\ --wait You can install the elasticache.aws.references.services.apps.tanzu.vmware.com package multiple times to produce various AWS Elasticache instances. You create a separate <INSTANCE-NAME>-values.yml for each instance, set a different name value, and then install the package with the instance-specific data values file.","title":"Create an AWS Elasticache instance through the Tanzu CLI"},{"location":"usecases/aws/packages/elasticache/ack/package/#verify-the-aws-resources","text":"Verify the creation status for the AWS Elasticache instance by inspecting the conditions in the Kubernetes API. To do so, run: kubectl -n service-instances get replicationgroups.elasticache.services.k8s.aws redis -o yaml After a few minutes, even up to 10 or more depending on how many replicas have been requested, you will be able to find the binding-compliant secrets produced by PackageInstall . Currently the package creates a reader and a writer user, each one with its own bindable secret. To view them, run: kubectl -n service-instances get secrettemplate redis-reader-creds-bindable -o jsonpath = \"{.status.secret.name}\" kubectl -n service-instances get secrettemplate redis-writer-creds-bindable -o jsonpath = \"{.status.secret.name}\"","title":"Verify the AWS Resources"},{"location":"usecases/aws/packages/elasticache/ack/package/#verify-the-service-instance","text":"First, wait until the Elasticache instance is ready. kubectl -n service-instances wait --for = condition = ACK.ResourceSynced = True replicationgroups.elasticache.services.k8s.aws ack-elasticache Next, ensure a bindable Secret was produced by the SecretTemplate . To do so, run: kubectl -n service-instances wait --for = condition = ReconcileSucceeded = True --timeout = 5m secrettemplate redis-reader-creds-bindable kubectl -n service-instances get secret -n default redis-reader-creds-bindable The same applies to the redis-writer-creds-bindable resources.","title":"Verify the Service Instance"},{"location":"usecases/aws/packages/elasticache/ack/package/#summary","text":"You have learnt to use Carvel's Package and PackageInstall APIs to create an AWS Elasticache instance. If you want to learn more about the pieces that comprise this service instance package, see Creating AWS Elasticache Instances manually using kubectl . Now that you have this available in the cluster, you can learn how to make use of it by continuing where you left off in Consuming AWS Elasticache with ACK .","title":"Summary"},{"location":"usecases/aws/packages/elasticache/ack/troubleshooting/","text":"Delete default user before group is deleted \u00b6 If a user named default is issued a delete command before the usergroup it belongs to has actually been deleted, ACK returns an unrecoverable error, thus preventing the deletion process to complete successfully. - message : \"DefaultUserAssociatedToUserGroup: User is associated to user group(s) as a default user and can't be deleted.\\n\\tstatus code: 400, request id: 65cb3646-e20f-4ba6-96c2-bf38ce13f78e\" status : \"True\" type : ACK.Terminal","title":"Troubleshooting AWS ACK"},{"location":"usecases/aws/packages/elasticache/ack/troubleshooting/#delete-default-user-before-group-is-deleted","text":"If a user named default is issued a delete command before the usergroup it belongs to has actually been deleted, ACK returns an unrecoverable error, thus preventing the deletion process to complete successfully. - message : \"DefaultUserAssociatedToUserGroup: User is associated to user group(s) as a default user and can't be deleted.\\n\\tstatus code: 400, request id: 65cb3646-e20f-4ba6-96c2-bf38ce13f78e\" status : \"True\" type : ACK.Terminal","title":"Delete default user before group is deleted"},{"location":"usecases/aws/prerequisites/","text":"This is a list of prerequisites that must be satisfied in order to deploy a TAP Reference Package on AWS EKS and consume it through TAP : Install ytt , a templating tool for YAML that is being widely used in these guides. Install the AWS CLI. For how to do so, see the AWS documentation . Log into AWS with your own credentials and assume a role with proper permissions to deal with EKS and Elasticache services. Check your AWS account number, user and assumed role running: aws sts get-caller-identity Make sure you have an available EKS cluster and the related OIDC provider configured. In order to create a new one you can follow this guide . Install Tanzu Application Platform v1.2.0 or later and Cluster Essentials v1.2.0 or later on the Kubernetes cluster. For more information, see Installing Tanzu Application Platform . Verify that you have the appropriate versions by running: kubectl api-resources | grep secrettemplate This command returns the SecretTemplate API. If it does not work for you, you might not have Cluster Essentials for VMware Tanzu v1.2.0 or later installed. Only if you want to deploy reference packages based on ACK : install the AWS Controller for Kubernetes (ACK) for the service(s) you are going to consume on AWS. Only if you want to deploy reference packages based on Crossplane : install Crossplane and the AWS provider.","title":"AWS prerequisites"},{"location":"usecases/aws/prerequisites/ack/","text":"AWS Controllers for Kubernetes (ACK) lets you define and use AWS service resources directly from Kubernetes, by mapping AWS services to Kubernetes CRDs. There's a number of different controllers to manage different AWS services, with different levels of maturity, listed in the documentation . The following example shows how to install the Elasticache controller, but the same concept applies to all of them. Install ElastiCache Controller \u00b6 SERVICE = \"elasticache\" RELEASE_VERSION = ` curl -sL https://api.github.com/repos/aws-controllers-k8s/ $SERVICE -controller/releases/latest | grep '\"tag_name\":' | cut -d '\"' -f4 ` ACK_SYSTEM_NAMESPACE = \"ack-system\" AWS_REGION = \"eu-central-1\" aws ecr-public get-login-password --region us-east-1 | \\ helm registry login --username AWS --password-stdin public.ecr.aws helm install \\ ack- $SERVICE -controller \\ oci://public.ecr.aws/aws-controllers-k8s/ $SERVICE -chart \\ --create-namespace \\ --namespace $ACK_SYSTEM_NAMESPACE \\ --version = $RELEASE_VERSION \\ --set = aws.region = $AWS_REGION Warning The --region flag in the aws ecr-public get-login-password command must be set either to us-east-1 or us-west-2 , as described in ECR public AWS documentation . Set the AWS_REGION variable according to your needs and configure the IAM role for ACK's service account . If you followed the guide for creating the EKS cluster v1.23+ you should have already configured the OIDC provider for authentication, therefore you can skip to configuring the IAM role and policy for the service account .","title":"Install the AWS Controllers for Kubernetes (ACK)"},{"location":"usecases/aws/prerequisites/ack/#install-elasticache-controller","text":"SERVICE = \"elasticache\" RELEASE_VERSION = ` curl -sL https://api.github.com/repos/aws-controllers-k8s/ $SERVICE -controller/releases/latest | grep '\"tag_name\":' | cut -d '\"' -f4 ` ACK_SYSTEM_NAMESPACE = \"ack-system\" AWS_REGION = \"eu-central-1\" aws ecr-public get-login-password --region us-east-1 | \\ helm registry login --username AWS --password-stdin public.ecr.aws helm install \\ ack- $SERVICE -controller \\ oci://public.ecr.aws/aws-controllers-k8s/ $SERVICE -chart \\ --create-namespace \\ --namespace $ACK_SYSTEM_NAMESPACE \\ --version = $RELEASE_VERSION \\ --set = aws.region = $AWS_REGION Warning The --region flag in the aws ecr-public get-login-password command must be set either to us-east-1 or us-west-2 , as described in ECR public AWS documentation . Set the AWS_REGION variable according to your needs and configure the IAM role for ACK's service account . If you followed the guide for creating the EKS cluster v1.23+ you should have already configured the OIDC provider for authentication, therefore you can skip to configuring the IAM role and policy for the service account .","title":"Install ElastiCache Controller"},{"location":"usecases/aws/prerequisites/eks/","text":"An EKS cluster can be created in a number of ways, including AWS console, CLI, Terraform or CloudFormation. The quickest and simplest way to create an EKS cluster is to use eksctl , a CloudFormation wrapper, that will be used in this guide. Define the environment parameters \u00b6 The following are the parameters needed for the commands in this guide, which must be set the values that match your environment and needs. # AWS region you're operating in export AWS_REGION = \"eu-central-1\" # autoscaling group minimum number of nodes ASG_MIN_NODES = \"2\" # autoscaling group maximum number of nodes ASG_MAX_NODES = \"4\" # EKS cluster name CLUSTER_NAME = \"my-eks-cluster\" # EKS kubernetes version to deploy KUBERNETES_VERSION = \"1.23\" Info The AWS_REGION variable must be an environment variable (thus the export ) to be used by eksctl and aws commands. The other variables can be just shell variables. Tip A list of available kubernetes versions for the KUBERNETES_VERSION variable can be obtained running aws eks describe-addon-versions --query \"addons[].addonVersions[].compatibilities[].clusterVersion\" | jq 'unique|sort' Create the EKS cluster \u00b6 The following command can create an EKS cluster based on the parameters defined above: eksctl create cluster -m ${ ASG_MIN_NODES } -M ${ ASG_MAX_NODES } -n ${ CLUSTER_NAME } --version ${ KUBERNETES_VERSION } The previous command waits until the cluster is created and also updates the KUBECONFIG file with the details of the new cluster. Configure the EBS CSI controller \u00b6 From version 1.23 onwards it is necessary to create the addon for the EBS CSI driver. aws eks create-addon --cluster-name ${ CLUSTER_NAME } --addon-name aws-ebs-csi-driver EKS pods' service accounts can assume AWS IAM roles to be able to authenticate and interact with the AWS APIs. They are mapped to web identities via an IAM OIDC provider that must be created for the EKS cluster. You will then need to create a proper role for the EBS CSI controller to be able to create and manage EBS volumes. # define variables for IAM ACCOUNT_ID = $( aws sts get-caller-identity --query Account --output text ) OIDC_ID = $( aws eks describe-cluster --name ${ CLUSTER_NAME } --output text --query \"cluster.identity.oidc.issuer\" | cut -d/ -f3- ) EBS_ROLE = AmazonEKS_EBS_CSI_Driver- ${ CLUSTER_NAME } ROLE_TRUST_POLICY = $( mktemp ) ROLE_PERMISSION_POLICY = $( mktemp ) # prepare the trust policy document cat > ${ ROLE_TRUST_POLICY } <<EOF { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_ID}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_ID}:sub\": \"system:serviceaccount:kube-system:ebs-csi-controller-sa\" } } }] } EOF # create the role for the EBS CSI controller with the proper trust policy aws iam create-role --role-name ${ EBS_ROLE } --assume-role-policy-document file:// ${ ROLE_TRUST_POLICY } # fetch a sample permission policy document curl -sSfL -o ${ ROLE_PERMISSION_POLICY } https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/master/docs/example-iam-policy.json # create the permission policy PERMISSION_POLICY_ARN = $( aws iam create-policy --policy-name ${ EBS_ROLE } --policy-document file:// ${ ROLE_PERMISSION_POLICY } --query Policy.Arn --output text ) # attach the policy to the role aws iam attach-role-policy --policy-arn ${ PERMISSION_POLICY_ARN } --role-name ${ EBS_ROLE } # clean up temporary files rm ${ ROLE_TRUST_POLICY } rm ${ ROLE_PERMISSION_POLICY } Next, you must prepare the OIDC provider (see also AWS documentation ). If the EKS cluster has just been created the OIDC provider does not exist yet, otherwise you can run the following command to make sure aws iam list-open-id-connect-providers | grep $OIDC_ID If no output is returned, the OIDC provider does not exist and you must create it. Option 1 (quick) Option 2 (know what you are doing) The eksctl does the heavy lifting for you and creates the OIDC provider with the proper configuration for your EKS cluster. eksctl utils associate-iam-oidc-provider --cluster ${ CLUSTER_NAME } --approve Get rid of the eksctl magic and configure the OIDC provider yourself. Get the SHA1 fingerprint for validating the OIDC provider certificate: OIDC_HOST = $( echo $OIDC_ID | cut -d/ -f1 ) SHA1_FINGERPRINT = $( { echo | openssl s_client -connect ${ OIDC_HOST } :443 -servername ${ OIDC_HOST } -showcerts | openssl x509 -fingerprint -noout -sha1 } 2 >/dev/null | cut -d = -f2 | sed s/://g ) Create the OIDC provider: aws iam create-open-id-connect-provider --url ${ OIDC_URL } --thumbprint-list ${ SHA1_FINGERPRINT } --client-id-list sts.amazonaws.com You must then configure the Kubernetes service account to assume the role # annotate the controller service account with the new role ARN kubectl -n kube-system annotate serviceaccount ebs-csi-controller-sa eks.amazonaws.com/role-arn = arn:aws:iam:: ${ ACCOUNT_ID } :role/ ${ EBS_ROLE } # restart ebs-csi-controller pods kubectl -n kube-system rollout restart deployment ebs-csi-controller Your EKS cluster is now configured to dynamically allocate EBS volumes. You need to create the proper StorageClass resources for your PersistentVolumeClaims to use, like in this example .","title":"Create an EKS cluster with EBS CSI driver"},{"location":"usecases/aws/prerequisites/eks/#define-the-environment-parameters","text":"The following are the parameters needed for the commands in this guide, which must be set the values that match your environment and needs. # AWS region you're operating in export AWS_REGION = \"eu-central-1\" # autoscaling group minimum number of nodes ASG_MIN_NODES = \"2\" # autoscaling group maximum number of nodes ASG_MAX_NODES = \"4\" # EKS cluster name CLUSTER_NAME = \"my-eks-cluster\" # EKS kubernetes version to deploy KUBERNETES_VERSION = \"1.23\" Info The AWS_REGION variable must be an environment variable (thus the export ) to be used by eksctl and aws commands. The other variables can be just shell variables. Tip A list of available kubernetes versions for the KUBERNETES_VERSION variable can be obtained running aws eks describe-addon-versions --query \"addons[].addonVersions[].compatibilities[].clusterVersion\" | jq 'unique|sort'","title":"Define the environment parameters"},{"location":"usecases/aws/prerequisites/eks/#create-the-eks-cluster","text":"The following command can create an EKS cluster based on the parameters defined above: eksctl create cluster -m ${ ASG_MIN_NODES } -M ${ ASG_MAX_NODES } -n ${ CLUSTER_NAME } --version ${ KUBERNETES_VERSION } The previous command waits until the cluster is created and also updates the KUBECONFIG file with the details of the new cluster.","title":"Create the EKS cluster"},{"location":"usecases/aws/prerequisites/eks/#configure-the-ebs-csi-controller","text":"From version 1.23 onwards it is necessary to create the addon for the EBS CSI driver. aws eks create-addon --cluster-name ${ CLUSTER_NAME } --addon-name aws-ebs-csi-driver EKS pods' service accounts can assume AWS IAM roles to be able to authenticate and interact with the AWS APIs. They are mapped to web identities via an IAM OIDC provider that must be created for the EKS cluster. You will then need to create a proper role for the EBS CSI controller to be able to create and manage EBS volumes. # define variables for IAM ACCOUNT_ID = $( aws sts get-caller-identity --query Account --output text ) OIDC_ID = $( aws eks describe-cluster --name ${ CLUSTER_NAME } --output text --query \"cluster.identity.oidc.issuer\" | cut -d/ -f3- ) EBS_ROLE = AmazonEKS_EBS_CSI_Driver- ${ CLUSTER_NAME } ROLE_TRUST_POLICY = $( mktemp ) ROLE_PERMISSION_POLICY = $( mktemp ) # prepare the trust policy document cat > ${ ROLE_TRUST_POLICY } <<EOF { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_ID}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_ID}:sub\": \"system:serviceaccount:kube-system:ebs-csi-controller-sa\" } } }] } EOF # create the role for the EBS CSI controller with the proper trust policy aws iam create-role --role-name ${ EBS_ROLE } --assume-role-policy-document file:// ${ ROLE_TRUST_POLICY } # fetch a sample permission policy document curl -sSfL -o ${ ROLE_PERMISSION_POLICY } https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/master/docs/example-iam-policy.json # create the permission policy PERMISSION_POLICY_ARN = $( aws iam create-policy --policy-name ${ EBS_ROLE } --policy-document file:// ${ ROLE_PERMISSION_POLICY } --query Policy.Arn --output text ) # attach the policy to the role aws iam attach-role-policy --policy-arn ${ PERMISSION_POLICY_ARN } --role-name ${ EBS_ROLE } # clean up temporary files rm ${ ROLE_TRUST_POLICY } rm ${ ROLE_PERMISSION_POLICY } Next, you must prepare the OIDC provider (see also AWS documentation ). If the EKS cluster has just been created the OIDC provider does not exist yet, otherwise you can run the following command to make sure aws iam list-open-id-connect-providers | grep $OIDC_ID If no output is returned, the OIDC provider does not exist and you must create it. Option 1 (quick) Option 2 (know what you are doing) The eksctl does the heavy lifting for you and creates the OIDC provider with the proper configuration for your EKS cluster. eksctl utils associate-iam-oidc-provider --cluster ${ CLUSTER_NAME } --approve Get rid of the eksctl magic and configure the OIDC provider yourself. Get the SHA1 fingerprint for validating the OIDC provider certificate: OIDC_HOST = $( echo $OIDC_ID | cut -d/ -f1 ) SHA1_FINGERPRINT = $( { echo | openssl s_client -connect ${ OIDC_HOST } :443 -servername ${ OIDC_HOST } -showcerts | openssl x509 -fingerprint -noout -sha1 } 2 >/dev/null | cut -d = -f2 | sed s/://g ) Create the OIDC provider: aws iam create-open-id-connect-provider --url ${ OIDC_URL } --thumbprint-list ${ SHA1_FINGERPRINT } --client-id-list sts.amazonaws.com You must then configure the Kubernetes service account to assume the role # annotate the controller service account with the new role ARN kubectl -n kube-system annotate serviceaccount ebs-csi-controller-sa eks.amazonaws.com/role-arn = arn:aws:iam:: ${ ACCOUNT_ID } :role/ ${ EBS_ROLE } # restart ebs-csi-controller pods kubectl -n kube-system rollout restart deployment ebs-csi-controller Your EKS cluster is now configured to dynamically allocate EBS volumes. You need to create the proper StorageClass resources for your PersistentVolumeClaims to use, like in this example .","title":"Configure the EBS CSI controller"},{"location":"usecases/azure/packages/mongodb/crossplane/","text":"This guide describes using the Services Toolkit to allow Tanzu Application Platform workloads to consume Azure MongoDB (via CosmosDB). This particular topic makes use of Universal Crossplane (UXP) by UpBound to manage resources in Azure. Prerequisites \u00b6 UpBound's Universal Crossplane installed Tanzu Application Platform (TAP) 1.3.0 or higher installed Tanzu CLI with TAP plugins Note To test the automated binding of external services to applications, you need TAP installed. In addition, you need the Services Toolkit(STK) resources ( ClusterInstanceClass , Claim ), created. If you want to test the Crossplane bindings without TAP, skip the sections creating STK resources. Instead of creating a TAP workload, we have an example Deployment (the Kubernetes CR), The expected secret is hard coded in the section Test Claim Without TAP . Create service instances that are compatible with Tanzu Application Platform \u00b6 For the sake of creating a MongoDB instance, we need to construct the following resources: Account MongoDatabase MongoCollection Secret containing the connection details in a supported format (Service Bindings) We also recommend, especially while testing or doing a PoC, creating a specific Resource Group for these resources. Success To instantiate the resources, we can either directly create the Crossplane instances , or we can use a Crossplane package that uses Crossplane's Compositions . Once your path of choice has reached the point the resources are created, it will redirect you back here. Once Resources Are Ready \u00b6 Once we have the MongoDB instance and the appropriate secret (adhering to Service Bindings), we can create a (STK) Claim . We create the following: ClusterInstanceClass Reader ClusterRole for Services Toolkit ResourceClaim TAP workload to test the database instance Create a Cluster Instance Class for Azure MongoDB \u00b6 We will create a generic class of bindable resources. So applications can be made aware of the specific name of the resources, or their bindable secret, to claim them. cluster-instance-class.yml echo \"apiVersion: services.apps.tanzu.vmware.com/v1alpha1 kind: ClusterInstanceClass metadata: name: azure-mongodb spec: description: short: azure mongodb pool: kind: Secret group: \\\"\\\" labelSelector: matchLabels: services.apps.tanzu.vmware.com/class: azure-mongodb \" > cluster-instance-class.yml kubectl apply -f cluster-instance-class.yml Services Toolkit Reader Cluster Role \u00b6 We make sure the STK controller can read the secrets anywhere. Feel free to limit this role to specific named resources via the resourceNames property. reader-cluster-role.yml echo \"apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: stk-secret-reader labels: servicebinding.io/controller: \\\"true\\\" rules: - apiGroups: - \\\"\\\" resources: - secrets verbs: - get - list - watch \" > reader-cluster-role.yml kubectl apply -f reader-cluster-role.yml Discover, Claim, and Bind to an Azure MongoDB instance \u00b6 First, confirm we have claimable class instances. export RESOURCE_NAMESPACE = ${ UNIQUE_NAME :- \"default\" } export RESOURCE_NAME = Hint If you use the instance path to create the resources, the RESOURCE_NAMESPACE will be $UNIQUE_NAME . Else, it will be default . If you used the instance path, RESOURCE_NAME is also $UNIQUE_NAME , else if you use the package path, it is $INSTANCE_NAME . tanzu services claimable list --class azure-mongodb \\ -n ${ RESOURCE_NAMESPACE } Which should yield: NAME NAMESPACE KIND APIVERSION trp-mongodb-docs-test-bindable trp-mongodb-docs-test Secret v1 We have to create the claim in the namespace of the consuming application. So set this variable to the namespace where you create TAP workloads. export TAP_DEV_NAMESPACE = default We then create the associated resource claim: Using Tanzu CLI Using Kubernetes Manifest tanzu service claim create azure-mongodb-claim-01 \\ --namespace ${ TAP_DEV_NAMESPACE } \\ --resource-name ${ RESOURCE_NAME } -bindable \\ --resource-namespace ${ RESOURCE_NAMESPACE } \\ --resource-kind Secret \\ --resource-api-version v1 resource-claim.yml echo \"apiVersion: services.apps.tanzu.vmware.com/v1alpha1 kind: ResourceClaim metadata: name: azure-mongodb-claim-01 namespace: ${ TAP_DEV_NAMESPACE } spec: ref: apiVersion: v1 kind: Secret name: ${ RESOURCE_NAME } -bindable namespace: ${ RESOURCE_NAMESPACE } \" > resource-claim.yml kubectl apply -f resource-claim.yml To verify your claim is ready, you run this command: tanzu service claim list -o wide Which should yield the following: NAME READY REASON CLAIM REF azure-mongodb-claim-01 True Ready services.apps.tanzu.vmware.com/v1alpha1:ResourceClaim:azure-mongodb-claim-01 You can now claim it with a TAP workload. Test Claim With TAP Workload \u00b6 We can now create a TAP workload that uses our resource claim. The runtime will fail once or twice as it takes time for the secret with the connection details to be mounted into the container. So wait for deployment-0002 or 0003 . Tanzu CLI Kubernetes Manifest tanzu apps workload create spring-boot-mongo-01 \\ --namespace ${ TAP_DEV_NAMESPACE } \\ --git-repo https://github.com/joostvdg/spring-boot-mongo.git \\ --git-branch main \\ --type web \\ --label app.kubernetes.io/part-of = spring-boot-mongo-01 \\ --annotation autoscaling.knative.dev/minScale = 1 \\ --build-env BP_JVM_VERSION = 17 \\ --service-ref db = services.apps.tanzu.vmware.com/v1alpha1:ResourceClaim:azure-mongodb-claim-01 \\ --yes echo \"apiVersion: carto.run/v1alpha1 kind: Workload metadata: labels: app.kubernetes.io/part-of: spring-boot-mongo-01 apps.tanzu.vmware.com/workload-type: web name: spring-boot-mongo-01 namespace: ${ TAP_DEV_NAMESPACE } spec: build: env: - name: BP_JVM_VERSION value: \\\"17\\\" params: - name: annotations value: autoscaling.knative.dev/minScale: \\\"1\\\" serviceClaims: - name: db ref: apiVersion: services.apps.tanzu.vmware.com/v1alpha1 kind: ResourceClaim name: azure-mongodb-claim-01 source: git: ref: branch: main url: https://github.com/joostvdg/spring-boot-mongo.git \" > workload.yml kubectl apply -f workload.yml To see the logs: tanzu apps workload tail spring-boot-mongo-01 To get the status: tanzu apps workload get spring-boot-mongo-01 Tap creates the deployment when the build and config writer workflows are complete. To see their pods: kubectl get pod -l app.kubernetes.io/part-of = spring-boot-mongo-01 Then you can wait for the application to be ready via the kubectl CLI. kubectl wait --for = condition = ready \\ pod -l app.kubernetes.io/component = run,app.kubernetes.io/part-of = spring-boot-mongo-01 \\ --timeout = 180s \\ --namespace ${ TAP_DEV_NAMESPACE } Ensure there is only one deployment active: kubectl get pod --namespace ${ TAP_DEV_NAMESPACE } \\ -l app.kubernetes.io/component = run,app.kubernetes.io/part-of = spring-boot-mongo-01 Which should list a single deployment with Ready 2/2: NAME READY STATUS RESTARTS AGE spring-boot-mongo-01-00002-deployment-8cd56bdc8-gb44n 2 /2 Running 0 6m11s We then collect the name of the **Pod**or copy it yourself from the command-line output. POD_NAME = $( kubectl get pod \\ --namespace ${ TAP_DEV_NAMESPACE } \\ -l app.kubernetes.io/component = run,app.kubernetes.io/part-of = spring-boot-mongo-01 \\ -o jsonpath = \"{.items[0].metadata.name}\" ) Continue with testing the application Test Claim Without TAP \u00b6 Here's the same application we use as TAP workload, but with a hardcoded Deployment . kubectl apply -f https://raw.githubusercontent.com/joostvdg/spring-boot-mongo/main/kubernetes/raw/deployment.yaml As the Deployment is hard coded, it doesn't know the name of your bindable secret. So let's patch that; first, create a patch file. spring-boot-mongo-deployment-patch.yml echo \" --- spec: template: spec: volumes: - name: secret-volume projected: defaultMode: 420 sources: - secret: name: \" ${ RESOURCE_NAME } -bindable \" \" > spring-boot-mongo-deployment-patch.yml And second, patch the deployment to update the secret name: kubectl patch --type merge deploy spring-boot-mongo \\ --patch-file = 'spring-boot-mongo-deployment-patch.yml' \\ --namespace ${ TAP_DEV_NAMESPACE } We wait for the Pod to become Ready . kubectl wait --for = condition = ready pod \\ --namespace ${ TAP_DEV_NAMESPACE } \\ -l app.kubernetes.io/name = spring-boot-mongo \\ --timeout = 60s Once that happens, we copy the Pod 's name to test the application. POD_NAME = $( kubectl get pod \\ --namespace ${ TAP_DEV_NAMESPACE } \\ -l app.kubernetes.io/name = spring-boot-mongo \\ -o jsonpath = \"{.items[0].metadata.name}\" ) Test Application \u00b6 Use the kubectl CLI to create a port forward. kubectl port-forward pod/ ${ POD_NAME } --namespace ${ TAP_DEV_NAMESPACE } 8080 And then open another terminal to test the application: curl -s \"http://localhost:8080\" Which should return an empty list [] . Add a value that gets stored in the MongoDB instance. curl --header \"Content-Type: application/json\" \\ --request POST --data '{\"name\":\"Alice\"}' \\ http://localhost:8080/create Making another GET request should return the stored entry: curl -s \"http://localhost:8080\" Success We have gone through all the steps. You can now clean up all the resources. Cleanup \u00b6 kubectl delete ResourceClaim azure-mongodb-claim-01 \\ --namespace ${ TAP_DEV_NAMESPACE } TAP Workload Deployment without TAP kubectl delete workload spring-boot-mongo-01 kubectl delete deploy spring-boot-mongo || true Once you have cleaned up the resources related to this guide, visit the specific sub-guides for their cleanup commands. Via Crossplane instances cleanup commands Via Crossplane package cleanup commands","title":"Consume MongoDB via Crossplane"},{"location":"usecases/azure/packages/mongodb/crossplane/#prerequisites","text":"UpBound's Universal Crossplane installed Tanzu Application Platform (TAP) 1.3.0 or higher installed Tanzu CLI with TAP plugins Note To test the automated binding of external services to applications, you need TAP installed. In addition, you need the Services Toolkit(STK) resources ( ClusterInstanceClass , Claim ), created. If you want to test the Crossplane bindings without TAP, skip the sections creating STK resources. Instead of creating a TAP workload, we have an example Deployment (the Kubernetes CR), The expected secret is hard coded in the section Test Claim Without TAP .","title":"Prerequisites"},{"location":"usecases/azure/packages/mongodb/crossplane/#create-service-instances-that-are-compatible-with-tanzu-application-platform","text":"For the sake of creating a MongoDB instance, we need to construct the following resources: Account MongoDatabase MongoCollection Secret containing the connection details in a supported format (Service Bindings) We also recommend, especially while testing or doing a PoC, creating a specific Resource Group for these resources. Success To instantiate the resources, we can either directly create the Crossplane instances , or we can use a Crossplane package that uses Crossplane's Compositions . Once your path of choice has reached the point the resources are created, it will redirect you back here.","title":"Create service instances that are compatible with Tanzu Application Platform"},{"location":"usecases/azure/packages/mongodb/crossplane/#once-resources-are-ready","text":"Once we have the MongoDB instance and the appropriate secret (adhering to Service Bindings), we can create a (STK) Claim . We create the following: ClusterInstanceClass Reader ClusterRole for Services Toolkit ResourceClaim TAP workload to test the database instance","title":"Once Resources Are Ready"},{"location":"usecases/azure/packages/mongodb/crossplane/#create-a-cluster-instance-class-for-azure-mongodb","text":"We will create a generic class of bindable resources. So applications can be made aware of the specific name of the resources, or their bindable secret, to claim them. cluster-instance-class.yml echo \"apiVersion: services.apps.tanzu.vmware.com/v1alpha1 kind: ClusterInstanceClass metadata: name: azure-mongodb spec: description: short: azure mongodb pool: kind: Secret group: \\\"\\\" labelSelector: matchLabels: services.apps.tanzu.vmware.com/class: azure-mongodb \" > cluster-instance-class.yml kubectl apply -f cluster-instance-class.yml","title":"Create a Cluster Instance Class for Azure MongoDB"},{"location":"usecases/azure/packages/mongodb/crossplane/#services-toolkit-reader-cluster-role","text":"We make sure the STK controller can read the secrets anywhere. Feel free to limit this role to specific named resources via the resourceNames property. reader-cluster-role.yml echo \"apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: stk-secret-reader labels: servicebinding.io/controller: \\\"true\\\" rules: - apiGroups: - \\\"\\\" resources: - secrets verbs: - get - list - watch \" > reader-cluster-role.yml kubectl apply -f reader-cluster-role.yml","title":"Services Toolkit Reader Cluster Role"},{"location":"usecases/azure/packages/mongodb/crossplane/#discover-claim-and-bind-to-an-azure-mongodb-instance","text":"First, confirm we have claimable class instances. export RESOURCE_NAMESPACE = ${ UNIQUE_NAME :- \"default\" } export RESOURCE_NAME = Hint If you use the instance path to create the resources, the RESOURCE_NAMESPACE will be $UNIQUE_NAME . Else, it will be default . If you used the instance path, RESOURCE_NAME is also $UNIQUE_NAME , else if you use the package path, it is $INSTANCE_NAME . tanzu services claimable list --class azure-mongodb \\ -n ${ RESOURCE_NAMESPACE } Which should yield: NAME NAMESPACE KIND APIVERSION trp-mongodb-docs-test-bindable trp-mongodb-docs-test Secret v1 We have to create the claim in the namespace of the consuming application. So set this variable to the namespace where you create TAP workloads. export TAP_DEV_NAMESPACE = default We then create the associated resource claim: Using Tanzu CLI Using Kubernetes Manifest tanzu service claim create azure-mongodb-claim-01 \\ --namespace ${ TAP_DEV_NAMESPACE } \\ --resource-name ${ RESOURCE_NAME } -bindable \\ --resource-namespace ${ RESOURCE_NAMESPACE } \\ --resource-kind Secret \\ --resource-api-version v1 resource-claim.yml echo \"apiVersion: services.apps.tanzu.vmware.com/v1alpha1 kind: ResourceClaim metadata: name: azure-mongodb-claim-01 namespace: ${ TAP_DEV_NAMESPACE } spec: ref: apiVersion: v1 kind: Secret name: ${ RESOURCE_NAME } -bindable namespace: ${ RESOURCE_NAMESPACE } \" > resource-claim.yml kubectl apply -f resource-claim.yml To verify your claim is ready, you run this command: tanzu service claim list -o wide Which should yield the following: NAME READY REASON CLAIM REF azure-mongodb-claim-01 True Ready services.apps.tanzu.vmware.com/v1alpha1:ResourceClaim:azure-mongodb-claim-01 You can now claim it with a TAP workload.","title":"Discover, Claim, and Bind to an Azure MongoDB instance"},{"location":"usecases/azure/packages/mongodb/crossplane/#test-claim-with-tap-workload","text":"We can now create a TAP workload that uses our resource claim. The runtime will fail once or twice as it takes time for the secret with the connection details to be mounted into the container. So wait for deployment-0002 or 0003 . Tanzu CLI Kubernetes Manifest tanzu apps workload create spring-boot-mongo-01 \\ --namespace ${ TAP_DEV_NAMESPACE } \\ --git-repo https://github.com/joostvdg/spring-boot-mongo.git \\ --git-branch main \\ --type web \\ --label app.kubernetes.io/part-of = spring-boot-mongo-01 \\ --annotation autoscaling.knative.dev/minScale = 1 \\ --build-env BP_JVM_VERSION = 17 \\ --service-ref db = services.apps.tanzu.vmware.com/v1alpha1:ResourceClaim:azure-mongodb-claim-01 \\ --yes echo \"apiVersion: carto.run/v1alpha1 kind: Workload metadata: labels: app.kubernetes.io/part-of: spring-boot-mongo-01 apps.tanzu.vmware.com/workload-type: web name: spring-boot-mongo-01 namespace: ${ TAP_DEV_NAMESPACE } spec: build: env: - name: BP_JVM_VERSION value: \\\"17\\\" params: - name: annotations value: autoscaling.knative.dev/minScale: \\\"1\\\" serviceClaims: - name: db ref: apiVersion: services.apps.tanzu.vmware.com/v1alpha1 kind: ResourceClaim name: azure-mongodb-claim-01 source: git: ref: branch: main url: https://github.com/joostvdg/spring-boot-mongo.git \" > workload.yml kubectl apply -f workload.yml To see the logs: tanzu apps workload tail spring-boot-mongo-01 To get the status: tanzu apps workload get spring-boot-mongo-01 Tap creates the deployment when the build and config writer workflows are complete. To see their pods: kubectl get pod -l app.kubernetes.io/part-of = spring-boot-mongo-01 Then you can wait for the application to be ready via the kubectl CLI. kubectl wait --for = condition = ready \\ pod -l app.kubernetes.io/component = run,app.kubernetes.io/part-of = spring-boot-mongo-01 \\ --timeout = 180s \\ --namespace ${ TAP_DEV_NAMESPACE } Ensure there is only one deployment active: kubectl get pod --namespace ${ TAP_DEV_NAMESPACE } \\ -l app.kubernetes.io/component = run,app.kubernetes.io/part-of = spring-boot-mongo-01 Which should list a single deployment with Ready 2/2: NAME READY STATUS RESTARTS AGE spring-boot-mongo-01-00002-deployment-8cd56bdc8-gb44n 2 /2 Running 0 6m11s We then collect the name of the **Pod**or copy it yourself from the command-line output. POD_NAME = $( kubectl get pod \\ --namespace ${ TAP_DEV_NAMESPACE } \\ -l app.kubernetes.io/component = run,app.kubernetes.io/part-of = spring-boot-mongo-01 \\ -o jsonpath = \"{.items[0].metadata.name}\" ) Continue with testing the application","title":"Test Claim With TAP Workload"},{"location":"usecases/azure/packages/mongodb/crossplane/#test-claim-without-tap","text":"Here's the same application we use as TAP workload, but with a hardcoded Deployment . kubectl apply -f https://raw.githubusercontent.com/joostvdg/spring-boot-mongo/main/kubernetes/raw/deployment.yaml As the Deployment is hard coded, it doesn't know the name of your bindable secret. So let's patch that; first, create a patch file. spring-boot-mongo-deployment-patch.yml echo \" --- spec: template: spec: volumes: - name: secret-volume projected: defaultMode: 420 sources: - secret: name: \" ${ RESOURCE_NAME } -bindable \" \" > spring-boot-mongo-deployment-patch.yml And second, patch the deployment to update the secret name: kubectl patch --type merge deploy spring-boot-mongo \\ --patch-file = 'spring-boot-mongo-deployment-patch.yml' \\ --namespace ${ TAP_DEV_NAMESPACE } We wait for the Pod to become Ready . kubectl wait --for = condition = ready pod \\ --namespace ${ TAP_DEV_NAMESPACE } \\ -l app.kubernetes.io/name = spring-boot-mongo \\ --timeout = 60s Once that happens, we copy the Pod 's name to test the application. POD_NAME = $( kubectl get pod \\ --namespace ${ TAP_DEV_NAMESPACE } \\ -l app.kubernetes.io/name = spring-boot-mongo \\ -o jsonpath = \"{.items[0].metadata.name}\" )","title":"Test Claim Without TAP"},{"location":"usecases/azure/packages/mongodb/crossplane/#test-application","text":"Use the kubectl CLI to create a port forward. kubectl port-forward pod/ ${ POD_NAME } --namespace ${ TAP_DEV_NAMESPACE } 8080 And then open another terminal to test the application: curl -s \"http://localhost:8080\" Which should return an empty list [] . Add a value that gets stored in the MongoDB instance. curl --header \"Content-Type: application/json\" \\ --request POST --data '{\"name\":\"Alice\"}' \\ http://localhost:8080/create Making another GET request should return the stored entry: curl -s \"http://localhost:8080\" Success We have gone through all the steps. You can now clean up all the resources.","title":"Test Application"},{"location":"usecases/azure/packages/mongodb/crossplane/#cleanup","text":"kubectl delete ResourceClaim azure-mongodb-claim-01 \\ --namespace ${ TAP_DEV_NAMESPACE } TAP Workload Deployment without TAP kubectl delete workload spring-boot-mongo-01 kubectl delete deploy spring-boot-mongo || true Once you have cleaned up the resources related to this guide, visit the specific sub-guides for their cleanup commands. Via Crossplane instances cleanup commands Via Crossplane package cleanup commands","title":"Cleanup"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/","text":"To create a MongoDB instance in Azure, we need the following resources: ResourceGroup CosmosDB Account MongoDatabase MongoCollection We will show you how to create all of them with Crossplane's official Azure provider . Important We suggest defining a unique name for the resources to avoid potential conflicts. This is because we need the Resource Group and the CosmosDB Account names to be globally unique (within Azure). export UNIQUE_NAME = We must set a location where Azure creates the resources. As a default, we use West Europe , but feel free to change this. export LOCATION = \"West Europe\" The examples help you create the files with the UNIQUE_NAME and LOCATION values are inserted in the places that matter. Pre-requisites \u00b6 UpBound's Universal Crossplane Crossplane Azure Provider SecretGen Controller : to transform the secret from Crossplane to the Service Binding specification Verify Provider \u00b6 kubectl api-resources --api-group cosmosdb.azure.upbound.io This gives us the following APIs: NAME SHORTNAMES APIVERSION NAMESPACED KIND accounts cosmosdb.azure.upbound.io/v1beta1 false Account cassandraclusters cosmosdb.azure.upbound.io/v1beta1 false CassandraCluster cassandradatacenters cosmosdb.azure.upbound.io/v1beta1 false CassandraDatacenter cassandrakeyspaces cosmosdb.azure.upbound.io/v1beta1 false CassandraKeySpace cassandratables cosmosdb.azure.upbound.io/v1beta1 false CassandraTable gremlindatabases cosmosdb.azure.upbound.io/v1beta1 false GremlinDatabase gremlingraphs cosmosdb.azure.upbound.io/v1beta1 false GremlinGraph mongocollections cosmosdb.azure.upbound.io/v1beta1 false MongoCollection mongodatabases cosmosdb.azure.upbound.io/v1beta1 false MongoDatabase sqlcontainers cosmosdb.azure.upbound.io/v1beta1 false SQLContainer sqldatabases cosmosdb.azure.upbound.io/v1beta1 false SQLDatabase sqlfunctions cosmosdb.azure.upbound.io/v1beta1 false SQLFunction sqlroleassignments cosmosdb.azure.upbound.io/v1beta1 false SQLRoleAssignment sqlroledefinitions cosmosdb.azure.upbound.io/v1beta1 false SQLRoleDefinition sqlstoredprocedures cosmosdb.azure.upbound.io/v1beta1 false SQLStoredProcedure sqltriggers cosmosdb.azure.upbound.io/v1beta1 false SQLTrigger tables cosmosdb.azure.upbound.io/v1beta1 false Table We also recommend, especially while testing or doing a PoC, creating a specific Resource Group for these resources. The Resource Group API is part of the azure.upbound.io API group. kubectl api-resources --api-group azure.upbound.io As you can see: NAME SHORTNAMES APIVERSION NAMESPACED KIND providerconfigs azure.upbound.io/v1beta1 false ProviderConfig providerconfigusages azure.upbound.io/v1beta1 false ProviderConfigUsage resourcegroups azure.upbound.io/v1beta1 false ResourceGroup resourceproviderregistrations azure.upbound.io/v1beta1 false ResourceProviderRegistration storeconfigs azure.upbound.io/v1alpha1 false StoreConfig subscriptions azure.upbound.io/v1beta1 false Subscription Create Namespace \u00b6 There are some places where we need to provide the name of the namespace. For the convenience of this example, we create a namespace with the same name. kubectl create namespace ${ UNIQUE_NAME } Create Resource Group \u00b6 We create a ResourceGroup with a unique name and label it. This is because some of the managed resources (i.e., the resources in Azure) depend on other managed resources. To find them, we must instruct Crossplane, which Crossplane resource maps to the dependent managed resource. For this, we use the label. resourcegroup.yml echo \"apiVersion: azure.upbound.io/v1beta1 kind: ResourceGroup metadata: name: ${ UNIQUE_NAME } namespace: ${ UNIQUE_NAME } labels: testing.upbound.io/example-name: ${ UNIQUE_NAME } spec: forProvider: location: ${ LOCATION } providerConfigRef: name: default \" > resourcegroup.yml And now, you can apply the file to create the Crossplane resource. Again, the Crossplane controller and the Azure provider controller will ensure the resource in Azure exists and the status is visible on our Crossplane resource in Kubernetes. kubectl apply -f resourcegroup.yml Create CosmosDB Account \u00b6 Next on the list is the CosmosDB Account . This object defines the initial configuration of the MongoDB instance. You make this Account a MongoDB instance by setting the capability EnableMongo . If you want to set a specific version, use the spec.forProvider.mongoServerVersion parameter. account.yml echo \"apiVersion: cosmosdb.azure.upbound.io/v1beta1 kind: account metadata: annotations: meta.upbound.io/example-id: cosmosdb/v1beta1/mongocollection labels: testing.upbound.io/example-name: ${ UNIQUE_NAME } name: ${ UNIQUE_NAME } namespace: ${ UNIQUE_NAME } spec: forProvider: capabilities: - name: EnableMongo - name: mongoEnableDocLevelTTL consistencyPolicy: - consistencyLevel: Strong geoLocation: - failoverPriority: 0 location: ${ LOCATION } kind: MongoDB location: ${ LOCATION } offerType: Standard resourceGroupNameSelector: matchLabels: testing.upbound.io/example-name: ${ UNIQUE_NAME } writeConnectionSecretToRef: namespace: ${ UNIQUE_NAME } name: ${ UNIQUE_NAME } \" > account.yml Create the file and apply it to the cluster. kubectl apply -f account.yml Info The Account requires a reference to a Resource Group . We do so via the resourceGroupNameSelector : resourceGroupNameSelector : matchLabels : testing.upbound.io/example-name : ${UNIQUE_NAME} Create Mongo Database \u00b6 Now that we have an Account that manages a MongoDB instance, we can create a MongoDB Database in the Account. database.yml echo \"apiVersion: cosmosdb.azure.upbound.io/v1beta1 kind: MongoDatabase metadata: annotations: meta.upbound.io/example-id: cosmosdb/v1beta1/mongocollection labels: testing.upbound.io/example-name: ${ UNIQUE_NAME } name: ${ UNIQUE_NAME } namespace: ${ UNIQUE_NAME } spec: forProvider: accountNameSelector: matchLabels: testing.upbound.io/example-name: ${ UNIQUE_NAME } resourceGroupNameSelector: matchLabels: testing.upbound.io/example-name: ${ UNIQUE_NAME } \" > database.yml kubectl apply -f database.yml Create Mongo Collection \u00b6 In the MongoDB Database , we want to have at least one Collection ( read more about Database and Collection ). All the same rules as before apply. We define the resource's properties and then reference the dependencies. collection.yml echo \"apiVersion: cosmosdb.azure.upbound.io/v1beta1 kind: MongoCollection metadata: annotations: meta.upbound.io/example-id: cosmosdb/v1beta1/mongocollection labels: testing.upbound.io/example-name: ${ UNIQUE_NAME } name: ${ UNIQUE_NAME } namespace: ${ UNIQUE_NAME } spec: forProvider: defaultTtlSeconds: 777 index: - keys: - _id unique: true shardKey: uniqueKey throughput: 400 accountNameSelector: matchLabels: testing.upbound.io/example-name: ${ UNIQUE_NAME } databaseNameSelector: matchLabels: testing.upbound.io/example-name: ${ UNIQUE_NAME } resourceGroupNameSelector: matchLabels: testing.upbound.io/example-name: ${ UNIQUE_NAME } \" > collection.yml And as always, we apply the resource to the cluster and let Crossplane work its magic. kubectl apply -f collection.yml Verify Managed Resource Creation \u00b6 Create the Crossplane resources and then verify the resources have a successful reconciliation. kubectl get resourcegroup,mongocollection,mongodatabase,account \\ --namespace ${ UNIQUE_NAME } This should yield something like this: (where trp-cosmosdb-mongo-01 will be your $UNIQUE_NAME ) NAME READY SYNCED EXTERNAL-NAME AGE resourcegroup.azure.upbound.io/trp-cosmosdb-mongo-01 True True trp-cosmosdb-mongo-01 26h NAME READY SYNCED EXTERNAL-NAME AGE mongocollection.cosmosdb.azure.upbound.io/trp-cosmosdb-mongo-01 True True trp-cosmosdb-mongo-01 26h NAME READY SYNCED EXTERNAL-NAME AGE mongodatabase.cosmosdb.azure.upbound.io/trp-cosmosdb-mongo-01 True True trp-cosmosdb-mongo-01 26h NAME READY SYNCED EXTERNAL-NAME AGE account.cosmosdb.azure.upbound.io/trp-cosmosdb-mongo-01 True True trp-cosmosdb-mongo-01 26h To use the MongoDB instance from our application, we need to bind the connection details. The secret generated by Crossplane via the writeConnectionSecretToRef does not use the expected syntax. So we use the SecretGen controller with a SecretTemplate to generate a secret that does. Create a connection details secret \u00b6 The Services Toolkit needs a Kubernetes Secret with a format that adheres to the Service Binding specification. Unfortunately, the secret that the Account manage resource generates does not conform. We use a SecretTemplate that maps the Account secret to a secret usable by the automatic binding. SecretGen RBAC permissions \u00b6 The SecretGen Controller needs permissions in the secret's namespace. So we create a ServiceAccount with the appropriate permissions and then the SecretTemplate . secretgen-rbac.yml echo \"apiVersion: v1 kind: ServiceAccount metadata: name: ${ UNIQUE_NAME } namespace: ${ UNIQUE_NAME } --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: secret-gen-reader namespace: ${ UNIQUE_NAME } rules: - apiGroups: - \\\"\\\" resources: - secrets verbs: - get - list - watch resourceNames: - ${ UNIQUE_NAME } --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: ${ UNIQUE_NAME } -role-binding namespace: ${ UNIQUE_NAME } roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: secret-gen-reader subjects: - kind: ServiceAccount name: ${ UNIQUE_NAME } namespace: ${ UNIQUE_NAME } \" > secretgen-rbac.yml kubectl apply -f secretgen-rbac.yml Secret Template \u00b6 And then we can create a SecretTemplate that transforms the secret from the Crossplane Account to a secret that the Services Toolkit can bind. secret-template.yml echo \"apiVersion: secretgen.carvel.dev/v1alpha1 kind: SecretTemplate metadata: name: ${ UNIQUE_NAME } -bindable namespace: ${ UNIQUE_NAME } spec: serviceAccountName: ${ UNIQUE_NAME } inputResources: - name: creds ref: apiVersion: v1 kind: Secret name: ${ UNIQUE_NAME } template: metadata: labels: app.kubernetes.io/component: ${ UNIQUE_NAME } app.kubernetes.io/instance: ${ UNIQUE_NAME } services.apps.tanzu.vmware.com/class: azure-mongodb type: mongodb stringData: type: mongodb database: ${ UNIQUE_NAME } data: uri: '\\$(.creds.data.attribute\\\\.connection_strings\\\\.0)' \" > secret-template.yml kubectl apply -f secret-template.yml Resource Claim Policy If we create our Claim in one namespace and the claimable resource in another, we need a ResourceClaimPolicy . See below how to create one! We verify the SecretTemplate's secret exists by running the following command: kubectl get secret -n $UNIQUE_NAME This should give you two secrets: (where trp-mongodb-docs-test will be your $UNIQUE_NAME ) NAME TYPE DATA AGE trp-mongodb-docs-test connection.crossplane.io/v1alpha1 8 24m trp-mongodb-docs-test-bindable mongodb 3 94s ResourceClaim Policy (optional) \u00b6 We determine the Developer namespace configured in the Tanzu Application Platform (TAP) installation. If in doubt, assume it is default . export TAP_DEV_NAMESPACE = default Then you create a ResourceClaimPolicy ensuring applications deployed in TAP can use the Service Toolkit claim. resource-claim-policy.yml echo \"apiVersion: services.apps.tanzu.vmware.com/v1alpha1 kind: ResourceClaimPolicy metadata: name: default-can-claim-azure-mongodb namespace: ${ UNIQUE_NAME } spec: subject: kind: Secret group: \\\"\\\" selector: matchLabels: services.apps.tanzu.vmware.com/class: azure-mongodb consumingNamespaces: [ \\\" ${ TAP_DEV_NAMESPACE } \\\" ] \" > resource-claim-policy.yml Verify the status of the ResourceClaimPolicy : kubectl apply -f resource-claim-policy.yml -n ${ UNIQUE_NAME } Next Steps \u00b6 Success You can return to the main guide to continue with the Services Toolkit sections. You should come back for the cleanup commands; see below. Cleanup \u00b6 Danger These commands delete the resources in Azure. Crossplane uses finalizers on its resources. This ensures that the kubectl delete blocks until it confirms the resources are removed at the provider level. kubectl delete -f resource-claim-policy.yml || true kubectl delete -f secret-template.yml || true kubectl delete -f collection.yml || true kubectl delete -f database.yml || true kubectl delete -f account.yml || true kubectl delete -f resourcegroup.yml || true kubectl delete namespace ${ UNIQUE_NAME } || true Warning If you are done with this Crossplane Provider, or want to try out the Crossplane package solution, you have to delete the Provider and ProviderConfig resources as well. Info ProviderConfig resources are owned by the Provider installed at the time. So installing the Azure Provider again via a package dependency, for example, would be blocked because the ProviderConfig is owned by the previous Provider installation. Resources, such as the MongoDB Database, using the ProviderConfig block its deletion. A ProviderConfig resource belongs to the Provider and has a unique Group . So to avoid possible conflicts with other ProviderConfigs , we delete it via its full name. kubectl delete providerconfigs.azure.upbound.io default kubectl delete providers.pkg.crossplane.io upbound-provider-azure","title":"Via Crossplane Instances"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#pre-requisites","text":"UpBound's Universal Crossplane Crossplane Azure Provider SecretGen Controller : to transform the secret from Crossplane to the Service Binding specification","title":"Pre-requisites"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#verify-provider","text":"kubectl api-resources --api-group cosmosdb.azure.upbound.io This gives us the following APIs: NAME SHORTNAMES APIVERSION NAMESPACED KIND accounts cosmosdb.azure.upbound.io/v1beta1 false Account cassandraclusters cosmosdb.azure.upbound.io/v1beta1 false CassandraCluster cassandradatacenters cosmosdb.azure.upbound.io/v1beta1 false CassandraDatacenter cassandrakeyspaces cosmosdb.azure.upbound.io/v1beta1 false CassandraKeySpace cassandratables cosmosdb.azure.upbound.io/v1beta1 false CassandraTable gremlindatabases cosmosdb.azure.upbound.io/v1beta1 false GremlinDatabase gremlingraphs cosmosdb.azure.upbound.io/v1beta1 false GremlinGraph mongocollections cosmosdb.azure.upbound.io/v1beta1 false MongoCollection mongodatabases cosmosdb.azure.upbound.io/v1beta1 false MongoDatabase sqlcontainers cosmosdb.azure.upbound.io/v1beta1 false SQLContainer sqldatabases cosmosdb.azure.upbound.io/v1beta1 false SQLDatabase sqlfunctions cosmosdb.azure.upbound.io/v1beta1 false SQLFunction sqlroleassignments cosmosdb.azure.upbound.io/v1beta1 false SQLRoleAssignment sqlroledefinitions cosmosdb.azure.upbound.io/v1beta1 false SQLRoleDefinition sqlstoredprocedures cosmosdb.azure.upbound.io/v1beta1 false SQLStoredProcedure sqltriggers cosmosdb.azure.upbound.io/v1beta1 false SQLTrigger tables cosmosdb.azure.upbound.io/v1beta1 false Table We also recommend, especially while testing or doing a PoC, creating a specific Resource Group for these resources. The Resource Group API is part of the azure.upbound.io API group. kubectl api-resources --api-group azure.upbound.io As you can see: NAME SHORTNAMES APIVERSION NAMESPACED KIND providerconfigs azure.upbound.io/v1beta1 false ProviderConfig providerconfigusages azure.upbound.io/v1beta1 false ProviderConfigUsage resourcegroups azure.upbound.io/v1beta1 false ResourceGroup resourceproviderregistrations azure.upbound.io/v1beta1 false ResourceProviderRegistration storeconfigs azure.upbound.io/v1alpha1 false StoreConfig subscriptions azure.upbound.io/v1beta1 false Subscription","title":"Verify Provider"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#create-namespace","text":"There are some places where we need to provide the name of the namespace. For the convenience of this example, we create a namespace with the same name. kubectl create namespace ${ UNIQUE_NAME }","title":"Create Namespace"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#create-resource-group","text":"We create a ResourceGroup with a unique name and label it. This is because some of the managed resources (i.e., the resources in Azure) depend on other managed resources. To find them, we must instruct Crossplane, which Crossplane resource maps to the dependent managed resource. For this, we use the label. resourcegroup.yml echo \"apiVersion: azure.upbound.io/v1beta1 kind: ResourceGroup metadata: name: ${ UNIQUE_NAME } namespace: ${ UNIQUE_NAME } labels: testing.upbound.io/example-name: ${ UNIQUE_NAME } spec: forProvider: location: ${ LOCATION } providerConfigRef: name: default \" > resourcegroup.yml And now, you can apply the file to create the Crossplane resource. Again, the Crossplane controller and the Azure provider controller will ensure the resource in Azure exists and the status is visible on our Crossplane resource in Kubernetes. kubectl apply -f resourcegroup.yml","title":"Create Resource Group"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#create-cosmosdb-account","text":"Next on the list is the CosmosDB Account . This object defines the initial configuration of the MongoDB instance. You make this Account a MongoDB instance by setting the capability EnableMongo . If you want to set a specific version, use the spec.forProvider.mongoServerVersion parameter. account.yml echo \"apiVersion: cosmosdb.azure.upbound.io/v1beta1 kind: account metadata: annotations: meta.upbound.io/example-id: cosmosdb/v1beta1/mongocollection labels: testing.upbound.io/example-name: ${ UNIQUE_NAME } name: ${ UNIQUE_NAME } namespace: ${ UNIQUE_NAME } spec: forProvider: capabilities: - name: EnableMongo - name: mongoEnableDocLevelTTL consistencyPolicy: - consistencyLevel: Strong geoLocation: - failoverPriority: 0 location: ${ LOCATION } kind: MongoDB location: ${ LOCATION } offerType: Standard resourceGroupNameSelector: matchLabels: testing.upbound.io/example-name: ${ UNIQUE_NAME } writeConnectionSecretToRef: namespace: ${ UNIQUE_NAME } name: ${ UNIQUE_NAME } \" > account.yml Create the file and apply it to the cluster. kubectl apply -f account.yml Info The Account requires a reference to a Resource Group . We do so via the resourceGroupNameSelector : resourceGroupNameSelector : matchLabels : testing.upbound.io/example-name : ${UNIQUE_NAME}","title":"Create CosmosDB Account"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#create-mongo-database","text":"Now that we have an Account that manages a MongoDB instance, we can create a MongoDB Database in the Account. database.yml echo \"apiVersion: cosmosdb.azure.upbound.io/v1beta1 kind: MongoDatabase metadata: annotations: meta.upbound.io/example-id: cosmosdb/v1beta1/mongocollection labels: testing.upbound.io/example-name: ${ UNIQUE_NAME } name: ${ UNIQUE_NAME } namespace: ${ UNIQUE_NAME } spec: forProvider: accountNameSelector: matchLabels: testing.upbound.io/example-name: ${ UNIQUE_NAME } resourceGroupNameSelector: matchLabels: testing.upbound.io/example-name: ${ UNIQUE_NAME } \" > database.yml kubectl apply -f database.yml","title":"Create Mongo Database"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#create-mongo-collection","text":"In the MongoDB Database , we want to have at least one Collection ( read more about Database and Collection ). All the same rules as before apply. We define the resource's properties and then reference the dependencies. collection.yml echo \"apiVersion: cosmosdb.azure.upbound.io/v1beta1 kind: MongoCollection metadata: annotations: meta.upbound.io/example-id: cosmosdb/v1beta1/mongocollection labels: testing.upbound.io/example-name: ${ UNIQUE_NAME } name: ${ UNIQUE_NAME } namespace: ${ UNIQUE_NAME } spec: forProvider: defaultTtlSeconds: 777 index: - keys: - _id unique: true shardKey: uniqueKey throughput: 400 accountNameSelector: matchLabels: testing.upbound.io/example-name: ${ UNIQUE_NAME } databaseNameSelector: matchLabels: testing.upbound.io/example-name: ${ UNIQUE_NAME } resourceGroupNameSelector: matchLabels: testing.upbound.io/example-name: ${ UNIQUE_NAME } \" > collection.yml And as always, we apply the resource to the cluster and let Crossplane work its magic. kubectl apply -f collection.yml","title":"Create Mongo Collection"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#verify-managed-resource-creation","text":"Create the Crossplane resources and then verify the resources have a successful reconciliation. kubectl get resourcegroup,mongocollection,mongodatabase,account \\ --namespace ${ UNIQUE_NAME } This should yield something like this: (where trp-cosmosdb-mongo-01 will be your $UNIQUE_NAME ) NAME READY SYNCED EXTERNAL-NAME AGE resourcegroup.azure.upbound.io/trp-cosmosdb-mongo-01 True True trp-cosmosdb-mongo-01 26h NAME READY SYNCED EXTERNAL-NAME AGE mongocollection.cosmosdb.azure.upbound.io/trp-cosmosdb-mongo-01 True True trp-cosmosdb-mongo-01 26h NAME READY SYNCED EXTERNAL-NAME AGE mongodatabase.cosmosdb.azure.upbound.io/trp-cosmosdb-mongo-01 True True trp-cosmosdb-mongo-01 26h NAME READY SYNCED EXTERNAL-NAME AGE account.cosmosdb.azure.upbound.io/trp-cosmosdb-mongo-01 True True trp-cosmosdb-mongo-01 26h To use the MongoDB instance from our application, we need to bind the connection details. The secret generated by Crossplane via the writeConnectionSecretToRef does not use the expected syntax. So we use the SecretGen controller with a SecretTemplate to generate a secret that does.","title":"Verify Managed Resource Creation"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#create-a-connection-details-secret","text":"The Services Toolkit needs a Kubernetes Secret with a format that adheres to the Service Binding specification. Unfortunately, the secret that the Account manage resource generates does not conform. We use a SecretTemplate that maps the Account secret to a secret usable by the automatic binding.","title":"Create a connection details secret"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#secretgen-rbac-permissions","text":"The SecretGen Controller needs permissions in the secret's namespace. So we create a ServiceAccount with the appropriate permissions and then the SecretTemplate . secretgen-rbac.yml echo \"apiVersion: v1 kind: ServiceAccount metadata: name: ${ UNIQUE_NAME } namespace: ${ UNIQUE_NAME } --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: secret-gen-reader namespace: ${ UNIQUE_NAME } rules: - apiGroups: - \\\"\\\" resources: - secrets verbs: - get - list - watch resourceNames: - ${ UNIQUE_NAME } --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: ${ UNIQUE_NAME } -role-binding namespace: ${ UNIQUE_NAME } roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: secret-gen-reader subjects: - kind: ServiceAccount name: ${ UNIQUE_NAME } namespace: ${ UNIQUE_NAME } \" > secretgen-rbac.yml kubectl apply -f secretgen-rbac.yml","title":"SecretGen RBAC permissions"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#secret-template","text":"And then we can create a SecretTemplate that transforms the secret from the Crossplane Account to a secret that the Services Toolkit can bind. secret-template.yml echo \"apiVersion: secretgen.carvel.dev/v1alpha1 kind: SecretTemplate metadata: name: ${ UNIQUE_NAME } -bindable namespace: ${ UNIQUE_NAME } spec: serviceAccountName: ${ UNIQUE_NAME } inputResources: - name: creds ref: apiVersion: v1 kind: Secret name: ${ UNIQUE_NAME } template: metadata: labels: app.kubernetes.io/component: ${ UNIQUE_NAME } app.kubernetes.io/instance: ${ UNIQUE_NAME } services.apps.tanzu.vmware.com/class: azure-mongodb type: mongodb stringData: type: mongodb database: ${ UNIQUE_NAME } data: uri: '\\$(.creds.data.attribute\\\\.connection_strings\\\\.0)' \" > secret-template.yml kubectl apply -f secret-template.yml Resource Claim Policy If we create our Claim in one namespace and the claimable resource in another, we need a ResourceClaimPolicy . See below how to create one! We verify the SecretTemplate's secret exists by running the following command: kubectl get secret -n $UNIQUE_NAME This should give you two secrets: (where trp-mongodb-docs-test will be your $UNIQUE_NAME ) NAME TYPE DATA AGE trp-mongodb-docs-test connection.crossplane.io/v1alpha1 8 24m trp-mongodb-docs-test-bindable mongodb 3 94s","title":"Secret Template"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#resourceclaim-policy-optional","text":"We determine the Developer namespace configured in the Tanzu Application Platform (TAP) installation. If in doubt, assume it is default . export TAP_DEV_NAMESPACE = default Then you create a ResourceClaimPolicy ensuring applications deployed in TAP can use the Service Toolkit claim. resource-claim-policy.yml echo \"apiVersion: services.apps.tanzu.vmware.com/v1alpha1 kind: ResourceClaimPolicy metadata: name: default-can-claim-azure-mongodb namespace: ${ UNIQUE_NAME } spec: subject: kind: Secret group: \\\"\\\" selector: matchLabels: services.apps.tanzu.vmware.com/class: azure-mongodb consumingNamespaces: [ \\\" ${ TAP_DEV_NAMESPACE } \\\" ] \" > resource-claim-policy.yml Verify the status of the ResourceClaimPolicy : kubectl apply -f resource-claim-policy.yml -n ${ UNIQUE_NAME }","title":"ResourceClaim Policy (optional)"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#next-steps","text":"Success You can return to the main guide to continue with the Services Toolkit sections. You should come back for the cleanup commands; see below.","title":"Next Steps"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#cleanup","text":"Danger These commands delete the resources in Azure. Crossplane uses finalizers on its resources. This ensures that the kubectl delete blocks until it confirms the resources are removed at the provider level. kubectl delete -f resource-claim-policy.yml || true kubectl delete -f secret-template.yml || true kubectl delete -f collection.yml || true kubectl delete -f database.yml || true kubectl delete -f account.yml || true kubectl delete -f resourcegroup.yml || true kubectl delete namespace ${ UNIQUE_NAME } || true Warning If you are done with this Crossplane Provider, or want to try out the Crossplane package solution, you have to delete the Provider and ProviderConfig resources as well. Info ProviderConfig resources are owned by the Provider installed at the time. So installing the Azure Provider again via a package dependency, for example, would be blocked because the ProviderConfig is owned by the previous Provider installation. Resources, such as the MongoDB Database, using the ProviderConfig block its deletion. A ProviderConfig resource belongs to the Provider and has a unique Group . So to avoid possible conflicts with other ProviderConfigs , we delete it via its full name. kubectl delete providerconfigs.azure.upbound.io default kubectl delete providers.pkg.crossplane.io upbound-provider-azure","title":"Cleanup"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-package/","text":"In this section, we look at installing a Crossplane package, and then use the resources provided by this package to create the MongoDB instance. Prerequisites \u00b6 UpBound's Universal Crossplane Install Package \u00b6 Or use a Kubernetes resource file: Upbound CLI Kubernetes manifest Do make sure you have installed the up CLI, as described here , and execute: up controlplane configuration install \\ --name azure-mongodb \\ ghcr.io/vmware-tanzu/tanzu-application-platform-reference-service-packages/azure/crossplane/mongodb:0.0.4 kubectl apply -f - <<EOF apiVersion: pkg.crossplane.io/v1 kind: Configuration metadata: name: azure-mongodb spec: package: ghcr.io/vmware-tanzu/tanzu-application-platform-reference-service-packages/azure/crossplane/mongodb:0.0.4 EOF The Crossplane docs clarify what you can configure in a Configuration. To verify the Configuration has been installed successfully, run this command: kubectl get configuration,configurationrevision Which should yield something like this: NAME INSTALLED HEALTHY PACKAGE AGE configuration.pkg.crossplane.io/azure-mongodb True True ghcr.io/vmware-tanzu/tanzu-application-platform-reference-service-packages/azure/crossplane/mongodb:0.0.4 13m NAME HEALTHY REVISION IMAGE STATE DEP-FOUND DEP-INSTALLED AGE configurationrevision.pkg.crossplane.io/azure-mongodb-99c3a327f258 True 1 ghcr.io/vmware-tanzu/tanzu-application-platform-reference-service-packages/azure/crossplane/mongodb:0.0.4 Active 1 1 13m Tip If the Configuration does not turn healthy within a few minutes, the information will be in the events. So use kubectl describe : kubectl describe configurationrevision This Configuration package lists the UpBound Azure Provider as its dependency. Crossplane resolves and installs the dependencies for you. If you have already installed the Provider, you can either remove it (and any related ProviderConfig ), or disable the dependency resolution. It does look like the up CLI currently does not support this, so you have to use the manifest solution instead. Setting the following option: spec.skipDependencyResolution: true . To verify the Provider is up and running: kubectl get provider This should yield the following: NAME INSTALLED HEALTHY PACKAGE AGE upbound-provider-azure True True xpkg.upbound.io/upbound/provider-azure:v0.19.0 29m Hint In case you need to recreate the ProviderConfig , here's how you do that. kubectl apply -f - <<EOF apiVersion: azure.upbound.io/v1beta1 kind: ProviderConfig metadata: name: default spec: credentials: source: Secret secretRef: namespace: upbound-system name: azure-secret key: creds EOF Create Crossplane Claim \u00b6 In contrast to creating a MongoDB instance through the managed resource definitions directly, installing a Crossplane package (or Configuration ) requires an additional step. You now have a blueprint available to create MongoDB instances, through Crossplane's CompositeResourceDefinition ( XRD ) and Composition resources. The package adds the following Custom Resources ( CR ) to your cluster: MongoDBInstance : the Claim resource you use for requesting a new instance XMongoDBInstance : the Composition \"instance\", which is the bridge between your Claim and the Composition It also adds an XRD and the aforementioned Composition . The XRD defines the API for a type, including how to implement it (via one or more Compositions ) and how to request an instance of that type via a Claim . In this package, the ClaimName is MongoDBInstance , which is the Kubernetes CR's kind . LOCATION = \"West Europe\" INSTANCE_NAME = my-mongodb-instance To create the claim, apply the following manifest: kubectl apply -f - <<EOF apiVersion: azure.ref.services.apps.tanzu.vmware.com/v1alpha1 kind: MongoDBInstance metadata: namespace: default name: ${INSTANCE_NAME} spec: compositionSelector: matchLabels: database: mongodb parameters: location: ${LOCATION} capabilities: - name: \"EnableMongo\" - name: \"mongoEnableDocLevelTTL\" publishConnectionDetailsTo: name: ${INSTANCE_NAME}-bindable configRef: name: default metadata: labels: services.apps.tanzu.vmware.com/class: azure-mongodb EOF Hint For the Services Toolkit to bind the connection secret, we use the publishConnectionDetailsTo beta feature instead of the original writeConnectionSecretToRef . This way, we can configure additional properties, such as the metadata.labels . Verify Managed Resource Creation \u00b6 Verify the reconciliation status when you finish creating the Crossplane resources. kubectl get resourcegroup,mongocollection,mongodatabase,account Which should yield something like this: NAME READY SYNCED EXTERNAL-NAME AGE resourcegroup.azure.upbound.io/my-mongodb-instance True True my-mongodb-instance 26h NAME READY SYNCED EXTERNAL-NAME AGE mongocollection.cosmosdb.azure.upbound.io/my-mongodb-instance True True my-mongodb-instance 26h NAME READY SYNCED EXTERNAL-NAME AGE mongodatabase.cosmosdb.azure.upbound.io/my-mongodb-instance True True my-mongodb-instance 26h NAME READY SYNCED EXTERNAL-NAME AGE account.cosmosdb.azure.upbound.io/my-mongodb-instance True True my-mongodb-instance 26h When all the resources are ready, the secret for your claim (the MongoDBInstance ) is created, and the claim is set to ready. You can wait for this to happen with this kubectl wait command. kubectl wait --for = condition = ready \\ mongodbinstances.azure.ref.services.apps.tanzu.vmware.com ${ INSTANCE_NAME } \\ --timeout = 400s Next Steps \u00b6 Success You can return to the main guide to continue with the Services Toolkit sections. You should come back for the cleanup commands; see below. Cleanup Resources \u00b6 Danger These commands delete the resources in Azure. Crossplane uses finalizers on its resources. This ensures that the kubectl delete blocks until it confirms the resources are removed at the provider level. kubectl delete mongodbinstance ${ INSTANCE_NAME } || true kubectl delete MongoDatabase -l crossplane.io/claim-name = ${ INSTANCE_NAME } || true kubectl delete MongoCollection -l crossplane.io/claim-name = ${ INSTANCE_NAME } || true kubectl delete Account -l crossplane.io/claim-name = ${ INSTANCE_NAME } || true kubectl delete ResourceGroup -l crossplane.io/claim-name = ${ INSTANCE_NAME } || true kubectl delete configuration ${ CONFIG_NAME } || true kubectl delete providerconfig.azure.upbound.io default || true And if the provider was installed via this package, you might want to clean that up as well. kubectl delete provider upbound-provider-azure Explore the XRD and Composition \u00b6 If you want to take a look at the XRD and Composition, use the following commands. To see them in the cluster: kubectl get xrd,composition This yields the following: NAME ESTABLISHED OFFERED AGE compositeresourcedefinition.apiextensions.crossplane.io/xmongodbinstances.azure.ref.services.apps.tanzu.vmware.com True True 112m NAME AGE composition.apiextensions.crossplane.io/mongodbinstance 112m For more information, output them as YAML via the -o yaml flag in your kubectl command.","title":"Via Crossplane Package"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-package/#prerequisites","text":"UpBound's Universal Crossplane","title":"Prerequisites"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-package/#install-package","text":"Or use a Kubernetes resource file: Upbound CLI Kubernetes manifest Do make sure you have installed the up CLI, as described here , and execute: up controlplane configuration install \\ --name azure-mongodb \\ ghcr.io/vmware-tanzu/tanzu-application-platform-reference-service-packages/azure/crossplane/mongodb:0.0.4 kubectl apply -f - <<EOF apiVersion: pkg.crossplane.io/v1 kind: Configuration metadata: name: azure-mongodb spec: package: ghcr.io/vmware-tanzu/tanzu-application-platform-reference-service-packages/azure/crossplane/mongodb:0.0.4 EOF The Crossplane docs clarify what you can configure in a Configuration. To verify the Configuration has been installed successfully, run this command: kubectl get configuration,configurationrevision Which should yield something like this: NAME INSTALLED HEALTHY PACKAGE AGE configuration.pkg.crossplane.io/azure-mongodb True True ghcr.io/vmware-tanzu/tanzu-application-platform-reference-service-packages/azure/crossplane/mongodb:0.0.4 13m NAME HEALTHY REVISION IMAGE STATE DEP-FOUND DEP-INSTALLED AGE configurationrevision.pkg.crossplane.io/azure-mongodb-99c3a327f258 True 1 ghcr.io/vmware-tanzu/tanzu-application-platform-reference-service-packages/azure/crossplane/mongodb:0.0.4 Active 1 1 13m Tip If the Configuration does not turn healthy within a few minutes, the information will be in the events. So use kubectl describe : kubectl describe configurationrevision This Configuration package lists the UpBound Azure Provider as its dependency. Crossplane resolves and installs the dependencies for you. If you have already installed the Provider, you can either remove it (and any related ProviderConfig ), or disable the dependency resolution. It does look like the up CLI currently does not support this, so you have to use the manifest solution instead. Setting the following option: spec.skipDependencyResolution: true . To verify the Provider is up and running: kubectl get provider This should yield the following: NAME INSTALLED HEALTHY PACKAGE AGE upbound-provider-azure True True xpkg.upbound.io/upbound/provider-azure:v0.19.0 29m Hint In case you need to recreate the ProviderConfig , here's how you do that. kubectl apply -f - <<EOF apiVersion: azure.upbound.io/v1beta1 kind: ProviderConfig metadata: name: default spec: credentials: source: Secret secretRef: namespace: upbound-system name: azure-secret key: creds EOF","title":"Install Package"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-package/#create-crossplane-claim","text":"In contrast to creating a MongoDB instance through the managed resource definitions directly, installing a Crossplane package (or Configuration ) requires an additional step. You now have a blueprint available to create MongoDB instances, through Crossplane's CompositeResourceDefinition ( XRD ) and Composition resources. The package adds the following Custom Resources ( CR ) to your cluster: MongoDBInstance : the Claim resource you use for requesting a new instance XMongoDBInstance : the Composition \"instance\", which is the bridge between your Claim and the Composition It also adds an XRD and the aforementioned Composition . The XRD defines the API for a type, including how to implement it (via one or more Compositions ) and how to request an instance of that type via a Claim . In this package, the ClaimName is MongoDBInstance , which is the Kubernetes CR's kind . LOCATION = \"West Europe\" INSTANCE_NAME = my-mongodb-instance To create the claim, apply the following manifest: kubectl apply -f - <<EOF apiVersion: azure.ref.services.apps.tanzu.vmware.com/v1alpha1 kind: MongoDBInstance metadata: namespace: default name: ${INSTANCE_NAME} spec: compositionSelector: matchLabels: database: mongodb parameters: location: ${LOCATION} capabilities: - name: \"EnableMongo\" - name: \"mongoEnableDocLevelTTL\" publishConnectionDetailsTo: name: ${INSTANCE_NAME}-bindable configRef: name: default metadata: labels: services.apps.tanzu.vmware.com/class: azure-mongodb EOF Hint For the Services Toolkit to bind the connection secret, we use the publishConnectionDetailsTo beta feature instead of the original writeConnectionSecretToRef . This way, we can configure additional properties, such as the metadata.labels .","title":"Create Crossplane Claim"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-package/#verify-managed-resource-creation","text":"Verify the reconciliation status when you finish creating the Crossplane resources. kubectl get resourcegroup,mongocollection,mongodatabase,account Which should yield something like this: NAME READY SYNCED EXTERNAL-NAME AGE resourcegroup.azure.upbound.io/my-mongodb-instance True True my-mongodb-instance 26h NAME READY SYNCED EXTERNAL-NAME AGE mongocollection.cosmosdb.azure.upbound.io/my-mongodb-instance True True my-mongodb-instance 26h NAME READY SYNCED EXTERNAL-NAME AGE mongodatabase.cosmosdb.azure.upbound.io/my-mongodb-instance True True my-mongodb-instance 26h NAME READY SYNCED EXTERNAL-NAME AGE account.cosmosdb.azure.upbound.io/my-mongodb-instance True True my-mongodb-instance 26h When all the resources are ready, the secret for your claim (the MongoDBInstance ) is created, and the claim is set to ready. You can wait for this to happen with this kubectl wait command. kubectl wait --for = condition = ready \\ mongodbinstances.azure.ref.services.apps.tanzu.vmware.com ${ INSTANCE_NAME } \\ --timeout = 400s","title":"Verify Managed Resource Creation"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-package/#next-steps","text":"Success You can return to the main guide to continue with the Services Toolkit sections. You should come back for the cleanup commands; see below.","title":"Next Steps"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-package/#cleanup-resources","text":"Danger These commands delete the resources in Azure. Crossplane uses finalizers on its resources. This ensures that the kubectl delete blocks until it confirms the resources are removed at the provider level. kubectl delete mongodbinstance ${ INSTANCE_NAME } || true kubectl delete MongoDatabase -l crossplane.io/claim-name = ${ INSTANCE_NAME } || true kubectl delete MongoCollection -l crossplane.io/claim-name = ${ INSTANCE_NAME } || true kubectl delete Account -l crossplane.io/claim-name = ${ INSTANCE_NAME } || true kubectl delete ResourceGroup -l crossplane.io/claim-name = ${ INSTANCE_NAME } || true kubectl delete configuration ${ CONFIG_NAME } || true kubectl delete providerconfig.azure.upbound.io default || true And if the provider was installed via this package, you might want to clean that up as well. kubectl delete provider upbound-provider-azure","title":"Cleanup Resources"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-package/#explore-the-xrd-and-composition","text":"If you want to take a look at the XRD and Composition, use the following commands. To see them in the cluster: kubectl get xrd,composition This yields the following: NAME ESTABLISHED OFFERED AGE compositeresourcedefinition.apiextensions.crossplane.io/xmongodbinstances.azure.ref.services.apps.tanzu.vmware.com True True 112m NAME AGE composition.apiextensions.crossplane.io/mongodbinstance 112m For more information, output them as YAML via the -o yaml flag in your kubectl command.","title":"Explore the XRD and Composition"},{"location":"usecases/azure/packages/mongodb/crossplane/produce-package/","text":"","title":"Produce package"},{"location":"usecases/azure/packages/mongodb/crossplane/test-package/","text":"","title":"Test Crossplane MongoDB Package"},{"location":"usecases/azure/packages/mongodb/crossplane/troubleshooting/","text":"","title":"Crossplane MongoDB Troubleshooting"},{"location":"usecases/azure/prerequisites/","text":"SecretGen Controller \u00b6 The SecretGen Controller is part of the Tanzu Cluster Essentials . If you install the Cluster Essentials , you can skip installing the SecretGen Controller yourself. If you have not installed the Cluster Essentials , either follow its installation docs or continue below. In most cases, you can safely install the latest version of the SecretGen Controller : kubectl apply -f https://github.com/vmware-tanzu/carvel-secretgen-controller/releases/latest/download/release.yml But, if that doesn't work, or you want to install a fixed version, take a look at the releases .","title":"Azure Packages Prerequisites"},{"location":"usecases/azure/prerequisites/#secretgen-controller","text":"The SecretGen Controller is part of the Tanzu Cluster Essentials . If you install the Cluster Essentials , you can skip installing the SecretGen Controller yourself. If you have not installed the Cluster Essentials , either follow its installation docs or continue below. In most cases, you can safely install the latest version of the SecretGen Controller : kubectl apply -f https://github.com/vmware-tanzu/carvel-secretgen-controller/releases/latest/download/release.yml But, if that doesn't work, or you want to install a fixed version, take a look at the releases .","title":"SecretGen Controller"}]}