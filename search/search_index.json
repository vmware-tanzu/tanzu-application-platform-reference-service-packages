{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TAP Reference Service Packages","text":"<p>This repository is a collection of reference packages for Kubernetes, aimed at demonstrating how to consume public cloud services and expose the generated secrets to applications via service bindings. They are fully compatible with Services Toolkit for VMware Tanzu Application Platform.</p> <p>Either Carvel packages or Crosspplane configurations have been used, in order to provide examples for both methods.</p> <p>Important</p> <p>Such packages are intended to be for reference only and haven't been tested nor are they supported for production use.</p>"},{"location":"ci/","title":"Continuous Integration","text":"<p>Continuous integration practices help reduce time-consuming activities as well as human errors, by defining some standards and automating operations, such as builds and tests.</p> <p>To maintain a clean and working main branch that can be a trusted source of code and documentation, we make changes exclusively via a pull request(PR).</p> <p>The PR-related events trigger workflows to run repetitive tasks that are automated, such as tests, thus reducing the toil on code maintainers.</p> <p>This repository features a few GitHub Actions workflows to handle different components, such as:</p> <ul> <li>Documentation</li> <li>Carvel and Crossplane packages</li> <li>Carvel repository</li> </ul> <p>Note</p> <p>GitHub Actions reusable workflows are being used in order to simplify writing and managing pipelines that become more readable and easier to maintain, as well as making it possible to reuse them effectively in other pipelines to respond to different events.</p> <p>They allow writing specific workflows, like functions in programming languages. These workflows can read inputs and produce outputs that they pass along to other workflows. It's important to highlight two things that might not be trivial:</p> <ol> <li>A workflow does not automatically transfer Environment variables to its children.    To pass values from one to the others, you use the inputs/outputs.</li> <li>Secrets are not immediately visible in children's workflows:    They must either be defined one by one or declare explicit inheritance. More info here. </li> </ol>"},{"location":"ci/carvel-crossplane-packages/","title":"Carvel and Crossplane packages","text":"<p>We store the code and configuration for the reference packages in the <code>packages</code> folder, in a structure like the following:</p> <pre><code>packages\n\u251c\u2500\u2500 &lt;PROVIDER&gt;\n\u2502   \u2514\u2500\u2500 &lt;PACKAGING&gt;\n\u2502       \u2514\u2500\u2500 &lt;NAME&gt;\n\u2502           \u251c\u2500\u2500 ...\n\u2502           \u251c\u2500\u2500 ...\n\u2506           \u2506\n</code></pre> <ul> <li><code>PROVIDER</code>:  the provider we build the package for (i.e., <code>aws</code>, <code>azure</code>, <code>gcp</code>, <code>multicloud</code>).</li> <li><code>PACKAGING</code>: the packaging system used (currently, it must be either <code>carvel</code> or <code>crossplane</code>).</li> <li><code>NAME</code>: the package name.</li> </ul> <p>The following diagram shows that multiple packages can be built and tested in parallel via the <code>publish-packages.yml</code> workflow.</p> <pre><code>graph LR\n  A(List packages) --&gt; B(Bump version)\n  B --&gt; C\n  B --&gt; D\n\n  subgraph Crossplane\n    D(Publish Crossplane packages) --&gt; G(Test Crossplane packages)\n  end\n\n  subgraph Carvel\n    C(Publish Carvel packages) --&gt; E(Publish Carvel repository)\n    E --&gt; F(Test Carvel packages)\n  end\n\n  click A \"#list-packages\"\n  click B \"#bump-version\"\n  click C \"#publish-carvel-packages\"\n  click E \"#publish-carvel-repository\"\n  click F \"#test-carvel-packages\"\n  click D \"#publish-crossplane-packages\"\n  click G \"#test-crossplane-packages\"</code></pre> <p>This workflow runs upon a push event to either <code>main</code> or <code>develop</code> branches and pull request events targeting the <code>main</code> branch. Only the files matching the <code>packages/*/*/*/**</code> glob pattern trigger the workflow: this reflects the packages structure described before and prevents changes to files outside actual package directories from triggering it.</p>"},{"location":"ci/carvel-crossplane-packages/#list-packages","title":"List packages","text":"<p>This job evaluates the differences since the previous commit and builds two lists of packages, for Carvel and Crossplane, to feed the following GitHub Actions matrix jobs.</p>"},{"location":"ci/carvel-crossplane-packages/#bump-version","title":"Bump version","text":"<p>The repository is tagged with a new version, bumping the latest one by either the major, minor, or patch fields. The git comment determines which section of the version we increment. Depending on the occurrence of <code>#major,</code> <code>#minor</code>, <code>#patch</code> (default is  <code>patch</code>). You can find more information here.</p>"},{"location":"ci/carvel-crossplane-packages/#publish-carvel-packages","title":"Publish Carvel packages","text":"<p>We build the changed Carvel packages in parallel and publish them to GitHub Packages using Carvel's <code>kctrl</code> CLI. <code>kctrl</code> also produces package metadata files needed to build the Carvel repository. We store these metadata files in a separate (temporary) git branch for each package.</p>"},{"location":"ci/carvel-crossplane-packages/#publish-carvel-repository","title":"Publish Carvel repository","text":"<p>We collect the packages' metadata files produced in the previous job into one branch. Finally, we use <code>kctrl</code> to author the pre-release Carvel repository and publish it to GitHub Packages in OCI format. The artifact produced by <code>kctrl</code> is a manifest file for the <code>PackageRepository</code> resource, useful for quickly deploying the repository on top of a kapp-enabled Kubernetes cluster. This file is attached to a pre-release; we use this file in the following job for testing the packages.</p> <p>Carvel Repository</p> <p>Because we build this Carvel repository to test the packages, we create a pre-release. The final repository release is taken care of by a different workflow.</p>"},{"location":"ci/carvel-crossplane-packages/#test-carvel-packages","title":"Test Carvel packages","text":"<p>This phase aims at testing the Carvel packages by spawning one (GitHub Workflow) job per package and running them in parallel on separate GitHub Actions runners. Each job performs the following steps:</p> <ol> <li>install the Carvel suite</li> <li>create a kind cluster</li> <li>install kapp-controller</li> <li>install secretgen-controller</li> <li>install the Carvel repository published in the previous job</li> <li> <p>run a test script that adheres to a specific naming convention (<code>scripts/carvel-e2e-$PACKAGE_PROVIDER-$PACKAGE_NAME.sh</code>), to:</p> <ol> <li>install the cloud operator specific to the current package (i.e., ACK, ASO, ...)</li> <li>deploy the package with sample input data (need to consider also firewall rules to allow network traffic)</li> <li>deploy an application to consume the service provided via the package</li> <li>ingest data into the application</li> <li>assert the ingested data</li> </ol> </li> <li> <p>clean up everything</p> </li> </ol>"},{"location":"ci/carvel-crossplane-packages/#publish-crossplane-packages","title":"Publish Crossplane packages","text":"<p>We build Crossplane packages leveraging <code>ytt</code>, for dealing with templating, and the <code>up</code> CLI from Upbound, for building the packages and pushing them to an OCI registry (GitHub Packages).</p>"},{"location":"ci/carvel-crossplane-packages/#test-crossplane-packages","title":"Test Crossplane packages","text":"<p>The test phase is similar to Carvel's, which means:</p> <ol> <li>create a kind cluster</li> <li>install Upbound's Universal Crossplane</li> <li> <p>run a test script that adheres to a specific naming convention (<code>scripts/crossplane-e2e-$PACKAGE_PROVIDER-$PACKAGE_NAME.sh</code>), to:</p> <ol> <li>install the Crossplane provider(s) needed by the package</li> <li>install the Crossplane package</li> <li>create a claim to trigger the build of the final managed resources</li> <li>deploy and test an application</li> </ol> </li> <li> <p>clean up everything</p> </li> </ol> <p>Warning</p> <p>Testing Crossplane packages in standard GitHub Actions runners might be tricky: In fact, standard runners have two vCPUs and 7 GB of RAM at the time of writing (specs here). This is on the low side for serving the number of APIs that Crossplane and its providers can bring (especially the AWS one). Unfortunately, this leads to race conditions that might make the workflows either fail or succeed at unpredictable stages.</p> <p>However, it is possible to use the very same scripts provided and test the packages on a larger Kubernetes cluster or make use of paid GitHub-hosted large runners. Although standard Linux runners are free for public repositories, larger ones are not. However, you can look at the per-minutes rates page, to get an overview of the price of the various instances.</p>"},{"location":"ci/documentation/","title":"Documentation","text":"<p>The current documentation is written using Markdown syntax and translated into static web pages with <code>mkdocs</code>, which are then published to GitHub Pages and directly linked to the source code repository.</p> <p>Via a single <code>mkdocs.yml</code> file, it is possible to fine-tune <code>mkdocs</code>, its extensions, and the theme of choice (Material), thus creating very nice-looking and tidy pages. The structure of the resulting website is defined in that file, too.</p> <p>Depending on the specific git event, different workflows may be triggered when docs files get added or updated inside the <code>docs</code> folder or the <code>mkdocs.yml</code> file is changed.</p> <p>Pull requests to the <code>main</code> branch trigger the <code>build-docs.yml</code> workflow that does the build to validate the syntax. In the following diagram, we create a pull request at commit <code>D</code>, and the following commits on the same branch (<code>E</code> and <code>F</code>) cause pull request synchronize events. All commits <code>D</code>, <code>E</code>, and <code>F</code> trigger the <code>build-docs.yml</code> workflow.</p> <p>When we accept the pull request, the code gets merged to the main branch (commit <code>G</code>), and the <code>publish-docs.yml</code> workflow builds and deploys the static website to GitHub Pages.</p> <pre><code>%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': { 'showBranches': true,'showCommitLabel': true, 'rotateCommitLabel': false } } }%%\ngitGraph\n   commit id: \"A\"\n   commit id: \"B\"\n   branch docs\n   checkout docs\n   commit id: \"C\"\n   commit id: \"D\" type: HIGHLIGHT\n   commit id: \"E\" type: HIGHLIGHT\n   commit id: \"F\" type: HIGHLIGHT\n   checkout main\n   merge docs id: \"G\"\n   commit id: \"H\"</code></pre>"},{"location":"crossplane/","title":"Install Crossplane","text":""},{"location":"crossplane/#install-crossplane-via-upbound-cli","title":"Install Crossplane via Upbound CLI","text":"<p>Download the <code>up</code> cli</p> <pre><code>curl -sL \"https://cli.upbound.io\" | sh\nsudo mv up /usr/local/bin/\n</code></pre> <p>Check the installed version:</p> <pre><code>up --version\n</code></pre> <p>Switch to the proper Kubernetes context and run the following command in order to install Upbound Universal Crossplane (UXP):</p> <pre><code>up uxp install\n</code></pre> <p>Verify all UXP pods are Running with kubectl get pods -n upbound-system. This may take up to five minutes depending on your Kubernetes cluster.</p> <pre><code>$ k get pods -n upbound-system\nNAME                                       READY   STATUS    RESTARTS      AGE\ncrossplane-65444df64-7wcb2                 1/1     Running   0             92s\ncrossplane-rbac-manager-69498f955b-2npkl   1/1     Running   0             92s\nupbound-bootstrapper-5c9864b546-lngkw      1/1     Running   0             92s\nxgql-6485cf5748-src2w                      1/1     Running   3 (70s ago)   92s\n</code></pre> <p>Note</p> <p>RESTARTS for the xgql pod are normal during initial installation.</p>"},{"location":"crossplane/providers/aws/","title":"UXP AWS provider","text":"<p>Upbound Universal Crossplane (UXP) AWS provider is a provider for Amazon Web Services developed and supported by Upbound.</p> <p>It can be deployed on top of a Kubernetes cluster with Crossplane, by using either the Upbound CLI (see here for details about installation) or a YAML manifest.</p>"},{"location":"crossplane/providers/aws/#installation","title":"Installation","text":"<p>You can check available releases on project's GitHub repository or using <code>gh</code> like</p> <pre><code>gh release list --repo upbound/provider-aws\n</code></pre> <p>and store the desired release into the <code>PROVIDER_AWS_RELEASE</code> variable.</p> Upbound CLIYAML manifest <p>Do make sure you have installed the <code>up</code> CLI as described here and execute <pre><code>up controlplane provider install xpkg.upbound.io/upbound/provider-aws:${PROVIDER_AWS_RELEASE} --name upbound-provider-aws\n</code></pre></p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: pkg.crossplane.io/v1\nkind: Provider\nmetadata:\n  name: upbound-provider-aws\nspec:\n  package: xpkg.upbound.io/upbound/provider-aws:${PROVIDER_AWS_RELEASE}\nEOF\n</code></pre> <p>It is now necessary to configure the provider's authentication to the AWS API endpoints.</p> <p>The authentication method can vary based on your company's policies: for example, you might be allowed to use long-term credentials such as access key and secret access key pairs, however, if your Kubernetes platform is AWS EKS, it's much more secure to use IAM roles for service accounts (IRSA).</p> <p>Please make sure you do create the OIDC provider as described in the EKS set-up guide before reading on. The following paragraphs explain how to configure IRSA for the Crossplane AWS provider.</p>"},{"location":"crossplane/providers/aws/#create-iam-role-and-policy","title":"Create IAM role and policy","text":"<p>You must create a proper role for the provider to assume, for granting the necessary and sufficient permissions to manage the AWS infrastructure. The least-privilege principle applies, therefore it's important to understand the actual needs and create the permission policy accordingly. For example, the following snippet creates a policy that allows the role it's attached to to execute actions only on the S3 service.</p> <p>First of all, set your the AWS region you're operating in and your EKS cluster name.</p> <pre><code># AWS region you're operating in\nexport AWS_REGION=\"eu-central-1\"\n\n# EKS cluster name\nCLUSTER_NAME=\"my-eks-cluster\"\n\n# define variables for IAM\nACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\nOIDC_ID=$(aws eks describe-cluster --name ${CLUSTER_NAME} --output text --query \"cluster.identity.oidc.issuer\" | cut -d/ -f3-)\nCROSSPLANE_ROLE=\"crossplane-for-${CLUSTER_NAME}\"\nROLE_TRUST_POLICY=$(mktemp)\nROLE_PERMISSION_POLICY=$(mktemp)\n</code></pre> <p>You can then define the role's trust policy, in order to allow Crossplane AWS provider's service account to assume it as WebIdentity, and then create the role.</p> <pre><code># prepare the trust policy document\ncat &gt; ${ROLE_TRUST_POLICY} &lt;&lt;EOF\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"Federated\": \"arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_ID}\"\n        },\n        \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n        \"Condition\": {\n            \"StringLike\": {\n                \"${OIDC_ID}:sub\": \"system:serviceaccount:upbound-system:upbound-provider-aws-*\"\n            }\n        }\n    }]\n}\nEOF\n\n# create the role with the proper trust policy\naws iam create-role --role-name ${CROSSPLANE_ROLE} --assume-role-policy-document file://${ROLE_TRUST_POLICY}\n</code></pre> <p>Now, the permission policies, that define which permissions are granted to the role, have to be created and attached to the role. For this example you will need just one policy, with a number of statements declaring what the role can or cannot do.</p> <pre><code># create the permission policy document\ncat &gt; ${ROLE_PERMISSION_POLICY} &lt;&lt;EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Action\": \"s3:*\",\n      \"Effect\": \"Allow\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\nEOF\n\n# create the permission policy\nPERMISSION_POLICY_ARN=$(aws iam create-policy --policy-name ${CROSSPLANE_ROLE} --policy-document file://${ROLE_PERMISSION_POLICY} --query Policy.Arn --output text)\n\n# attach the policy to the role\naws iam attach-role-policy --policy-arn ${PERMISSION_POLICY_ARN} --role-name ${CROSSPLANE_ROLE}\n\n# clean up temporary files\nrm ${ROLE_TRUST_POLICY}\nrm ${ROLE_PERMISSION_POLICY}\n</code></pre>"},{"location":"crossplane/providers/aws/#create-kubernetes-resources","title":"Create Kubernetes resources","text":"<p>Create a <code>ProviderConfig</code> resource to specify IRSA as authentication method.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: aws.upbound.io/v1beta1\nkind: ProviderConfig\nmetadata:\n  name: default\nspec:\n  credentials:\n    source: IRSA\nEOF\n</code></pre> <p>Create a <code>ControllerConfig</code> resource to specify the AWS provider's settings, including the IRSA role to assume:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: pkg.crossplane.io/v1alpha1\nkind: ControllerConfig\nmetadata:\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::${ACCOUNT_ID}:role/${CROSSPLANE_ROLE}\n  name: aws-irsa\nEOF\n</code></pre> <p>and patch the AWS provider to use it:</p> <pre><code>kubectl patch providers.pkg.crossplane.io upbound-provider-aws --type='merge' --patch '{\"spec\": { \"controllerConfigRef\": { \"name\": \"aws-irsa\" } } }'\n</code></pre> <p>Destroy the existing pods to make sure that new ones will be created:</p> <pre><code>kubectl -n upbound-system delete pods -l pkg.crossplane.io/provider=upbound-provider-aws\n</code></pre> <p>Now you can test the effectiveness of the configuration by creating a simple S3 bucket:</p> <pre><code>BUCKET_NAME=$(kubectl create -o yaml -f - &lt;&lt;EOF | yq '.metadata.name'\napiVersion: s3.aws.upbound.io/v1beta1\nkind: Bucket\nmetadata:\n  generateName: crossplane-test-bucket-\nspec:\n  forProvider:\n    region: ${AWS_REGION}\nEOF\n)\n</code></pre> <p>and verify its status</p> <pre><code>$ kubectl get buckets.s3.aws.upbound.io ${BUCKET_NAME}\nNAME                           READY   SYNCED   EXTERNAL-NAME                  AGE\ncrossplane-test-bucket-cxr9g   True    True     crossplane-test-bucket-cxr9g   80s\n</code></pre> <p>As the bucket is marked as synced, it's worth checking the status of the AWS resource:</p> <pre><code>$ aws s3api list-buckets | jq '.Buckets[]|select(.Name == \"'${BUCKET_NAME}'\")'\n{\n\"Name\": \"crossplane-test-bucket-cxr9g\",\n  \"CreationDate\": \"2022-11-05T00:36:49+00:00\"\n}\n</code></pre> <p>This proves that the provider is configured correctly and you can safely delete the test bucket:</p> <pre><code>kubectl delete buckets.s3.aws.upbound.io ${BUCKET_NAME}\n</code></pre>"},{"location":"crossplane/providers/azure/","title":"UXP Azure provider","text":"<p>Upbound's Azure Provider is an Azure provider for Crossplane that is developed and supported by Upbound.</p> <p>It can be deployed on top of a Kubernetes cluster with Crossplane using the Upbound CLI (see here for details about installation) or a YAML manifest.</p>"},{"location":"crossplane/providers/azure/#installation","title":"Installation","text":"<p>You can check available releases on project's GitHub repository or using <code>gh</code> like</p> <pre><code>gh release list --repo upbound/provider-azure\n</code></pre> <p>Store the desired release in the <code>PROVIDER_AZURE_RELEASE</code> variable.</p> <p>Note</p> Upbound CLIYAML manifest <p>Do make sure you have installed the <code>up</code> CLI, as described here, and execute <pre><code>up controlplane provider install xpkg.upbound.io/upbound/provider-azure:${PROVIDER_AZURE_RELEASE}\n</code></pre></p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: pkg.crossplane.io/v1\nkind: Provider\nmetadata:\n  name: provider-azure\nspec:\n  package: xpkg.upbound.io/upbound/provider-azure:${PROVIDER_AZURE_RELEASE}\nEOF\n</code></pre> <p>Ensure the provider is installed and healthy by running the following:</p> <pre><code>kubectl get provider\n</code></pre> <p>Which should yield something like the following:</p> <pre><code>NAME             INSTALLED   HEALTHY   PACKAGE                                          AGE\nprovider-azure   True        True      xpkg.upbound.io/upbound/provider-azure:v0.17.0   2m35s\n</code></pre> <p>Before we can use the provider, we need to supply it with credentials. We can use a Service Provider or a Managed Service Identity.</p>"},{"location":"crossplane/providers/azure/#verify-provider","title":"Verify Provider","text":"<p>The installation of the Crossplane Azure provider results in the availability of new Kubernetes APIs for interacting with Azure resources from within the TAP cluster.</p> <p>The total number of available resources is relatively high, so let us focus on the resources related to CosmosDB.</p> <pre><code>kubectl api-resources --api-group cosmosdb.azure.upbound.io\n</code></pre> <p>Running this command prints the following APIs:</p> <pre><code>NAME                   SHORTNAMES   APIVERSION                          NAMESPACED   KIND\naccounts                            cosmosdb.azure.upbound.io/v1beta1   false        Account\ncassandraclusters                   cosmosdb.azure.upbound.io/v1beta1   false        CassandraCluster\ncassandradatacenters                cosmosdb.azure.upbound.io/v1beta1   false        CassandraDatacenter\ncassandrakeyspaces                  cosmosdb.azure.upbound.io/v1beta1   false        CassandraKeySpace\ncassandratables                     cosmosdb.azure.upbound.io/v1beta1   false        CassandraTable\ngremlindatabases                    cosmosdb.azure.upbound.io/v1beta1   false        GremlinDatabase\ngremlingraphs                       cosmosdb.azure.upbound.io/v1beta1   false        GremlinGraph\nmongocollections                    cosmosdb.azure.upbound.io/v1beta1   false        MongoCollection\nmongodatabases                      cosmosdb.azure.upbound.io/v1beta1   false        MongoDatabase\nsqlcontainers                       cosmosdb.azure.upbound.io/v1beta1   false        SQLContainer\nsqldatabases                        cosmosdb.azure.upbound.io/v1beta1   false        SQLDatabase\nsqlfunctions                        cosmosdb.azure.upbound.io/v1beta1   false        SQLFunction\nsqlroleassignments                  cosmosdb.azure.upbound.io/v1beta1   false        SQLRoleAssignment\nsqlroledefinitions                  cosmosdb.azure.upbound.io/v1beta1   false        SQLRoleDefinition\nsqlstoredprocedures                 cosmosdb.azure.upbound.io/v1beta1   false        SQLStoredProcedure\nsqltriggers                         cosmosdb.azure.upbound.io/v1beta1   false        SQLTrigger\ntables                              cosmosdb.azure.upbound.io/v1beta1   false        Table\n</code></pre>"},{"location":"crossplane/providers/azure/#create-configure-service-provider","title":"Create &amp; Configure Service Provider","text":"<p>Before proceeding, we need an Azure subscription, an Azure account with sufficient privileges, and the <code>az</code> (Azure) CLI. You can find how to install the CLI here.</p> <p>Login to Azure via its CLI (<code>az</code>).</p> <pre><code>az login\n</code></pre> <p>If you're not sure about your subscription id, you can run the following command:</p> <pre><code>az account show --query \"{subscriptionId:id, tenantId:tenantId}\"\n</code></pre> <pre><code>SUBSCRIPTION_ID=\n</code></pre> <p>Then create a Service Principle (<code>sp</code>), which has sufficient permissions to create all the necessary resources in Azure.</p> <p>For example:</p> <pre><code>az ad sp create-for-rbac \\\n--sdk-auth \\\n--role Owner \\\n--scopes \"/subscriptions/${SUBSCRIPTION_ID}\"\n</code></pre> <p>Warning</p> <p>You probably do not want to give it the role <code>Owner</code> if this is a production account.</p> <p>Save the output as <code>azure-credentials.json</code> and create a Kubernetes secret in the <code>upbound-system</code> (assuming you use uxp).</p> <pre><code>kubectl create secret generic azure-secret \\\n-n upbound-system \\\n--from-file=creds=./azure-credentials.json\n</code></pre> <p>We then create a <code>ProviderConfig</code>, pointing to this credential.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: azure.upbound.io/v1beta1\nkind: ProviderConfig\nmetadata:\n  name: default\nspec:\n  credentials:\n    source: Secret\n    secretRef:\n      namespace: upbound-system\n      name: azure-secret\n      key: creds\nEOF\n</code></pre>"},{"location":"crossplane/providers/azure/#create-configure-managed-service-identity","title":"Create &amp; Configure Managed Service Identity","text":"<p>TBD</p>"},{"location":"crossplane/providers/helm/","title":"Helm provider","text":"<p>The Helm provider is a community provider. As the name suggests, it lets you manage Helm chart installations with Crossplane.</p>"},{"location":"crossplane/providers/helm/#install","title":"Install","text":"<p>You can install the provider via the up CLI or a Kubernetes manifest.</p> Upbound CLIKubernetes Manifest <pre><code>up controlplane provider install \\\nxpkg.upbound.io/crossplane-contrib/provider-helm:v0.12.0\n</code></pre> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: pkg.crossplane.io/v1\nkind: Provider\nmetadata:\n  name: provider-helm\nspec:\n  package: xpkg.upbound.io/crossplane-contrib/provider-helm:v0.12.0\nEOF\n</code></pre> <p>Once created, you can wait for the provider to become healthy.</p> <pre><code>kubectl wait --for=condition=\"Healthy\" providers.pkg.crossplane.io crossplane-contrib-provider-helm\n</code></pre>"},{"location":"crossplane/providers/helm/#configure","title":"Configure","text":"<p>The Helm provider needs Kubernetes credentials to install Helm charts. The following is required if installing in the same cluster as the provider.</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: helm.crossplane.io/v1beta1\nkind: ProviderConfig\nmetadata:\n  name: default\nspec:\n  credentials:\n    source: InjectedIdentity\nEOF\n</code></pre> <p>For options, look at the documentation and the examples.</p>"},{"location":"crossplane/providers/helm/#give-service-account-permissions","title":"Give Service Account permissions","text":"<p>The Helm provider needs several RBAC permissions. It does so via a generated Service Account, which you can find like this:</p> <pre><code>SA=$(kubectl -n upbound-system get sa -o name | grep provider-helm | sed -e 's|serviceaccount\\/|upbound-system:|g')\n</code></pre> <p>You can give it the specific RBAC configuration you want or the <code>cluster-admin</code> cluster role, as in the example below.</p> <p>Danger</p> <p>Do not do this in production environments.</p> <p>In production environments, ensure the Service Accounts have only the permissions you need.</p> <pre><code>kubectl create clusterrolebinding provider-helm-admin-binding --clusterrole cluster-admin --serviceaccount=\"${SA}\" || true\n</code></pre>"},{"location":"crossplane/providers/kubernetes/","title":"Kubernetes provider","text":"<p>The Kubernetes provider is a community provider. As the name suggests, it lets you manage Helm chart installations with Crossplane.</p>"},{"location":"crossplane/providers/kubernetes/#install","title":"Install","text":"<p>You can install the provider via the up CLI or a Kubernetes manifest.</p> Upbound CLIKubernetes Manifest <pre><code>up controlplane provider install \\\nxpkg.upbound.io/crossplane-contrib/provider-kubernetes:v0.5.0\n</code></pre> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: pkg.crossplane.io/v1\nkind: Provider\nmetadata:\n  name: provider-kubernetes\nspec:\n  package: xpkg.upbound.io/crossplane-contrib/provider-kubernetes:v0.5.0\nEOF\n</code></pre> <p>Once created, you can wait for the provider to become healthy.</p> <pre><code>kubectl wait --for=condition=\"Healthy\" providers.pkg.crossplane.io crossplane-contrib-provider-kubernetes\n</code></pre>"},{"location":"crossplane/providers/kubernetes/#configure","title":"Configure","text":"<p>The Kubernetes provider needs Kubernetes credentials to install Kubernetes manifests. The following is required if installing in the same cluster as the provider.</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: kubernetes.crossplane.io/v1alpha1\nkind: ProviderConfig\nmetadata:\n  name: default\nspec:\n  credentials:\n    source: InjectedIdentity\nEOF\n</code></pre>"},{"location":"crossplane/providers/kubernetes/#give-service-account-permissions","title":"Give Service Account permissions","text":"<p>The Kubernetes provider needs several RBAC permissions. It does so via a generated Service Account, which you can find like this:</p> <pre><code>SA=$(kubectl -n upbound-system get sa -o name | grep provider-kubernetes | sed -e 's|serviceaccount\\/|upbound-system:|g')\n</code></pre> <p>You can give it the specific RBAC configuration you want or the <code>cluster-admin</code> cluster role, as in the example below.</p> <p>Danger</p> <p>Do not do this in production environments.</p> <p>In production environments, ensure the Service Accounts have only the permissions you need.</p> <pre><code>kubectl create clusterrolebinding provider-kubernetes-admin-binding --clusterrole cluster-admin --serviceaccount=\"${SA}\" || true\n</code></pre>"},{"location":"crossplane/providers/terraform/","title":"UXP Terraform provider","text":"<p>In December 2022, Upbound released an official provider for Terraform. We recommend you use Upbound's provider over the community version.</p>"},{"location":"crossplane/providers/terraform/#install","title":"Install","text":"<p>You can install the provider via the up CLI or a Kubernetes manifest.</p> Upbound CLIKubernetes Manifest <pre><code>up controlplane provider install \\\nxpkg.upbound.io/upbound/provider-terraform:v0.2.0\n</code></pre> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: pkg.crossplane.io/v1\nkind: Provider\nmetadata:\n  name: provider-terraform\nspec:\n  package: xpkg.upbound.io/upbound/provider-terraform:v0.2.0\nEOF\n</code></pre> <p>Once created, you can wait for the provider to become healthy.</p> <pre><code>kubectl wait --for=condition=\"Healthy\" providers.pkg.crossplane.io provider-terraform\n</code></pre>"},{"location":"crossplane/providers/terraform/#give-service-account-permissions","title":"Give Service Account permissions","text":"<p>The Terraform provider needs several RBAC permissions. It does so via a generated Service Account, which you can find like this:</p> <pre><code>SA=$(kubectl -n upbound-system get sa -o name | grep provider-helm | sed -e 's|serviceaccount\\/|upbound-system:|g')\n</code></pre> <p>You can give it the specific RBAC configuration you want or the <code>cluster-admin</code> cluster role, as in the example below.</p> <p>Danger</p> <p>Do not do this in production environments.</p> <p>In production environments, ensure the Service Accounts have only the permissions you need.</p> <pre><code>kubectl create clusterrolebinding provider-helm-admin-binding --clusterrole cluster-admin --serviceaccount=\"${SA}\"\n</code></pre>"},{"location":"usecases/aws/packages/elasticache/ack/","title":"Consuming AWS Elasticache with ACK","text":"<p>This guide describes using the Services Toolkit to allow Tanzu Application Platform workloads to consume AWS Elasticache. This particular topic makes use of AWS Controller for Kubernetes (ACK) to manage AWS Elasticache instances.</p> <p>Note</p> <p>This usecase is not currently compatible with TAP air-gapped installations.</p>"},{"location":"usecases/aws/packages/elasticache/ack/#prerequisites","title":"Prerequisites","text":"<p>You need to meet a number of prerequisites before being able to effectively follow this guide.</p>"},{"location":"usecases/aws/packages/elasticache/ack/#create-service-instances-that-are-compatible-with-tanzu-application-platform","title":"Create service instances that are compatible with Tanzu Application Platform","text":"<p>The installation of the AWS Elasticache Controller for Kubernetes results in the availability of new Kubernetes APIs for interacting with Elasticache resources from within the TAP cluster.</p> <pre><code>kubectl api-resources --api-group elasticache.services.k8s.aws\n</code></pre> <pre><code>NAME                   SHORTNAMES   APIVERSION                              NAMESPACED   KIND\ncacheparametergroups                elasticache.services.k8s.aws/v1alpha1   true         CacheParameterGroup\ncachesubnetgroups                   elasticache.services.k8s.aws/v1alpha1   true         CacheSubnetGroup\nreplicationgroups                   elasticache.services.k8s.aws/v1alpha1   true         ReplicationGroup\nsnapshots                           elasticache.services.k8s.aws/v1alpha1   true         Snapshot\nusergroups                          elasticache.services.k8s.aws/v1alpha1   true         UserGroup\nusers                               elasticache.services.k8s.aws/v1alpha1   true         User\n</code></pre> <p>To create an AWS Elasticache service instance for consumption by Tanzu Application Platform, you can use a ready-made, reference Carvel Package. The Service Operator typically performs this step. Follow the steps in Creating an AWS Elasticache service instance using a Carvel Package.</p> <p>Alternatively, if you are interested in authoring your own Reference Package and want to learn about the underlying APIs and how they come together to produce a useable service instance for the Tanzu Application Platform, you can achieve the same outcome using the more advanced Creating an AWS Elasticache service instance manually.</p> <p>Once you have completed either of these steps and have a running AWS Elasticache service instance, please return here to continue with the rest of the use case.</p>"},{"location":"usecases/aws/packages/elasticache/ack/#create-a-service-instance-class-for-aws-elasticache","title":"Create a service instance class for AWS Elasticache","text":"<p>Now that you know how to create AWS Elasticache instances, it's time to learn how to make those instances discoverable to Application Operators. Again, this step is typically performed by the service operator persona.</p> <p>You can use Services Toolkit's <code>ClusterInstanceClass</code> API to create a service instance class to represent Elasticache service instances within the cluster. The existence of such classes makes these logical service instances discoverable to application operators, thus allowing them to create Resource Claims for such instances and to then bind them to application workloads.</p> <p>Create the following Kubernetes resource on your AKS cluster:</p> clusterinstanceclass.yaml<pre><code>---\napiVersion: services.apps.tanzu.vmware.com/v1alpha1\nkind: ClusterInstanceClass\nmetadata:\nname: aws-elasticache\nspec:\ndescription:\nshort: AWS Elasticache instances\npool:\nkind: Secret\nlabelSelector:\nmatchLabels:\nservices.apps.tanzu.vmware.com/class: aws-elasticache\n</code></pre> <pre><code>kubectl apply -f clusterinstanceclass.yaml\n</code></pre> <p>In this particular example, the class represents claimable instances of Postgresql by a <code>Secret</code> object with the label <code>services.apps.tanzu.vmware.com/class</code> set to <code>aws-elasticache</code>.</p> <p>In addition, you need to grant sufficient RBAC permissions to Services Toolkit to be able to read the secrets specified by the class. Create the following RBAC on your AKS cluster:</p> clusterrole.yaml<pre><code>---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\nname: stk-secret-reader\nlabels:\nservicebinding.io/controller: \"true\"\nrules:\n- apiGroups:\n- \"\"\nresources:\n- secrets\nverbs:\n- get\n- list\n- watch\n</code></pre> <pre><code>kubectl apply -f clusterrole.yaml\n</code></pre> <p>If you want to claim resources across namespace boundaries, you will have to create a corresponding <code>ResourceClaimPolicy</code>. For example, if the provisioned AWS Elasticache instance named <code>redis</code> exists in namespace <code>service-instances</code> and you want to allow App Operators to claim them for workloads residing in the <code>default</code> namespace, you would have to create the following <code>ResourceClaimPolicy</code>:</p> resourceclaimpolicy.yaml<pre><code>---\napiVersion: services.apps.tanzu.vmware.com/v1alpha1\nkind: ResourceClaimPolicy\nmetadata:\nname: default-can-claim-aws-elasticache\nnamespace: service-instances\nspec:\nsubject:\nkind: Secret\ngroup: \"\"\nselector:\nmatchLabels:\nservices.apps.tanzu.vmware.com/class: aws-elasticache\nconsumingNamespaces: [ \"default\" ]\n</code></pre> <pre><code>kubectl apply -f resourceclaimpolicy.yaml\n</code></pre>"},{"location":"usecases/aws/packages/elasticache/ack/#discover-claim-and-bind-to-an-aws-elasticache-for-redis-instance","title":"Discover, Claim, and Bind to an AWS Elasticache for Redis instance","text":"<p>The act of creating the <code>ClusterInstanceClass</code> and the corresponding RBAC essentially advertises to application operators that AWS Elasticache for Redis is available to use with their application workloads on Tanzu Application Platform. In this section, you learn how to discover, claim, and bind to the AWS Elasticache service instance previously created. Discovery and claiming service instances is typically the responsibility of the application operator persona. Binding is typically a step for Application Developers.</p> <p>To discover what service instances are available to them, application operators can run:</p> <pre><code>$ tanzu services classes list\n\nNAME             DESCRIPTION\n  aws-elasticache  AWS Elasticache instances\n</code></pre> <p>You can see information about the <code>ClusterInstanceClass</code> created in the previous step. Each <code>ClusterInstanceClass</code> created will be added to the list of classes returned here.</p> <p>The next step is to \"claim\" an instance of the desired class, but to do that, the application operators must first discover the list of currently claimable instances for the class. The capacity to claim instances is affected by many variables (including namespace boundaries, claim policies, and the exclusivity of claims) and so Services Toolkit provides a CLI command to help inform application operators of the instances that can result in successful claims. This command is the <code>tanzu service claimable list</code> command.</p> <pre><code>$ tanzu services claimable list --class aws-elasticache -n default\n\nNAME                         NAMESPACE          KIND    APIVERSION\n  redis-reader-creds-bindable  service-instances  Secret  v1\n  redis-writer-creds-bindable  service-instances  Secret  v1\n</code></pre> <p>Create a claim for the newly created secret by running:</p> <pre><code>tanzu services claim create redis-writer-claim \\\n--namespace default \\\n--resource-namespace service-instances \\\n--resource-name redis-writer-creds-bindable \\\n--resource-kind Secret \\\n--resource-api-version v1\n</code></pre> <p>Obtain the claim reference of the claim by running:</p> <pre><code>$ tanzu services claim list -o wide\n\nNAME                READY  REASON  CLAIM REF\n  redis-writer-claim  True   Ready   services.apps.tanzu.vmware.com/v1alpha1:ResourceClaim:redis-writer-claim\n</code></pre>"},{"location":"usecases/aws/packages/elasticache/ack/#test-claim-with-tap-workload","title":"Test Claim With TAP Workload","text":"<p>Create an application workload that consumes the claimed AWS Elasticache by running:</p> <p>Example:</p> <pre><code>tanzu apps workload create my-workload \\\n--git-repo &lt;a-git-repo&gt; \\\n--git-tag &lt;a-tag-to-checkout&gt; \\\n--type web \\\n--annotation autoscaling.knative.dev/minScale=1 \\\n--service-ref db=services.apps.tanzu.vmware.com/v1alpha1:ResourceClaim:redis-writer-claim\n</code></pre> <p><code>--service-ref</code> is set to the claim reference obtained previously.</p> <p>Your application workload starts and gets automatically the credentials to the AWS Elasticache instance via service bindings.</p>"},{"location":"usecases/aws/packages/elasticache/ack/#delete-an-aws-elasticache-service-instance-resources","title":"Delete an AWS Elasticache service instance resources","text":"<p>To delete the AWS Elasticache service instance, you can run the appropriate cleanup commands for how you created the service.</p>"},{"location":"usecases/aws/packages/elasticache/ack/#delete-an-aws-elasticache-instance-via-carvel-package","title":"Delete an AWS Elasticache instance via Carvel Package","text":"<pre><code>tanzu package installed delete redis-instance\n</code></pre>"},{"location":"usecases/aws/packages/elasticache/ack/#delete-an-aws-elasticache-instance-via-kubectl","title":"Delete an AWS Elasticache instance via Kubectl","text":"<p>Delete the AWS Elasticache instance by running:</p> <pre><code>kubectl delete -n service-instances replicationgroup.elasticache.services.k8s.aws redis\nkubectl delete -n service-instances usergroup.elasticache.services.k8s.aws redis\nkubectl delete -n service-instances user.elasticache.services.k8s.aws redis-default\nkubectl delete -n service-instances user.elasticache.services.k8s.aws redis-reader\nkubectl delete -n service-instances user.elasticache.services.k8s.aws redis-writer\nkubectl delete -n service-instances cachesubnetgroups.elasticache.services.k8s.aws ack-elasticache\nkubectl delete -n service-instances secrettemplate.secretgen.carvel.dev redis-reader-creds-bindable\nkubectl delete -n service-instances secrettemplate.secretgen.carvel.dev redis-writer-creds-bindable\nkubectl delete -n service-instances password.secretgen.carvel.dev redis-reader-creds\nkubectl delete -n service-instances password.secretgen.carvel.dev redis-writer-creds\nkubectl delete -n service-instances serviceaccounts redis-elasticache-reader\nkubectl delete -n service-instances role redis-elasticache-reader\nkubectl delete -n service-instances rolebinding redis-elasticache-reader\n</code></pre>"},{"location":"usecases/aws/packages/elasticache/ack/manual/","title":"Creating AWS Elasticache Instances manually using kubectl (experimental)","text":"<p>This topic describes how to use Services Toolkit to allow Tanzu Application Platform workloads to consume AWS Elasticache for Redis. This particular topic makes use of AWS Controller for Kubernetes (ACK) to manage AWS Elasticache resources.</p> <p>Following this guide, you will be creating all the resources in the <code>service-instances</code> namespace. It's important to make sure it exists before reading on.</p>"},{"location":"usecases/aws/packages/elasticache/ack/manual/#create-a-cache-subnet-group","title":"Create a cache subnet group","text":"<p>The Elasticache instances must be created in a Cache Subnet Group, that can be created in a number of ways, here you will leverage ACK to create it.</p> <p>First of all, you have to get the IDs of the subnets you want to build the Cache Subnet Group for.</p> <p>Example</p> <p>You could have a list of all the subnets in a given VPC and then choose amongst them.</p> <pre><code>VPC_NAME=\"my-vpc-name\"\nVPC_ID=$(aws ec2 describe-vpcs --filter \"Name=tag:Name,Values=${VPC_NAME}\" --query \"Vpcs[0].VpcId\" --output text)\nSUBNET_IDS=$(aws ec2 describe-subnets --filters \"Name=vpc-id,Values=${VPC_ID}\" --query \"Subnets[].SubnetId\")\n</code></pre> <p>This is just an example. Make sure you do choose your subnets carefully.</p> <p>When you have a list of subnetIDs stored in the <code>SUBNET_IDS</code> shell variable, you can create your <code>CacheSubnetGroup</code>. Create the following <code>ytt</code> template</p> cache-subnet-group.ytt.yaml<pre><code>#@ load(\"@ytt:data\", \"data\")\n---\napiVersion: elasticache.services.k8s.aws/v1alpha1\nkind: CacheSubnetGroup\nmetadata:\nname: ack-elasticache\nnamespace: service-instances\nspec:\ncacheSubnetGroupDescription: A subnet group for Elasticache\ncacheSubnetGroupName: #@ data.values.cacheSubnetGroupName\nsubnetIDs: #@ data.values.subnetIDs\n</code></pre> <p>Now you can use <code>ytt</code> to add the proper values and pipe it to <code>kubectl apply</code></p> <pre><code>CACHE_SUBNET_GROUP_NAME=\"ack-elasticache\"\n\nytt \\\n-v cacheSubnetGroupName=\"${CACHE_SUBNET_GROUP_NAME}\" \\\n-v subnetIDs=\"${SUBNET_IDS}\" \\\n-f cache-subnet-group.ytt.yaml \\\n| kubectl apply -f -\n</code></pre>"},{"location":"usecases/aws/packages/elasticache/ack/manual/#create-the-users-to-log-into-elasticache","title":"Create the users to log into Elasticache","text":"<p>You need at least one usergroup with at least one member user to associate to the Elasticache instance. Every group must have one user with name <code>default</code> and up to 100 total users (more on Elasticache quotas).</p> <p>However, there can be only one user with id <code>default</code> per Elasticache instance, which is automatically made available by AWS, but there can be more users with name <code>default</code> and different id. This is useful to know in order to create proper default users for each group.</p> <p>For the sake of this example, you will create just one user group and a default user in it with all the permissions. In order to generate a random password, you will make use of Secretgen Controller, which is provided out of the box by TAP, thus if you went through the prerequisites it should have already been installed.</p> <p>The following snippet declares the default user along with its auto-generated password and the usergroup:</p> elasticache-user.yaml<pre><code>---\napiVersion: secretgen.k14s.io/v1alpha1\nkind: Password\nmetadata:\nname: ack-elasticache-default-creds\nnamespace: service-instances\nspec:\nlength: 128\nsecretTemplate:\ntype: Opaque\nstringData:\npassword: $(value)\n---\napiVersion: elasticache.services.k8s.aws/v1alpha1\nkind: User\nmetadata:\nname: ack-elasticache-default\nnamespace: service-instances\nspec:\naccessString: on ~* +@all\nengine: redis\npasswords:\n- name: ack-elasticache-default-creds\nkey: password\nnamespace: service-instances\nuserID: ack-elasticache-default\nuserName: default\n---\napiVersion: elasticache.services.k8s.aws/v1alpha1\nkind: UserGroup\nmetadata:\nname: ack-elasticache\nnamespace: service-instances\nspec:\nengine: redis\nuserGroupID: ack-elasticache\nuserIDs:\n- ack-elasticache-default\n</code></pre> <p>Store it into the <code>elasticache-user.yaml</code> file and apply it:</p> <pre><code>kubectl apply -f elasticache-user.yaml\n</code></pre>"},{"location":"usecases/aws/packages/elasticache/ack/manual/#create-the-replicationgroup","title":"Create the ReplicationGroup","text":"<p>Before going ahead and create the <code>ReplicationGroup</code> resource, which maps to the actual instance that can be consumed, you need to create a proper security group for filtering the incoming and outgoing traffic.</p> <p>Assuming that you want to be able to connect to Elasticache from the EKS instance created previously, you can use the EKS security group as source for the new security group which will actually filter the Elasticache traffic.</p> <pre><code># AWS region you're operating in\nexport AWS_REGION=\"eu-west-1\"\n\n# EKS cluster name\nCLUSTER_NAME=\"my-eks-cluster\"\n\nEKS_SECURITY_GROUP_ID=$(aws eks describe-cluster --name ${CLUSTER_NAME} --query 'cluster.resourcesVpcConfig.clusterSecurityGroupId' --output text)\n\n# VPC_ID has been defined above in \"Create a cache subnet group\"\nELASTICACHE_SECURITY_GROUP_ID=$(aws ec2 create-security-group --group-name \"Elasticache\" --description \"Elasticache security group\" --vpc-id ${VPC_ID} --output text --query GroupId)\n\nREDIS_PORT=6379\naws ec2 authorize-security-group-ingress --group-id ${ELASTICACHE_SECURITY_GROUP_ID} --source-group ${EKS_SECURITY_GROUP_ID} --protocol tcp --port ${REDIS_PORT}\n</code></pre> <p>Now you can define the <code>ReplicationGroup</code> using the following <code>ytt</code> template</p> replication-group.ytt.yaml<pre><code>#@ load(\"@ytt:data\", \"data\")\n---\napiVersion: elasticache.services.k8s.aws/v1alpha1\nkind: ReplicationGroup\nmetadata:\nname: ack-elasticache\nnamespace: service-instances\nspec:\ndescription: A redis service instance\nengine: redis\nreplicationGroupID: ack-elasticache\ncacheNodeType: cache.t2.micro\ncacheSubnetGroupName: #@ data.values.cacheSubnetGroupName\nsecurityGroupIDs:\n- #@ data.values.securityGroupID\nuserGroupIDs:\n- ack-elasticache\n</code></pre> <p>and apply it</p> <pre><code>ytt \\\n-v cacheSubnetGroupName=\"${CACHE_SUBNET_GROUP_NAME}\" \\\n-v securityGroupID=\"${ELASTICACHE_SECURITY_GROUP_ID}\" \\\n-f replication-group.ytt.yaml \\\n| kubectl apply -f -\n</code></pre> <p>It will take 5 to 10 minutes to create. You can wait for the resource to be ready running the command</p> <pre><code>kubectl -n service-instances wait --for=condition=ACK.ResourceSynced=True replicationgroups.elasticache.services.k8s.aws ack-elasticache\n</code></pre> <p>or you can take a closer look at the new resource</p> <pre><code>kubectl -n service-instances get replicationgroups.elasticache.services.k8s.aws ack-elasticache -o yaml\n</code></pre> <p>particularly at the <code>status</code> field, which eventually will display something like</p> <pre><code>status:\n...\nconditions:\n- status: \"True\"\ntype: ACK.ResourceSynced\n...\nstatus: available\n</code></pre> <p>The status object also contains the details of the provisioned AWS resource, along with the nodegroups and their endpoints.</p>"},{"location":"usecases/aws/packages/elasticache/ack/manual/#create-a-binding-specification-compatible-secret","title":"Create a Binding Specification Compatible Secret","text":"<p>As mentioned in Creating service instances that are compatible with Tanzu Application Platform, in order for Tanzu Application Platform workloads to be able to claim and bind to services such as AWS Elasticache, a resource compatible with Service Binding Specification must exist in the cluster. This can take the form of either a <code>ProvisionedService</code>, as defined by the specification, or a Kubernetes <code>Secret</code> with some known keys, also as defined in the specification.</p> <p>In this guide, you create a Kubernetes secret in the necessary format using the secretgen-controller tooling. You do so by using the <code>SecretTemplate</code> API to extract values from the ACK resources and populate a new spec-compatible secret with the values.</p>"},{"location":"usecases/aws/packages/elasticache/ack/manual/#create-a-serviceaccount-for-secret-templating","title":"Create a ServiceAccount for Secret Templating","text":"<p>As part of using the <code>SecretTemplate</code> API, a Kubernetes <code>ServiceAccount</code> must be provided. The <code>ServiceAccount</code> is used for reading the <code>ReplicationGroup</code> resource and the <code>Secret</code> created from the <code>Password</code> resource above.</p> <p>Create the following Kubernetes resources on your EKS cluster:</p> rbac.yaml<pre><code>---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: ack-elasticache-reader\nnamespace: service-instances\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\nname: ack-elasticache-reader\nnamespace: service-instances\nrules:\n- apiGroups:\n- \"\"\nresources:\n- secrets\nverbs:\n- get\n- list\n- watch\nresourceNames:\n- ack-elasticache-default-creds\n- apiGroups:\n- elasticache.services.k8s.aws\nresources:\n- replicationgroups\nverbs:\n- get\n- list\n- watch\nresourceNames:\n- ack-elasticache\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\nname: ack-elasticache-reader\nnamespace: service-instances\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: Role\nname: ack-elasticache-reader\nsubjects:\n- kind: ServiceAccount\nname: ack-elasticache-reader\nnamespace: service-instances\n</code></pre> <pre><code>kubectl apply -f rbac.yaml\n</code></pre>"},{"location":"usecases/aws/packages/elasticache/ack/manual/#create-a-secrettemplate","title":"Create a SecretTemplate","text":"<p>In combination with the <code>ServiceAccount</code> just created, a <code>SecretTemplate</code> can be used to declaratively create a secret that is compatible with the service binding specification. For more information on this API see the Secret Template Documentation.</p> <p>Create the following Kubernetes resource on your EKS cluster:</p> secrettemplate.yaml<pre><code>apiVersion: secretgen.carvel.dev/v1alpha1\nkind: SecretTemplate\nmetadata:\nname: ack-elasticache-default-creds-bindable\nnamespace: service-instances\nspec:\nserviceAccountName: ack-elasticache-reader\ninputResources:\n- name: replicationGroup\nref:\napiVersion: elasticache.services.k8s.aws/v1alpha1\nkind: ReplicationGroup\nname: ack-elasticache\n- name: creds\nref:\napiVersion: v1\nkind: Secret\nname: ack-elasticache-default-creds\ntemplate:\nmetadata:\nlabels:\nservices.apps.tanzu.vmware.com/class: aws-elasticache\ntype: servicebinding.io/redis\nstringData:\ntype: redis\nusername: $(.creds.data.username)\nssl: \"true\"\nhost: $(.replicationGroup.status.nodeGroups[0].primaryEndpoint.address)\nport: $(.replicationGroup.status.nodeGroups[0].primaryEndpoint.port)\ndata:\npassword: $(.creds.data.password)\n</code></pre> <pre><code>kubectl apply -f secrettemplate.yaml\n</code></pre>"},{"location":"usecases/aws/packages/elasticache/ack/manual/#verify-the-service-instance","title":"Verify the Service Instance","text":"<p>Wait until the <code>ReplicationGroup</code> instance is ready as described before.</p> <p>Next, ensure a bindable <code>Secret</code> was produced by the <code>SecretTemplate</code>. To do so, run:</p> <pre><code>kubectl -n service-instances wait --for=condition=ReconcileSucceeded=True secrettemplates.secretgen.carvel.dev ack-elasticache-default-creds-bindable\n\nkubectl -n service-instances get secret ack-elasticache-default-creds-bindable\n</code></pre>"},{"location":"usecases/aws/packages/elasticache/ack/package/","title":"Creating AWS Elasticache instances by using a Carvel package (experimental)","text":"<p>This topic describes creating, updating, and deleting AWS Elasticache instances using a Carvel package. For a more detailed and low-level alternative procedure, see Creating Service Instances that are compatible with Tanzu Application Platform.</p>"},{"location":"usecases/aws/packages/elasticache/ack/package/#add-a-reference-package-repository-to-the-cluster","title":"Add a reference package repository to the cluster","text":"<p>The namespace <code>tanzu-package-repo-global</code> has a special significance. The kapp-controller defines a Global Packaging namespace. In this namespace, any package that is made available through a Package Repository is available in every namespace.</p> <p>When the kapp-controller is installed via Tanzu Application Platform, the namespace is <code>tanzu-package-repo-global</code>. If you install the controller in another way, verify which namespace is considered the Global Packaging namespace. You can use the following command to get the global namespace:</p> <pre><code>GLOBAL_NAMESPACE=$(kubectl -n kapp-controller get deployment kapp-controller -o json | jq -r '.spec.template.spec.containers[]|select(.name==\"kapp-controller\").args[]|select(.|startswith(\"-packaging-global-namespace\"))|split(\"=\")[1]')\n</code></pre> <p>To add a reference package repository to the cluster:</p> <ol> <li>Use the Tanzu CLI to add the new Service Reference packages repository:</li> </ol> <pre><code>tanzu package repository add tap-reference-service-packages \\\n--url ghcr.io/vmware-tanzu/tanzu-application-platform-reference-service-packages:0.0.3 \\\n-n ${GLOBAL_NAMESPACE}\n</code></pre> <ol> <li> <p>Create a <code>ServiceAccount</code> to provision <code>PackageInstall</code> resources by using the following example.    The namespace of this <code>ServiceAccount</code> must match the namespace of the <code>tanzu package install</code>    command in the next step.</p> rbac.yaml<pre><code>---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: elasticache-install\n---\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\nname: elasticache-install\nrules:\n- apiGroups: [\"elasticache.services.k8s.aws\"]\nresources: [\"*\"]\nverbs: [\"*\"]\n- apiGroups: [\"secretgen.carvel.dev\", \"secretgen.k14s.io\"]\nresources: [\"secrettemplates\",\"passwords\"]\nverbs:     [\"*\"]\n- apiGroups: [\"\"]\nresources: [\"serviceaccounts\",\"configmaps\"]\nverbs:     [\"*\"]\n- apiGroups: [\"\"]\nresources: [\"namespaces\"]\nverbs:     [\"get\", \"list\"]  - apiGroups: [\"rbac.authorization.k8s.io\"]\nresources: [\"roles\",\"rolebindings\"]\nverbs:     [\"*\"]\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\nname: elasticache-install\nsubjects:\n- kind: ServiceAccount\nname: elasticache-install\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: Role\nname: elasticache-install\n</code></pre> <pre><code>kubectl apply -f rbac.yaml\n</code></pre> </li> </ol>"},{"location":"usecases/aws/packages/elasticache/ack/package/#create-an-aws-elasticache-instance-through-the-tanzu-cli","title":"Create an AWS Elasticache instance through the Tanzu CLI","text":"<p>In order to configure the package installation, you must provide a values file. Here are some values highlighted:</p> <ul> <li><code>namespace</code> is the namespace where to deploy the AWS resources to, it may differ from the one   dedicated to the package(s).   The following example uses the <code>service-instances</code> namespace, do make sure it exists   or set <code>createNamespace: true</code>.</li> <li><code>cacheSubnetGroupName</code> is the name of the AWS <code>CacheSubnetGroup</code> to use for deploying the   Elasticache instance.   If it doesn't exist it can be created as part of the package setting the <code>createCacheSubnetGroup</code> flag   to <code>true</code> and providing the <code>subnetIDs</code> list.</li> <li><code>vpcSecurityGroupIDs</code> is a mandatory list of security group IDs that will be associated to the   Elasticache instances and will filter network traffic to/from them.</li> </ul> <p>Warning</p> <p>Because of the ephemeral nature of such IDs, if the security groups are destroyed and re-created, the package will need to be updated with the new values, otherwise Elasticache instances will become unreachable.</p> <p>It is recommended to set the value of the <code>name</code> field below from <code>redis</code> to something unique, using only lowercase letters, digits and hyphens. Do make sure you also change the commands below using a <code>redis</code> value, such as the <code>redis-writer-creds-bindable</code> from the SecretTemplate, and replace <code>redis</code> with the actual <code>name</code>.</p> <ol> <li> <p>Create a values file holding the configuration of the AWS Elasticache service instance:</p> redis-instance-values.yml<pre><code>---\nname: redis\nnamespace: service-instances\ncacheSubnetGroupName: redis-subnets\nreplicasPerNodeGroup: 1\nvpcSecurityGroupIDs:\n- sg-0a4ddae4fbf426cc8\ntags:\n- key: Generator\nvalue: Carvel package\n</code></pre> <p>Tip</p> <p>To understand which settings are available for this package you can run: <pre><code>tanzu package available get \\\n--values-schema elasticache.aws.references.services.apps.tanzu.vmware.com/0.0.1-alpha\n</code></pre> This shows a list of all configuration options you can use in the <code>redis-instance-values.yml</code> file.</p> </li> <li> <p>Use the Tanzu CLI to install an instance of the reference service instance package.</p> <pre><code>tanzu package install redis-instance \\\n--package-name elasticache.aws.references.services.apps.tanzu.vmware.com \\\n--version 0.0.1-alpha \\\n--service-account-name elasticache-install \\\n--values-file redis-instance-values.yml \\\n--wait\n</code></pre> </li> </ol> <p>You can install the <code>elasticache.aws.references.services.apps.tanzu.vmware.com</code> package multiple times to produce various AWS Elasticache instances. You create a separate <code>&lt;INSTANCE-NAME&gt;-values.yml</code> for each instance, set a different <code>name</code> value, and then install the package with the instance-specific data values file.</p>"},{"location":"usecases/aws/packages/elasticache/ack/package/#verify-the-aws-resources","title":"Verify the AWS Resources","text":"<p>Verify the creation status for the AWS Elasticache instance by inspecting the conditions in the Kubernetes API. To do so, run:</p> <pre><code>kubectl -n service-instances get replicationgroups.elasticache.services.k8s.aws redis -o yaml\n</code></pre> <p>After a few minutes, even up to 10 or more depending on how many replicas have been requested, you will be able to find the binding-compliant secrets produced by <code>PackageInstall</code>. Currently the package creates a <code>reader</code> and a <code>writer</code> user, each one with its own bindable secret. To view them, run:</p> <pre><code>kubectl -n service-instances get secrettemplate redis-reader-creds-bindable -o jsonpath=\"{.status.secret.name}\"\nkubectl -n service-instances get secrettemplate redis-writer-creds-bindable -o jsonpath=\"{.status.secret.name}\"\n</code></pre>"},{"location":"usecases/aws/packages/elasticache/ack/package/#verify-the-service-instance","title":"Verify the Service Instance","text":"<p>First, wait until the Elasticache instance is ready.</p> <pre><code>kubectl -n service-instances wait --for=condition=ACK.ResourceSynced=True replicationgroups.elasticache.services.k8s.aws ack-elasticache\n</code></pre> <p>Next, ensure a bindable <code>Secret</code> was produced by the <code>SecretTemplate</code>. To do so, run:</p> <pre><code>kubectl -n service-instances wait --for=condition=ReconcileSucceeded=True --timeout=5m secrettemplate redis-reader-creds-bindable\n\nkubectl -n service-instances get secret -n default redis-reader-creds-bindable\n</code></pre> <p>The same applies to the <code>redis-writer-creds-bindable</code> resources.</p>"},{"location":"usecases/aws/packages/elasticache/ack/package/#summary","title":"Summary","text":"<p>You have learnt to use Carvel's <code>Package</code> and <code>PackageInstall</code> APIs to create an AWS Elasticache instance. If you want to learn more about the pieces that comprise this service instance package, see Creating AWS Elasticache Instances manually using kubectl.</p> <p>Now that you have this available in the cluster, you can learn how to make use of it by continuing where you left off in Consuming AWS Elasticache with ACK.</p>"},{"location":"usecases/aws/packages/elasticache/ack/troubleshooting/","title":"Troubleshooting AWS ACK","text":""},{"location":"usecases/aws/packages/elasticache/ack/troubleshooting/#delete-default-user-before-group-is-deleted","title":"Delete default user before group is deleted","text":"<p>If a user named <code>default</code> is issued a <code>delete</code> command before the usergroup it belongs to has actually been deleted, ACK returns an unrecoverable error, thus preventing the deletion process to complete successfully.</p> <pre><code>  - message: \"DefaultUserAssociatedToUserGroup: User is associated to user group(s)\nas a default user and can't be deleted.\\n\\tstatus code: 400, request id: 65cb3646-e20f-4ba6-96c2-bf38ce13f78e\"\nstatus: \"True\"\ntype: ACK.Terminal\n</code></pre>"},{"location":"usecases/aws/prerequisites/","title":"AWS prerequisites","text":"<p>This is a list of prerequisites that must be satisfied in order to deploy a TAP Reference Package on AWS EKS and consume it through TAP:</p> <ol> <li> <p>Install <code>ytt</code>, a templating tool for YAML that    is being widely used in these guides.</p> </li> <li> <p>Install the AWS CLI. For how to do so, see the    AWS documentation.</p> </li> <li> <p>Log into AWS with your own credentials and assume a role with proper permissions    to deal with EKS and Elasticache services.    Check your AWS account number, user and assumed role running:</p> <pre><code>aws sts get-caller-identity\n</code></pre> </li> <li> <p>Make sure you have an available EKS cluster and the related OIDC provider configured.    In order to create a new one you can follow this guide.</p> </li> <li> <p>Install Tanzu Application Platform v1.2.0 or later and Cluster Essentials v1.2.0 or later on the Kubernetes cluster.    For more information, see Installing Tanzu Application Platform.</p> </li> <li> <p>Verify that you have the appropriate versions by running:</p> <pre><code>kubectl api-resources | grep secrettemplate\n</code></pre> <p>This command returns the <code>SecretTemplate</code> API.  If it does not work for you, you might not have Cluster Essentials for VMware Tanzu v1.2.0 or later installed.</p> </li> <li> <p>Only if you want to deploy reference packages based on ACK:    install the AWS Controller for Kubernetes (ACK) for the service(s) you are going to consume on AWS.</p> </li> <li> <p>Only if you want to deploy reference packages based on Crossplane:    install Crossplane and the AWS provider.</p> </li> </ol>"},{"location":"usecases/aws/prerequisites/ack/","title":"Install the AWS Controllers for Kubernetes (ACK)","text":"<p>AWS Controllers for Kubernetes (ACK) lets you define and use AWS service resources directly from Kubernetes, by mapping AWS services to Kubernetes CRDs.</p> <p>There's a number of different controllers to manage different AWS services, with different levels of maturity, listed in the documentation. The following example shows how to install the Elasticache controller, but the same concept applies to all of them.</p>"},{"location":"usecases/aws/prerequisites/ack/#install-elasticache-controller","title":"Install ElastiCache Controller","text":"<pre><code>SERVICE=\"elasticache\"\nRELEASE_VERSION=`curl -sL https://api.github.com/repos/aws-controllers-k8s/$SERVICE-controller/releases/latest | grep '\"tag_name\":' | cut -d'\"' -f4`\nACK_SYSTEM_NAMESPACE=\"ack-system\"\nAWS_REGION=\"eu-central-1\"\n\naws ecr-public get-login-password --region us-east-1 | \\\nhelm registry login --username AWS --password-stdin public.ecr.aws\n\nhelm install \\\nack-$SERVICE-controller \\\noci://public.ecr.aws/aws-controllers-k8s/$SERVICE-chart \\\n--create-namespace \\\n--namespace $ACK_SYSTEM_NAMESPACE \\\n--version=$RELEASE_VERSION \\\n--set=aws.region=$AWS_REGION\n</code></pre> <p>Warning</p> <p>The <code>--region</code> flag in the <code>aws ecr-public get-login-password</code> command must be set either to <code>us-east-1</code> or <code>us-west-2</code>, as described in ECR public AWS documentation.</p> <p>Set the <code>AWS_REGION</code> variable according to your needs and configure the IAM role for ACK's service account.</p> <p>If you followed the guide for creating the EKS cluster v1.23+ you should have already configured the OIDC provider for authentication, therefore you can skip to configuring the IAM role and policy for the service account.</p>"},{"location":"usecases/aws/prerequisites/eks/","title":"Create an EKS cluster with EBS CSI driver","text":"<p>An EKS cluster can be created in a number of ways, including AWS console, CLI, Terraform or CloudFormation. The quickest and simplest way to create an EKS cluster is to use eksctl, a CloudFormation wrapper, that will be used in this guide.</p>"},{"location":"usecases/aws/prerequisites/eks/#define-the-environment-parameters","title":"Define the environment parameters","text":"<p>The following are the parameters needed for the commands in this guide, which must be set the values that match your environment and needs.</p> <pre><code># AWS region you're operating in\nexport AWS_REGION=\"eu-central-1\"\n\n# autoscaling group minimum number of nodes\nASG_MIN_NODES=\"2\"\n\n# autoscaling group maximum number of nodes\nASG_MAX_NODES=\"4\"\n\n# EKS cluster name\nCLUSTER_NAME=\"my-eks-cluster\"\n\n# EKS kubernetes version to deploy\nKUBERNETES_VERSION=\"1.23\"\n</code></pre> <p>Info</p> <p>The <code>AWS_REGION</code> variable must be an environment variable (thus the <code>export</code>) to be used by <code>eksctl</code> and <code>aws</code> commands. The other variables can be just shell variables.</p> <p>Tip</p> <p>A list of available kubernetes versions for the <code>KUBERNETES_VERSION</code> variable can be obtained running</p> <pre><code>aws eks describe-addon-versions --query \"addons[].addonVersions[].compatibilities[].clusterVersion\" | jq 'unique|sort'\n</code></pre>"},{"location":"usecases/aws/prerequisites/eks/#create-the-eks-cluster","title":"Create the EKS cluster","text":"<p>The following command can create an EKS cluster based on the parameters defined above:</p> <pre><code>eksctl create cluster -m ${ASG_MIN_NODES} -M ${ASG_MAX_NODES} -n ${CLUSTER_NAME} --version ${KUBERNETES_VERSION}\n</code></pre> <p>The previous command waits until the cluster is created and also updates the <code>KUBECONFIG</code> file with the details of the new cluster.</p>"},{"location":"usecases/aws/prerequisites/eks/#configure-the-ebs-csi-controller","title":"Configure the EBS CSI controller","text":"<p>From version 1.23 onwards it is necessary to create the addon for the EBS CSI driver.</p> <pre><code>aws eks create-addon --cluster-name ${CLUSTER_NAME} --addon-name aws-ebs-csi-driver\n</code></pre> <p>EKS pods' service accounts can assume AWS IAM roles to be able to authenticate and interact with the AWS APIs. They are mapped to web identities via an IAM OIDC provider that must be created for the EKS cluster. You will then need to create a proper role for the EBS CSI controller to be able to create and manage EBS volumes.</p> <pre><code># define variables for IAM\nACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\nOIDC_URL=$(aws eks describe-cluster --name ${CLUSTER_NAME} --output text --query \"cluster.identity.oidc.issuer\")\nOIDC_ID=$(cut -d/ -f3- &lt;&lt;&lt;$OIDC_URL)\nEBS_ROLE=\"AmazonEKS_EBS_CSI_Driver-${CLUSTER_NAME}\"\nROLE_TRUST_POLICY=$(mktemp)\nROLE_PERMISSION_POLICY=$(mktemp)\n\n# prepare the trust policy document\ncat &gt; ${ROLE_TRUST_POLICY} &lt;&lt;EOF\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [{\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"Federated\": \"arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_ID}\"\n        },\n        \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n        \"Condition\": {\n            \"StringEquals\": {\n                \"${OIDC_ID}:sub\": \"system:serviceaccount:kube-system:ebs-csi-controller-sa\"\n            }\n        }\n    }]\n}\nEOF\n\n# create the role for the EBS CSI controller with the proper trust policy\naws iam create-role --role-name ${EBS_ROLE} --assume-role-policy-document file://${ROLE_TRUST_POLICY}\n\n# fetch a sample permission policy document\ncurl -sSfL -o ${ROLE_PERMISSION_POLICY} https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/master/docs/example-iam-policy.json\n\n# create the permission policy\nPERMISSION_POLICY_ARN=$(aws iam create-policy --policy-name ${EBS_ROLE} --policy-document file://${ROLE_PERMISSION_POLICY} --query Policy.Arn --output text)\n\n# attach the policy to the role\naws iam attach-role-policy --policy-arn ${PERMISSION_POLICY_ARN} --role-name ${EBS_ROLE}\n\n# clean up temporary files\nrm ${ROLE_TRUST_POLICY}\nrm ${ROLE_PERMISSION_POLICY}\n</code></pre> <p>Next, you must prepare the OIDC provider (see also AWS documentation). If the EKS cluster has just been created the OIDC provider does not exist yet, otherwise you can run the following command to make sure</p> <pre><code>aws iam list-open-id-connect-providers | grep $OIDC_ID\n</code></pre> <p>If no output is returned, the OIDC provider does not exist and you must create it.</p> Option 1 (quick)Option 2 (know what you are doing) <p>The <code>eksctl</code> does the heavy lifting for you and creates the OIDC provider with the proper configuration for your EKS cluster.</p> <pre><code>eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} --approve\n</code></pre> <p>Get rid of the <code>eksctl</code> magic and configure the OIDC provider yourself.</p> <p>Get the SHA1 fingerprint for validating the OIDC provider certificate: <pre><code>OIDC_HOST=$(echo $OIDC_ID | cut -d/ -f1)\nSHA1_FINGERPRINT=$({echo | openssl s_client -connect ${OIDC_HOST}:443 -servername ${OIDC_HOST} -showcerts | openssl x509 -fingerprint -noout -sha1} 2&gt;/dev/null | cut -d= -f2 | sed s/://g)\n</code></pre></p> <p>Create the OIDC provider: <pre><code>aws iam create-open-id-connect-provider --url ${OIDC_URL} --thumbprint-list ${SHA1_FINGERPRINT} --client-id-list sts.amazonaws.com\n</code></pre></p> <p>You must then configure the Kubernetes service account to assume the role</p> <pre><code># annotate the controller service account with the new role ARN\nkubectl -n kube-system annotate serviceaccount ebs-csi-controller-sa eks.amazonaws.com/role-arn=arn:aws:iam::${ACCOUNT_ID}:role/${EBS_ROLE}\n\n# restart ebs-csi-controller pods\nkubectl -n kube-system rollout restart deployment ebs-csi-controller\n</code></pre> <p>Your EKS cluster is now configured to dynamically allocate EBS volumes. You need to create the proper <code>StorageClass</code> resources for your <code>PersistentVolumeClaims</code> to use, like in this example.</p>"},{"location":"usecases/azure/packages/mongodb/crossplane/","title":"Consume MongoDB via Crossplane","text":"<p>This guide describes using the Services Toolkit to allow Tanzu Application Platform workloads to consume Azure MongoDB (via CosmosDB).</p> <p>This particular topic makes use of Universal Crossplane (UXP) by UpBound to manage resources in Azure.</p>"},{"location":"usecases/azure/packages/mongodb/crossplane/#prerequisites","title":"Prerequisites","text":"<ul> <li>UpBound's Universal Crossplane installed</li> <li>Tanzu Application Platform (TAP) 1.3.0 or higher installed</li> <li>Tanzu CLI with TAP plugins</li> </ul> <p>Note</p> <p>To test the automated binding of external services to applications, you need TAP installed. In addition, you need the Services Toolkit(STK) resources (ClusterInstanceClass, Claim), created.</p> <p>If you want to test the Crossplane bindings without TAP, skip the sections creating STK resources.  Instead of creating a TAP workload, we have an example Deployment (the Kubernetes CR), The expected secret is hard coded in the section Test Claim Without TAP.</p>"},{"location":"usecases/azure/packages/mongodb/crossplane/#create-service-instances-that-are-compatible-with-tanzu-application-platform","title":"Create service instances that are compatible with Tanzu Application Platform","text":"<p>For the sake of creating a MongoDB instance, we need to construct the following resources:</p> <ul> <li>Account</li> <li>MongoDatabase</li> <li>MongoCollection</li> <li>Secret containing the connection details in a supported format (Service Bindings)</li> </ul> <p>We also recommend, especially while testing or doing a PoC, creating a specific Resource Group for these resources.</p> <p>Success</p> <p>To instantiate the resources, we can either directly create the Crossplane instances, or we can use a Crossplane package that uses Crossplane's Compositions.</p> <p>Once your path of choice has reached the point the resources are created,  it will redirect you back here.</p>"},{"location":"usecases/azure/packages/mongodb/crossplane/#once-resources-are-ready","title":"Once Resources Are Ready","text":"<p>Once we have the MongoDB instance and the appropriate secret (adhering to Service Bindings), we can create a (STK) Claim.</p> <p>We create the following:</p> <ul> <li>ClusterInstanceClass</li> <li>Reader ClusterRole for Services Toolkit</li> <li>ResourceClaim</li> <li>TAP workload to test the database instance</li> </ul>"},{"location":"usecases/azure/packages/mongodb/crossplane/#create-a-cluster-instance-class-for-azure-mongodb","title":"Create a Cluster Instance Class for Azure MongoDB","text":"<p>We will create a generic class of bindable resources. So applications can be made aware of the specific name of the resources, or their bindable secret, to claim them.</p> cluster-instance-class.yml<pre><code>echo \"apiVersion: services.apps.tanzu.vmware.com/v1alpha1\nkind: ClusterInstanceClass\nmetadata:\n  name: azure-mongodb\nspec:\n  description:\n    short: azure mongodb\n  pool:\n    kind: Secret\n    group: \\\"\\\"\n    labelSelector:\n      matchLabels:\n        services.apps.tanzu.vmware.com/class: azure-mongodb\n\" &gt; cluster-instance-class.yml\n</code></pre> <pre><code>kubectl apply -f cluster-instance-class.yml\n</code></pre>"},{"location":"usecases/azure/packages/mongodb/crossplane/#services-toolkit-reader-cluster-role","title":"Services Toolkit Reader Cluster Role","text":"<p>We make sure the STK controller can read the secrets anywhere. Feel free to limit this role to specific named resources via the <code>resourceNames</code> property.</p> reader-cluster-role.yml<pre><code>echo \"apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: stk-secret-reader\n  labels:\n    servicebinding.io/controller: \\\"true\\\"\nrules:\n- apiGroups:\n  - \\\"\\\"\n  resources:\n  - secrets\n  verbs:\n  - get\n  - list\n  - watch\n\" &gt; reader-cluster-role.yml\n</code></pre> <pre><code>kubectl apply -f reader-cluster-role.yml\n</code></pre>"},{"location":"usecases/azure/packages/mongodb/crossplane/#discover-claim-and-bind-to-an-azure-mongodb-instance","title":"Discover, Claim, and Bind to an Azure MongoDB instance","text":"<p>First, confirm we have claimable class instances.</p> <pre><code>export RESOURCE_NAMESPACE=${UNIQUE_NAME:-\"default\"}\nexport RESOURCE_NAME=\n</code></pre> <p>Hint</p> <p>If you use the <code>instance</code> path to create the resources, the <code>RESOURCE_NAMESPACE</code> will be <code>$UNIQUE_NAME</code>. Else, it will be <code>default</code>.</p> <p>If you used the <code>instance</code> path, <code>RESOURCE_NAME</code> is also <code>$UNIQUE_NAME</code>, else if you use the package path, it is <code>$INSTANCE_NAME</code>.</p> <pre><code>tanzu services claimable list --class azure-mongodb \\\n-n ${RESOURCE_NAMESPACE}\n</code></pre> <p>Which should yield:</p> <pre><code>NAME                            NAMESPACE              KIND    APIVERSION\ntrp-mongodb-docs-test-bindable  trp-mongodb-docs-test  Secret  v1\n</code></pre> <p>We have to create the claim in the namespace of the consuming application. So set this variable to the namespace where you create TAP workloads.</p> <pre><code>export TAP_DEV_NAMESPACE=default\n</code></pre> <p>We then create the associated resource claim:</p> Using Tanzu CLIUsing Kubernetes Manifest <pre><code>tanzu service claim create azure-mongodb-claim-01 \\\n--namespace ${TAP_DEV_NAMESPACE} \\\n--resource-name ${RESOURCE_NAME}-bindable \\\n--resource-namespace ${RESOURCE_NAMESPACE} \\\n--resource-kind Secret \\\n--resource-api-version v1\n</code></pre> resource-claim.yml<pre><code>echo \"apiVersion: services.apps.tanzu.vmware.com/v1alpha1\nkind: ResourceClaim\nmetadata:\n  name: azure-mongodb-claim-01\n  namespace: ${TAP_DEV_NAMESPACE}\nspec:\n  ref:\n    apiVersion: v1\n    kind: Secret\n    name: ${RESOURCE_NAME}-bindable\n    namespace: ${RESOURCE_NAMESPACE}\n\" &gt; resource-claim.yml\n</code></pre> <pre><code>kubectl apply -f resource-claim.yml\n</code></pre> <p>To verify your claim is ready, you run this command:</p> <pre><code>tanzu service claim list -o wide </code></pre> <p>Which should yield the following:</p> <pre><code>NAME                    READY  REASON  CLAIM REF\nazure-mongodb-claim-01  True   Ready   services.apps.tanzu.vmware.com/v1alpha1:ResourceClaim:azure-mongodb-claim-01\n</code></pre> <p>You can now claim it with a TAP workload.</p>"},{"location":"usecases/azure/packages/mongodb/crossplane/#test-claim-with-tap-workload","title":"Test Claim With TAP Workload","text":"<p>We can now create a TAP workload that uses our resource claim.</p> <p>The runtime will fail once or twice as it takes time for the secret with the connection details to be mounted into the container. So wait for  <code>deployment-0002</code> or <code>0003</code>. </p> Tanzu CLIKubernetes Manifest <pre><code>tanzu apps workload create spring-boot-mongo-01 \\\n--namespace ${TAP_DEV_NAMESPACE} \\\n--git-repo https://github.com/joostvdg/spring-boot-mongo.git \\\n--git-branch main \\\n--type web \\\n--label app.kubernetes.io/part-of=spring-boot-mongo-01 \\\n--annotation autoscaling.knative.dev/minScale=1 \\\n--build-env BP_JVM_VERSION=17 \\\n--service-ref db=services.apps.tanzu.vmware.com/v1alpha1:ResourceClaim:azure-mongodb-claim-01 \\\n--yes\n</code></pre> <pre><code>echo \"apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\n  labels:\n    app.kubernetes.io/part-of: spring-boot-mongo-01\n    apps.tanzu.vmware.com/workload-type: web\n  name: spring-boot-mongo-01\n  namespace: ${TAP_DEV_NAMESPACE}\nspec:\n  build:\n    env:\n    - name: BP_JVM_VERSION\n      value: \\\"17\\\"\n  params:\n  - name: annotations\n    value:\n      autoscaling.knative.dev/minScale: \\\"1\\\"\n  serviceClaims:\n  - name: db\n    ref:\n      apiVersion: services.apps.tanzu.vmware.com/v1alpha1\n      kind: ResourceClaim\n      name: azure-mongodb-claim-01\n  source:\n    git:\n      ref:\n        branch: main\n      url: https://github.com/joostvdg/spring-boot-mongo.git\n\" &gt; workload.yml\n</code></pre> <pre><code>kubectl apply -f workload.yml\n</code></pre> <p>To see the logs: <pre><code>tanzu apps workload tail spring-boot-mongo-01\n</code></pre></p> <p>To get the status:</p> <pre><code>tanzu apps workload get spring-boot-mongo-01\n</code></pre> <p>Tap creates the deployment when the build and config writer workflows are complete.</p> <p>To see their pods:</p> <pre><code>kubectl get pod -l app.kubernetes.io/part-of=spring-boot-mongo-01\n</code></pre> <p>Then you can wait for the application to be ready via the <code>kubectl</code> CLI.</p> <pre><code>kubectl wait --for=condition=ready \\\npod -l app.kubernetes.io/component=run,app.kubernetes.io/part-of=spring-boot-mongo-01 \\\n--timeout=180s \\\n--namespace ${TAP_DEV_NAMESPACE}\n</code></pre> <p>Ensure there is only one deployment active:</p> <pre><code>kubectl get pod --namespace ${TAP_DEV_NAMESPACE} \\\n-l app.kubernetes.io/component=run,app.kubernetes.io/part-of=spring-boot-mongo-01\n</code></pre> <p>Which should list a single deployment with Ready 2/2:</p> <pre><code>NAME                                                    READY   STATUS    RESTARTS   AGE\nspring-boot-mongo-01-00002-deployment-8cd56bdc8-gb44n   2/2     Running   0          6m11s\n</code></pre> <p>We then collect the name of the **Pod**or copy it yourself from the command-line output.</p> <pre><code>POD_NAME=$(kubectl get pod \\\n--namespace ${TAP_DEV_NAMESPACE} \\\n-l app.kubernetes.io/component=run,app.kubernetes.io/part-of=spring-boot-mongo-01 \\\n-o jsonpath=\"{.items[0].metadata.name}\")\n</code></pre> <p>Continue with testing the application</p>"},{"location":"usecases/azure/packages/mongodb/crossplane/#test-claim-without-tap","title":"Test Claim Without TAP","text":"<p>Here's the same application we use as TAP workload, but with a hardcoded Deployment.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/joostvdg/spring-boot-mongo/main/kubernetes/raw/deployment.yaml </code></pre> <p>As the Deployment is hard coded, it doesn't know the name of your bindable secret. So let's patch that; first, create a patch file.</p> spring-boot-mongo-deployment-patch.yml<pre><code>echo \"\n---\nspec:\n  template:\n    spec:\n      volumes:\n      - name: secret-volume\n        projected:\n          defaultMode: 420\n          sources:\n          - secret:\n              name: \"${RESOURCE_NAME}-bindable\"\n\" &gt; spring-boot-mongo-deployment-patch.yml\n</code></pre> <p>And second, patch the deployment to update the secret name:</p> <pre><code>kubectl patch --type merge deploy spring-boot-mongo\\\n--patch-file='spring-boot-mongo-deployment-patch.yml' \\\n--namespace ${TAP_DEV_NAMESPACE}\n</code></pre> <p>We wait for the Pod to become Ready.</p> <pre><code>kubectl wait --for=condition=ready pod \\\n--namespace ${TAP_DEV_NAMESPACE} \\\n-l app.kubernetes.io/name=spring-boot-mongo \\\n--timeout=60s\n</code></pre> <p>Once that happens, we copy the Pod's name to test the application.</p> <pre><code>POD_NAME=$(kubectl get pod \\\n--namespace ${TAP_DEV_NAMESPACE} \\\n-l app.kubernetes.io/name=spring-boot-mongo \\\n-o jsonpath=\"{.items[0].metadata.name}\")\n</code></pre>"},{"location":"usecases/azure/packages/mongodb/crossplane/#test-application","title":"Test Application","text":"<p>Use the <code>kubectl</code> CLI to create a port forward.</p> <pre><code>kubectl port-forward pod/${POD_NAME} --namespace ${TAP_DEV_NAMESPACE} 8080\n</code></pre> <p>And then open another terminal to test the application:</p> <pre><code>curl -s \"http://localhost:8080\"\n</code></pre> <p>Which should return an empty list <code>[]</code>. Add a value that gets stored in the MongoDB instance.</p> <pre><code>curl --header \"Content-Type: application/json\" \\\n--request POST --data '{\"name\":\"Alice\"}' \\\nhttp://localhost:8080/create\n</code></pre> <p>Making another GET request should return the stored entry:</p> <pre><code>curl -s \"http://localhost:8080\"\n</code></pre> <p>Success</p> <p>We have gone through all the steps. You can now clean up all the resources.</p>"},{"location":"usecases/azure/packages/mongodb/crossplane/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete ResourceClaim azure-mongodb-claim-01 \\\n--namespace ${TAP_DEV_NAMESPACE}\n</code></pre> TAP WorkloadDeployment without TAP <pre><code>kubectl delete workload spring-boot-mongo-01\n</code></pre> <pre><code>kubectl delete deploy spring-boot-mongo || true\n</code></pre> <p>Once you have cleaned up the resources related to this guide, visit the specific sub-guides for their cleanup commands.</p> <ul> <li>Via Crossplane instances cleanup commands</li> <li>Via Crossplane package cleanup commands</li> </ul>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/","title":"Via Crossplane Instances","text":"<p>To create a MongoDB instance in Azure, we need the following resources:</p> <ul> <li>ResourceGroup</li> <li>CosmosDB Account</li> <li>MongoDatabase</li> <li>MongoCollection</li> </ul> <p>We will show you how to create all of them with Crossplane's  official Azure provider.</p> <p>Important</p> <p>We suggest defining a unique name for the resources to avoid potential conflicts. This is because we need the Resource Group and the CosmosDB Account names to be globally unique (within Azure).</p> <pre><code>export UNIQUE_NAME=\n</code></pre> <p>We must set a location where Azure creates the resources. As a default, we use <code>West Europe</code>, but feel free to change this.</p> <pre><code>export LOCATION=\"West Europe\"\n</code></pre> <p>The examples help you create the files with the <code>UNIQUE_NAME</code>     and <code>LOCATION</code> values are inserted in the places that matter.</p>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>UpBound's Universal Crossplane</li> <li>Crossplane Azure Provider</li> <li>SecretGen Controller: to transform the secret from Crossplane to the Service Binding specification</li> </ul>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#verify-provider","title":"Verify Provider","text":"<pre><code>kubectl api-resources --api-group cosmosdb.azure.upbound.io\n</code></pre> <p>This gives us the following APIs:</p> <pre><code>NAME                   SHORTNAMES   APIVERSION                          NAMESPACED   KIND\naccounts                            cosmosdb.azure.upbound.io/v1beta1   false        Account\ncassandraclusters                   cosmosdb.azure.upbound.io/v1beta1   false        CassandraCluster\ncassandradatacenters                cosmosdb.azure.upbound.io/v1beta1   false        CassandraDatacenter\ncassandrakeyspaces                  cosmosdb.azure.upbound.io/v1beta1   false        CassandraKeySpace\ncassandratables                     cosmosdb.azure.upbound.io/v1beta1   false        CassandraTable\ngremlindatabases                    cosmosdb.azure.upbound.io/v1beta1   false        GremlinDatabase\ngremlingraphs                       cosmosdb.azure.upbound.io/v1beta1   false        GremlinGraph\nmongocollections                    cosmosdb.azure.upbound.io/v1beta1   false        MongoCollection\nmongodatabases                      cosmosdb.azure.upbound.io/v1beta1   false        MongoDatabase\nsqlcontainers                       cosmosdb.azure.upbound.io/v1beta1   false        SQLContainer\nsqldatabases                        cosmosdb.azure.upbound.io/v1beta1   false        SQLDatabase\nsqlfunctions                        cosmosdb.azure.upbound.io/v1beta1   false        SQLFunction\nsqlroleassignments                  cosmosdb.azure.upbound.io/v1beta1   false        SQLRoleAssignment\nsqlroledefinitions                  cosmosdb.azure.upbound.io/v1beta1   false        SQLRoleDefinition\nsqlstoredprocedures                 cosmosdb.azure.upbound.io/v1beta1   false        SQLStoredProcedure\nsqltriggers                         cosmosdb.azure.upbound.io/v1beta1   false        SQLTrigger\ntables                              cosmosdb.azure.upbound.io/v1beta1   false        Table\n</code></pre> <p>We also recommend, especially while testing or doing a PoC, creating a specific Resource Group for these resources.</p> <p>The Resource Group API is part of the <code>azure.upbound.io</code> API group.</p> <pre><code>kubectl api-resources --api-group azure.upbound.io\n</code></pre> <p>As you can see:</p> <pre><code>NAME                            SHORTNAMES   APIVERSION                  NAMESPACED   KIND\nproviderconfigs                              azure.upbound.io/v1beta1    false        ProviderConfig\nproviderconfigusages                         azure.upbound.io/v1beta1    false        ProviderConfigUsage\nresourcegroups                               azure.upbound.io/v1beta1    false        ResourceGroup\nresourceproviderregistrations                azure.upbound.io/v1beta1    false        ResourceProviderRegistration\nstoreconfigs                                 azure.upbound.io/v1alpha1   false        StoreConfig\nsubscriptions                                azure.upbound.io/v1beta1    false        Subscription\n</code></pre>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#create-namespace","title":"Create Namespace","text":"<p>There are some places where we need to provide the name of the namespace.</p> <p>For the convenience of this example, we create a namespace with the same name.</p> <pre><code>kubectl create namespace ${UNIQUE_NAME}\n</code></pre>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#create-resource-group","title":"Create Resource Group","text":"<p>We create a ResourceGroup with a unique name and label it. This is because some of the managed resources (i.e., the resources in Azure) depend on other managed resources.</p> <p>To find them, we must instruct Crossplane, which Crossplane resource maps to the dependent managed resource. For this, we use the label.</p> resourcegroup.yml<pre><code>echo \"apiVersion: azure.upbound.io/v1beta1\nkind: ResourceGroup\nmetadata:\n  name: ${UNIQUE_NAME}\n  namespace: ${UNIQUE_NAME}\n  labels:\n    testing.upbound.io/example-name: ${UNIQUE_NAME}\nspec:\n  forProvider:\n    location: ${LOCATION}\n  providerConfigRef:\n    name: default\n\" &gt; resourcegroup.yml\n</code></pre> <p>And now, you can apply the file to create the Crossplane resource. Again, the Crossplane controller and the Azure provider controller will ensure the resource in Azure exists and the status is visible on our Crossplane resource in Kubernetes.</p> <pre><code>kubectl apply -f resourcegroup.yml\n</code></pre>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#create-cosmosdb-account","title":"Create CosmosDB Account","text":"<p>Next on the list is the CosmosDB Account. This object defines the initial configuration of the MongoDB instance.</p> <p>You make this Account a MongoDB instance by setting the capability <code>EnableMongo</code>. If you want to set a specific version, use the <code>spec.forProvider.mongoServerVersion</code> parameter.</p> account.yml<pre><code>echo \"apiVersion: cosmosdb.azure.upbound.io/v1beta1\nkind: account\nmetadata:\n  annotations:\n    meta.upbound.io/example-id: cosmosdb/v1beta1/mongocollection\n  labels:\n    testing.upbound.io/example-name: ${UNIQUE_NAME}\n  name: ${UNIQUE_NAME}\n  namespace: ${UNIQUE_NAME}\nspec:\n  forProvider:\n    capabilities:\n      - name: EnableMongo\n      - name: mongoEnableDocLevelTTL\n    consistencyPolicy:\n      - consistencyLevel: Strong\n    geoLocation:\n      - failoverPriority: 0\n        location: ${LOCATION}\n    kind: MongoDB\n    location: ${LOCATION}\n    offerType: Standard\n    resourceGroupNameSelector:\n      matchLabels:\n        testing.upbound.io/example-name: ${UNIQUE_NAME}\n  writeConnectionSecretToRef:\n    namespace: ${UNIQUE_NAME}\n    name: ${UNIQUE_NAME}\n\" &gt; account.yml\n</code></pre> <p>Create the file and apply it to the cluster.</p> <pre><code>kubectl apply -f account.yml\n</code></pre> <p>Info</p> <p>The Account requires a reference to a Resource Group.</p> <p>We do so via the <code>resourceGroupNameSelector</code>:</p> <pre><code>resourceGroupNameSelector:\nmatchLabels:\ntesting.upbound.io/example-name: ${UNIQUE_NAME}\n</code></pre>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#create-mongo-database","title":"Create Mongo Database","text":"<p>Now that we have an Account that manages a MongoDB instance, we can create a MongoDB Database in the Account.</p> database.yml<pre><code>echo \"apiVersion: cosmosdb.azure.upbound.io/v1beta1\nkind: MongoDatabase\nmetadata:\n  annotations:\n    meta.upbound.io/example-id: cosmosdb/v1beta1/mongocollection\n  labels:\n    testing.upbound.io/example-name: ${UNIQUE_NAME}\n  name: ${UNIQUE_NAME}\n  namespace: ${UNIQUE_NAME}\nspec:\n  forProvider:\n    accountNameSelector:\n      matchLabels:\n        testing.upbound.io/example-name: ${UNIQUE_NAME}\n    resourceGroupNameSelector:\n      matchLabels:\n        testing.upbound.io/example-name: ${UNIQUE_NAME}\n\" &gt; database.yml\n</code></pre> <pre><code>kubectl apply -f database.yml\n</code></pre>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#create-mongo-collection","title":"Create Mongo Collection","text":"<p>In the MongoDB Database, we want to have at least one Collection (read more about Database and Collection).</p> <p>All the same rules as before apply. We define the resource's properties and then reference the dependencies.</p> collection.yml<pre><code>echo \"apiVersion: cosmosdb.azure.upbound.io/v1beta1\nkind: MongoCollection\nmetadata:\n  annotations:\n    meta.upbound.io/example-id: cosmosdb/v1beta1/mongocollection\n  labels:\n    testing.upbound.io/example-name: ${UNIQUE_NAME}\n  name: ${UNIQUE_NAME}\n  namespace: ${UNIQUE_NAME}\nspec:\n  forProvider:\n    defaultTtlSeconds: 777\n    index:\n    - keys:\n      - _id\n      unique: true\n    shardKey: uniqueKey\n    throughput: 400\n    accountNameSelector:\n      matchLabels:\n        testing.upbound.io/example-name: ${UNIQUE_NAME}\n    databaseNameSelector:\n      matchLabels:\n        testing.upbound.io/example-name: ${UNIQUE_NAME}\n    resourceGroupNameSelector:\n      matchLabels:\n        testing.upbound.io/example-name: ${UNIQUE_NAME}\n\" &gt; collection.yml\n</code></pre> <p>And as always, we apply the resource to the cluster and let Crossplane work its magic.</p> <pre><code>kubectl apply -f collection.yml\n</code></pre>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#verify-managed-resource-creation","title":"Verify Managed Resource Creation","text":"<p>Create the Crossplane resources and then verify the resources have a successful reconciliation.</p> <pre><code>kubectl get resourcegroup,mongocollection,mongodatabase,account \\\n--namespace ${UNIQUE_NAME}\n</code></pre> <p>This should yield something like this: (where <code>trp-cosmosdb-mongo-01</code> will be your <code>$UNIQUE_NAME</code>)</p> <pre><code>NAME                                                            READY   SYNCED   EXTERNAL-NAME           AGE\nresourcegroup.azure.upbound.io/trp-cosmosdb-mongo-01            True    True     trp-cosmosdb-mongo-01   26h\n\nNAME                                                            READY   SYNCED   EXTERNAL-NAME           AGE\nmongocollection.cosmosdb.azure.upbound.io/trp-cosmosdb-mongo-01 True    True     trp-cosmosdb-mongo-01   26h\n\nNAME                                                            READY   SYNCED   EXTERNAL-NAME           AGE\nmongodatabase.cosmosdb.azure.upbound.io/trp-cosmosdb-mongo-01   True    True     trp-cosmosdb-mongo-01   26h\n\nNAME                                                            READY   SYNCED   EXTERNAL-NAME           AGE\naccount.cosmosdb.azure.upbound.io/trp-cosmosdb-mongo-01         True    True     trp-cosmosdb-mongo-01   26h\n</code></pre> <p>To use the MongoDB instance from our application, we need to bind the connection details.</p> <p>The secret generated by Crossplane via the <code>writeConnectionSecretToRef</code> does not use the expected syntax. So we use the SecretGen controller with a <code>SecretTemplate</code> to generate a secret that does.</p>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#create-a-connection-details-secret","title":"Create a connection details secret","text":"<p>The Services Toolkit needs a Kubernetes Secret with a format that adheres to the Service Binding specification.</p> <p>Unfortunately, the secret that the Account manage resource generates does not conform. We use a SecretTemplate that maps the Account secret to a secret usable by the automatic binding.</p>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#secretgen-rbac-permissions","title":"SecretGen RBAC permissions","text":"<p>The SecretGen Controller needs permissions in the secret's namespace. So we create a <code>ServiceAccount</code> with the appropriate permissions and then the <code>SecretTemplate</code>.</p> secretgen-rbac.yml<pre><code>echo \"apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: ${UNIQUE_NAME}\n  namespace: ${UNIQUE_NAME}\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: secret-gen-reader\n  namespace: ${UNIQUE_NAME}\nrules:\n- apiGroups:\n  - \\\"\\\"\n  resources:\n  - secrets\n  verbs:\n  - get\n  - list\n  - watch\n  resourceNames:\n  - ${UNIQUE_NAME}\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: ${UNIQUE_NAME}-role-binding\n  namespace: ${UNIQUE_NAME}\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: secret-gen-reader\nsubjects:\n- kind: ServiceAccount\n  name: ${UNIQUE_NAME}\n  namespace: ${UNIQUE_NAME}\n\" &gt; secretgen-rbac.yml\n</code></pre> <pre><code>kubectl apply -f secretgen-rbac.yml\n</code></pre>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#secret-template","title":"Secret Template","text":"<p>And then we can create a SecretTemplate that transforms the secret from the Crossplane Account to a secret that the Services Toolkit can bind.</p> secret-template.yml<pre><code>echo \"apiVersion: secretgen.carvel.dev/v1alpha1\nkind: SecretTemplate\nmetadata:\n  name: ${UNIQUE_NAME}-bindable\n  namespace: ${UNIQUE_NAME}\nspec:\n  serviceAccountName: ${UNIQUE_NAME}\n  inputResources:\n  - name: creds\n    ref:\n      apiVersion: v1\n      kind: Secret     \n      name: ${UNIQUE_NAME}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: ${UNIQUE_NAME}\n        app.kubernetes.io/instance: ${UNIQUE_NAME}\n        services.apps.tanzu.vmware.com/class: azure-mongodb\n    type: mongodb\n    stringData:\n      type: mongodb\n      database: ${UNIQUE_NAME}\n    data:\n      uri: '\\$(.creds.data.attribute\\\\.connection_strings\\\\.0)'\n\" &gt; secret-template.yml\n</code></pre> <pre><code>kubectl apply -f secret-template.yml\n</code></pre> <p>Resource Claim Policy</p> <p>If we create our Claim in one namespace and the claimable resource in another, we need a ResourceClaimPolicy.</p> <p>See below how to create one!</p> <p>We verify the SecretTemplate's secret exists by running the following command:</p> <pre><code>kubectl get secret -n $UNIQUE_NAME\n</code></pre> <p>This should give you two secrets: (where <code>trp-mongodb-docs-test</code> will be your <code>$UNIQUE_NAME</code>)</p> <pre><code>NAME                             TYPE                                DATA   AGE\ntrp-mongodb-docs-test            connection.crossplane.io/v1alpha1   8      24m\ntrp-mongodb-docs-test-bindable   mongodb                             3      94s\n</code></pre>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#resourceclaim-policy-optional","title":"ResourceClaim Policy (optional)","text":"<p>We determine the Developer namespace configured in the Tanzu Application Platform (TAP) installation. If in doubt, assume it is <code>default</code>.</p> <pre><code>export TAP_DEV_NAMESPACE=default\n</code></pre> <p>Then you create a ResourceClaimPolicy ensuring applications deployed in TAP can use the Service Toolkit claim.</p> resource-claim-policy.yml<pre><code>echo \"apiVersion: services.apps.tanzu.vmware.com/v1alpha1\nkind: ResourceClaimPolicy\nmetadata:\n  name: default-can-claim-azure-mongodb\n  namespace: ${UNIQUE_NAME}\nspec:\n  subject:\n    kind: Secret\n    group: \\\"\\\"\n    selector:\n      matchLabels:\n        services.apps.tanzu.vmware.com/class: azure-mongodb\n  consumingNamespaces: [ \\\"${TAP_DEV_NAMESPACE}\\\" ] \n\" &gt; resource-claim-policy.yml\n</code></pre> <p>Verify the status of the ResourceClaimPolicy:</p> <pre><code>kubectl apply -f resource-claim-policy.yml -n ${UNIQUE_NAME}\n</code></pre>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#next-steps","title":"Next Steps","text":"<p>Success</p> <p>You can return to the main guide to continue with the Services Toolkit sections.</p> <p>You should come back for the cleanup commands; see below.</p>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-instance/#cleanup","title":"Cleanup","text":"<p>Danger</p> <p>These commands delete the resources in Azure.</p> <p>Crossplane uses finalizers on its resources. This ensures that the <code>kubectl delete</code> blocks until it confirms the resources are removed at the provider level.</p> <pre><code>kubectl delete -f resource-claim-policy.yml || true\nkubectl delete -f secret-template.yml || true\nkubectl delete -f collection.yml || true\nkubectl delete -f database.yml || true\nkubectl delete -f account.yml || true\nkubectl delete -f resourcegroup.yml || true\nkubectl delete namespace ${UNIQUE_NAME} || true\n</code></pre> <p>Warning</p> <p>If you are done with this Crossplane Provider, or want to try out the Crossplane package solution, you have to delete the Provider and ProviderConfig resources as well.</p> <p>Info</p> <p>ProviderConfig resources are owned by the Provider installed at the time. So installing the Azure Provider again via a package dependency, for example, would be blocked because the ProviderConfig is owned by the previous Provider installation.</p> <p>Resources, such as the MongoDB Database, using the ProviderConfig block its deletion.</p> <p>A ProviderConfig resource belongs to the Provider and has a unique Group. So to avoid possible conflicts with other ProviderConfigs, we delete it via its full name.</p> <pre><code>kubectl delete providerconfigs.azure.upbound.io default\nkubectl delete providers.pkg.crossplane.io upbound-provider-azure\n</code></pre>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-package/","title":"Via Crossplane Package","text":"<p>In this section, we look at installing a Crossplane package, and then use the resources provided by this package to create the MongoDB instance.</p>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-package/#prerequisites","title":"Prerequisites","text":"<ul> <li>UpBound's Universal Crossplane</li> </ul>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-package/#install-package","title":"Install Package","text":"<p>Or use a Kubernetes resource file:</p> Upbound CLIKubernetes manifest <p>Do make sure you have installed the <code>up</code> CLI, as described here, and execute: <pre><code>up controlplane configuration install \\\n--name azure-mongodb \\\nghcr.io/vmware-tanzu/tanzu-application-platform-reference-service-packages/azure/crossplane/mongodb:0.0.4\n</code></pre></p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: pkg.crossplane.io/v1\nkind: Configuration\nmetadata:\n    name: azure-mongodb\nspec:\n    package: ghcr.io/vmware-tanzu/tanzu-application-platform-reference-service-packages/azure/crossplane/mongodb:0.0.4\nEOF\n</code></pre> <p>The Crossplane docs clarify what you can configure in a Configuration.</p> <p>To verify the Configuration has been installed successfully, run this command:</p> <pre><code>kubectl get configuration,configurationrevision\n</code></pre> <p>Which should yield something like this:</p> <pre><code>NAME                                            INSTALLED   HEALTHY   PACKAGE                                                                                                            AGE\nconfiguration.pkg.crossplane.io/azure-mongodb   True        True      ghcr.io/vmware-tanzu/tanzu-application-platform-reference-service-packages/azure/crossplane/mongodb:0.0.4          13m\n\nNAME                                                                 HEALTHY   REVISION   IMAGE                                                                                                              STATE    DEP-FOUND   DEP-INSTALLED   AGE\nconfigurationrevision.pkg.crossplane.io/azure-mongodb-99c3a327f258   True      1          ghcr.io/vmware-tanzu/tanzu-application-platform-reference-service-packages/azure/crossplane/mongodb:0.0.4          Active   1           1               13m\n</code></pre> <p>Tip</p> <p>If the Configuration does not turn healthy within a few minutes, the information will be in the events.</p> <p>So use <code>kubectl describe</code>:</p> <pre><code>kubectl describe configurationrevision\n</code></pre> <p>This Configuration package lists the UpBound Azure Provider as its dependency. Crossplane resolves and installs the dependencies for you.</p> <p>If you have already installed the Provider, you can either remove it (and any related ProviderConfig), or disable the dependency resolution.</p> <p>It does look like the <code>up</code> CLI currently does not support this, so you have to use the manifest solution instead. Setting the following option: <code>spec.skipDependencyResolution: true</code>.</p> <p>To verify the Provider is up and running:</p> <pre><code>kubectl get provider\n</code></pre> <p>This should yield the following:</p> <pre><code>NAME                     INSTALLED   HEALTHY   PACKAGE                                          AGE\nupbound-provider-azure   True        True      xpkg.upbound.io/upbound/provider-azure:v0.19.0   29m\n</code></pre> <p>Hint</p> <p>In case you need to recreate the ProviderConfig, here's how you do that.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: azure.upbound.io/v1beta1\nkind: ProviderConfig\nmetadata:\n  name: default\nspec:\n  credentials:\n    source: Secret\n    secretRef:\n      namespace: upbound-system\n      name: azure-secret\n      key: creds\nEOF\n</code></pre>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-package/#create-crossplane-claim","title":"Create Crossplane Claim","text":"<p>In contrast to creating a MongoDB instance through the managed resource definitions directly, installing a Crossplane package (or Configuration) requires an additional step.</p> <p>You now have a blueprint available to create MongoDB instances, through Crossplane's CompositeResourceDefinition(XRD) and Composition resources.</p> <p>The package adds the following Custom Resources (CR) to your cluster:</p> <ul> <li>MongoDBInstance: the Claim resource you use for requesting a new instance</li> <li>XMongoDBInstance: the Composition \"instance\", which is the bridge between your Claim and the Composition</li> </ul> <p>It also adds an XRD and the aforementioned Composition. The XRD defines the API for a type, including how to implement it (via one or more Compositions) and how to request an instance of that type via a Claim.</p> <p>In this package, the ClaimName is <code>MongoDBInstance</code>, which is the Kubernetes CR's kind.</p> <pre><code>LOCATION=\"West Europe\"\nINSTANCE_NAME=my-mongodb-instance\n</code></pre> <p>To create the claim, apply the following manifest:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: azure.ref.services.apps.tanzu.vmware.com/v1alpha1\nkind: MongoDBInstance\nmetadata:\n  namespace: default\n  name: ${INSTANCE_NAME}\nspec:\n  compositionSelector:\n    matchLabels:\n      database: mongodb\n  parameters:\n    location: ${LOCATION}\n    capabilities:\n      - name: \"EnableMongo\"\n      - name: \"mongoEnableDocLevelTTL\"\n  publishConnectionDetailsTo:\n    name: ${INSTANCE_NAME}-bindable\n    configRef:\n      name: default\n    metadata:\n      labels:\n        services.apps.tanzu.vmware.com/class: azure-mongodb\nEOF\n</code></pre> <p>Hint</p> <p>For the Services Toolkit to bind the connection secret,  we use the <code>publishConnectionDetailsTo</code> beta feature instead of the original <code>writeConnectionSecretToRef</code>.</p> <p>This way, we can configure additional properties, such as the <code>metadata.labels</code>.</p>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-package/#verify-managed-resource-creation","title":"Verify Managed Resource Creation","text":"<p>Verify the reconciliation status when you finish creating the Crossplane resources.</p> <pre><code>kubectl get resourcegroup,mongocollection,mongodatabase,account\n</code></pre> <p>Which should yield something like this:</p> <pre><code>NAME                                                            READY   SYNCED   EXTERNAL-NAME         AGE\nresourcegroup.azure.upbound.io/my-mongodb-instance              True    True     my-mongodb-instance   26h\n\nNAME                                                            READY   SYNCED   EXTERNAL-NAME         AGE\nmongocollection.cosmosdb.azure.upbound.io/my-mongodb-instance   True    True     my-mongodb-instance   26h\n\nNAME                                                            READY   SYNCED   EXTERNAL-NAME         AGE\nmongodatabase.cosmosdb.azure.upbound.io/my-mongodb-instance     True    True     my-mongodb-instance   26h\n\nNAME                                                            READY   SYNCED   EXTERNAL-NAME         AGE\naccount.cosmosdb.azure.upbound.io/my-mongodb-instance           True    True     my-mongodb-instance   26h\n</code></pre> <p>When all the resources are ready, the secret for your claim (the MongoDBInstance) is created, and the claim is set to ready.</p> <p>You can wait for this to happen with this <code>kubectl wait</code> command.</p> <pre><code>kubectl wait --for=condition=ready \\\nmongodbinstances.azure.ref.services.apps.tanzu.vmware.com ${INSTANCE_NAME} \\\n--timeout=400s\n</code></pre>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-package/#next-steps","title":"Next Steps","text":"<p>Success</p> <p>You can return to the main guide to continue with the Services Toolkit sections.</p> <p>You should come back for the cleanup commands; see below.</p>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-package/#cleanup-resources","title":"Cleanup Resources","text":"<p>Danger</p> <p>These commands delete the resources in Azure.</p> <p>Crossplane uses finalizers on its resources. This ensures that the <code>kubectl delete</code> blocks until it confirms the resources are removed at the provider level.</p> <pre><code>kubectl delete mongodbinstance ${INSTANCE_NAME} || true\nkubectl delete MongoDatabase -l crossplane.io/claim-name=${INSTANCE_NAME} || true\nkubectl delete MongoCollection -l crossplane.io/claim-name=${INSTANCE_NAME} || true\nkubectl delete Account -l crossplane.io/claim-name=${INSTANCE_NAME} ||  true\nkubectl delete ResourceGroup -l crossplane.io/claim-name=${INSTANCE_NAME} || true\n</code></pre> <pre><code>kubectl delete configuration ${CONFIG_NAME} || true\nkubectl delete providerconfig.azure.upbound.io default || true\n</code></pre> <p>And if the provider was installed via this package, you might want to clean that up as well.</p> <pre><code>kubectl delete provider upbound-provider-azure\n</code></pre>"},{"location":"usecases/azure/packages/mongodb/crossplane/consume-package/#explore-the-xrd-and-composition","title":"Explore the XRD and Composition","text":"<p>If you want to take a look at the XRD and Composition, use the following commands.</p> <p>To see them in the cluster:</p> <pre><code>kubectl get xrd,composition\n</code></pre> <p>This yields the following:</p> <pre><code>NAME                                                                                                                 ESTABLISHED   OFFERED   AGE\ncompositeresourcedefinition.apiextensions.crossplane.io/xmongodbinstances.azure.ref.services.apps.tanzu.vmware.com   True          True      112m\n\nNAME                                                      AGE\ncomposition.apiextensions.crossplane.io/mongodbinstance   112m\n</code></pre> <p>For more information, output them as <code>YAML</code> via the <code>-o yaml</code> flag in your <code>kubectl</code> command.</p>"},{"location":"usecases/azure/prerequisites/","title":"Azure Packages Prerequisites","text":""},{"location":"usecases/azure/prerequisites/#secretgen-controller","title":"SecretGen Controller","text":"<p>The SecretGen Controller is part of the Tanzu Cluster Essentials.</p> <p>If you install the Cluster Essentials, you can skip installing the SecretGen Controller yourself.</p> <p>If you have not installed the Cluster Essentials, either follow its installation docs or continue below.</p> <p>In most cases, you can safely install the latest version of the SecretGen Controller:</p> <pre><code>kubectl apply -f https://github.com/vmware-tanzu/carvel-secretgen-controller/releases/latest/download/release.yml\n</code></pre> <p>But, if that doesn't work, or you want to install a fixed version, take a look at the releases.</p>"},{"location":"usecases/multicloud/packages/psql/","title":"Consuming Postgresql Multicloud","text":""},{"location":"usecases/multicloud/packages/psql/#why-multicloud","title":"Why Multicloud","text":"<p>There are various reasons why companies are running applications in multiple clouds. We count data centers as local or private clouds.</p> <p>It is increasingly common for companies to run Kubernetes clusters in multiple locations - some clusters in the data center and others in AWS, for example.</p> <p>When (service) consumers do not care about the specific performance parameters of a database, you can simplify onboarding and relocation pain by providing a generic API.</p> <p>This package is an example of such a generic API. It holds as long as the consumers want a database and there is no need (yet) to fine-tune it.</p> <p>Good examples are PoCs, preview environments, or applications where the performance bottleneck lies elsewhere.</p>"},{"location":"usecases/multicloud/packages/psql/#pre-requisites","title":"Pre-requisites","text":"<p>This package supports three platforms; Kubernetes, Azure, and AWS. Therefore, we split the prerequisites into shared requirements and cloud-specific (Azure and AWS, respectively).</p>"},{"location":"usecases/multicloud/packages/psql/#shared","title":"Shared","text":"<p>Terraform provider configuration included</p> <p>The package creates a ProviderConfig for the Terraform provider for you. To ensure it is unique, it has the UID from Composite Resource (<code>XPostgreSQLInstance</code>) as the name.</p> <p>So when installing the Terraform Provider (to meet the prerequisites), there is no need to create a ProviderConfig.</p> <ul> <li>Kubernetes 1.23+</li> <li>Crossplane 1.10+</li> <li>Crossplane Kubernetes provider (Community)</li> <li>Crossplane Terraform provider (Upbound official)</li> </ul>"},{"location":"usecases/multicloud/packages/psql/#kubernetes","title":"Kubernetes","text":"<ul> <li>Crossplane Helm provider (Community)</li> </ul>"},{"location":"usecases/multicloud/packages/psql/#azure","title":"Azure","text":"<p>When wanting to consume the Azure FlexibleServer variant, you also need the following:</p> <ul> <li>Azure credentials</li> <li>Crossplane Azure provider (Upbound official)</li> </ul>"},{"location":"usecases/multicloud/packages/psql/#aws","title":"AWS","text":"<ul> <li>AWS credentials</li> <li>Crossplane AWS provider (Upbound official)</li> </ul>"},{"location":"usecases/multicloud/packages/psql/#install-crossplane-package","title":"Install Crossplane Package","text":"<p>We start by setting some environment variables and installing our Crossplane package (Configuration CR).</p> <pre><code>CONFIG_NAME=\"trp-multicloud-psql\"\nCONFIG_IMAGE=\"ghcr.io/vmware-tanzu-labs/trp-multicloud-psql\"\nCONFIG_VERSION=\"0.1.0-rc-4\"\nCROSSPLANE_NAMESPACE=\"upbound-system\"\n</code></pre> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: pkg.crossplane.io/v1\nkind: Configuration\nmetadata:\n  name: ${CONFIG_NAME}\nspec:\n  ignoreCrossplaneConstraints: true\n  package: ${CONFIG_IMAGE}:${CONFIG_VERSION}\n  packagePullPolicy: Allways\n  revisionActivationPolicy: Automatic\n  revisionHistoryLimit: 3\n  skipDependencyResolution: true\nEOF\n</code></pre> <p>We then wait until the Configuration object is healthy.</p> <pre><code>kubectl wait --for=condition=Healthy configuration ${CONFIG_NAME}\n</code></pre> <p>Verify the configuration exists and is valid.</p> <pre><code>kubectl get configuration\n</code></pre> <p>You can debug the installation if something is wrong via the ConfigurationRevision object(s).</p> <pre><code>kubectl describe configurationrevisions.pkg.crossplane.io\n</code></pre>"},{"location":"usecases/multicloud/packages/psql/#create-crossplane-claim","title":"Create Crossplane Claim","text":"<p>The package contains four Crossplane Compositions or four implementations.</p> <p>The supported implementations are:</p> <ul> <li>Helm: installs and configures a Bitnami PostgreSQL helm chart install</li> <li>FlexibleServer: creates and configures an Azure FlexibleServer with Postgresql, including a firewall rule, a database, and a ResourceGroup</li> <li>RDS Private: creates and configures an AWS RDS instance, which is only available within the configured VPC</li> <li>RDS Public:  creates and configures an AWS RDS instance, which is publicly available, and does this by creating more AWS network resources (not recommended)</li> </ul>"},{"location":"usecases/multicloud/packages/psql/#helm","title":"Helm","text":"<p>First, define the name of the claim, the crossplane namespace, and the storage class to use.</p> <pre><code>CLAIM_NAME=\"postgresql-0001\"\nSTORAGE_CLASS=\"default\"\n</code></pre> <p>We can then create the Crossplain CR to claim (or request) an instance:</p> <p>Notice the highlighted lines. That is how we select which one of the four implementations we request.</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: multi.ref.services.apps.tanzu.vmware.com/v1alpha1\nkind: PostgreSQLInstance\nmetadata:\n  namespace: default\n  name: ${CLAIM_NAME}\n  labels:\n    services.apps.tanzu.vmware.com/claimable: \"true\"\nspec:\n  compositionSelector:\n    matchLabels:\n      provider: helm\n  parameters:\n    location: local\n    version: \"12\"\n    database: demo\n    collation: en_GB.utf8\n    storageClass: ${STORAGE_CLASS}\nEOF\n</code></pre> <p>Tip</p> <p>Make sure you update the <code>STORAGE_CLASS</code> variable to a storage class available in your cluster.</p> <p>To get a list of available storage classes, run the following:</p> <pre><code>kubectl get storageclass\n</code></pre> <p>Success</p> <p>Verify the claim exists:</p> <pre><code>kubectl get postgresqlinstances.multi.ref.services.apps.tanzu.vmware.com $CLAIM_NAME\n</code></pre> <p>It should return something like this:</p> <pre><code>NAME              ADDRESS                                                        LOCATION         VERSION   CONNECTION-DETAILS   SYNCED   READY   CONNECTION-SECRET   AGE\npostgresql-0001   postgresql-0001-cmrpp-84lrw.upbound-system.svc.cluster.local   upbound-system   12.13.0                        True     False                       85s\n</code></pre> <p>You can continue with Verify Package Installation.</p>"},{"location":"usecases/multicloud/packages/psql/#flexibleserver-azure","title":"FlexibleServer (Azure)","text":"<p>First, define the name of the claim, the crossplane namespace, and the storage class to use.</p> <pre><code>CLAIM_NAME=\"postgresql-0001\"\nLOCATION=\"West Europe\"\n</code></pre> <p>And then create the Crossplane claim.</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: multi.ref.services.apps.tanzu.vmware.com/v1alpha1\nkind: PostgreSQLInstance\nmetadata:\n  name: ${CLAIM_NAME}\n  labels:\n    services.apps.tanzu.vmware.com/claimable: \"true\"\nspec:\n  compositionSelector:\n    matchLabels:\n      provider: azure\n  parameters:\n    location: ${LOCATION}\n    version: \"12\"\n    database: demo\n    collation: en_GB.utf8\n    storageClass: \"default\"\n    firewallRule:\n      startIpAddress: \"0.0.0.0\"\n      endIpAddress: \"255.255.255.255\"\nEOF\n</code></pre> <p>Public instance</p> <p>The firewall rule, in the claim, is a \"special\" rule. If defined like this, Azure will make the instance public for the whole internet.</p> <pre><code>firewallRule:\nstartIpAddress: \"0.0.0.0\"\nendIpAddress: \"255.255.255.255\"\n</code></pre> <p>There is another distinctive case. If you want the instance to be accessible only within Azure, use the following:</p> <pre><code>firewallRule:\nstartIpAddress: \"0.0.0.0\"\nendIpAddress: \"0.0.0.0\"\n</code></pre> <p>To take a look at the Crossplane managed resources for Azure, you can run the command below:</p> <pre><code>kubectl get resourcegroups.azure.upbound.io,flexibleserverdatabases.dbforpostgresql.azure.upbound.io,flexibleserverfirewallrules.dbforpostgresql.azure.upbound.io,flexibleservers.dbforpostgresql.azure.upbound.io\n</code></pre> <p>Success</p> <p>Verify the claim exists</p> <pre><code>kubectl get postgresqlinstances.multi.ref.services.apps.tanzu.vmware.com $CLAIM_NAME\n</code></pre> <p>It should return something like this:</p> <pre><code>NAME              ADDRESS                                                        LOCATION         VERSION   CONNECTION-DETAILS   SYNCED   READY   CONNECTION-SECRET   AGE\npostgresql-0001   postgresql-0001-cmrpp-84lrw.upbound-system.svc.cluster.local   upbound-system   12.13.0                        True     False                       85s\n</code></pre> <p>You can continue with Verify Package Installation.</p>"},{"location":"usecases/multicloud/packages/psql/#rds-private-aws","title":"RDS - Private  (AWS)","text":"<p>Info</p> <p>With the Azure FlexibleServer, we can get away with configuring the firewall rule.</p> <p>The AWS network configuration is more complex and involves more resources. So we decided to split them into two separate Compositions.</p> <p>The RDS Private implementation generates a minimal amount of AWS resources. This works in a Kubernetes cluster (such as EKS) within the same VPC and Subnet Group.</p> <p>Define the name of the claim, the crossplane namespace, and the AWS networking config (VPC and Subnet Group).</p> <pre><code>CLAIM_NAME=\"postgresql-0001\"\nLOCATION=\"eu-central-1\"\nVPC_ID=\"\"\nSUBNET_GROUP_NAME=\"\"\n</code></pre> <p>Because the AWS provider has two implementations (private and public), we use a second label, as you can see in the highlighted labels.</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: multi.ref.services.apps.tanzu.vmware.com/v1alpha1\nkind: PostgreSQLInstance\nmetadata:\n  name: ${CLAIM_NAME}\n  labels:\n    services.apps.tanzu.vmware.com/claimable: \"true\"\nspec:\n  compositionSelector:\n    matchLabels:\n      provider: aws\n      connectivity: \"private\"\n  parameters:\n    location: ${LOCATION}\n    version: \"12\"\n    database: demo\n    collation: en_GB.utf8\n    storageClass: gp2\n    aws:\n      vpcId: ${VPC_ID}\n      dbSubnetGroupName: ${SUBNET_GROUP_NAME}\n      cidrBlocks:\n        - 0.0.0.0/0\nEOF\n</code></pre> <p>In addition to using an existing VPC and SubnetGroup specified, the package also creates the following AWS resources:</p> <ul> <li>Security Group</li> <li>Security Group Rule</li> <li>RDS Instance</li> </ul> <p>Success</p> <p>Verify the claim exists:</p> <pre><code>kubectl get postgresqlinstances.multi.ref.services.apps.tanzu.vmware.com $CLAIM_NAME\n</code></pre> <p>It should return something like this:</p> <pre><code>NAME              ADDRESS                                                        LOCATION         VERSION   CONNECTION-DETAILS   SYNCED   READY   CONNECTION-SECRET   AGE\npostgresql-0001   postgresql-0001-cmrpp-84lrw.upbound-system.svc.cluster.local   upbound-system   12.13.0                        True     False                       85s\n</code></pre> <p>You can continue with Verify Package Installation.</p>"},{"location":"usecases/multicloud/packages/psql/#rds-public-aws","title":"RDS - Public (AWS)","text":"<p>The RDS Public implementation generates all the additional AWS networking resources to enable public (outside of AWS) access to the RDS instance.</p> <p>Define the name of the claim, the crossplane namespace, and the AWS networking config.</p> <pre><code>CLAIM_NAME=\"postgresql-0001\"\nLOCATION=\"eu-central-1\"\nVPC_ID=\"\"\nSUBNET_GROUP_NAME=\"\"\nINTERNET_GATEWAY_ID=\"\"\nSUBNET_A_CIDR=\"\"\nSUBNET_B_CIDR=\"\"\n</code></pre> <p>For public access, we need additional AWS resources. Due to the complexities and permissions involved, the package requires the (Internet) Gateway for the specified VPC to exist.</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: multi.ref.services.apps.tanzu.vmware.com/v1alpha1\nkind: PostgreSQLInstance\nmetadata:\n  name: ${CLAIM_NAME}\n  labels:\n    services.apps.tanzu.vmware.com/claimable: \"true\"\nspec:\n  compositionSelector:\n    matchLabels:\n      provider: aws\n      connectivity: \"public\"\n  parameters:\n    location: ${LOCATION}\n    version: \"12\"\n    database: demo\n    collation: en_GB.utf8\n    storageClass: gp2\n    aws:\n      vpcId: ${VPC_ID}\n      dbSubnetGroupName: ${SUBNET_GROUP_NAME}\n      cidrBlocks:\n        - 0.0.0.0/0\n      public:\n        gatewayId: ${INTERNET_GATEWAY_ID}\n        subnetACidrBlock: ${SUBNET_A_CIDR}\n        subnetBCidrBlock: ${SUBNET_B_CIDR}\nEOF\n</code></pre> <p>In addition to using an existing VPC and SubnetGroup specified, the package also creates the following AWS resources:</p> <ul> <li>Security Group</li> <li>Security Group Rule</li> <li>RDS Instance</li> <li>Route Table</li> <li>2x Subnet (Subnet A and Subnet B)</li> <li>Route Table Association (one for each Subnet)</li> <li>Route</li> </ul> <p>Important</p> <p>As we cannot know what CIDR blocks your Gateway supports, we ask you to specify them.</p> <p>For example, we have a Gateway with <code>10.100.0.0/16</code>. To limit the range we give to the Subnets, we break the <code>10.100.255.0</code> in half (via <code>/25</code>).</p> <pre><code>public:\ngatewayId: ${INTERNET_GATEWAY_ID}\nsubnetACidrBlock: \"10.100.255.0/25\"\nsubnetBCidrBlock: \"10.100.255.128/25\"\n</code></pre> <p>Success</p> <p>Verify the claim exists:</p> <pre><code>kubectl get postgresqlinstances.multi.ref.services.apps.tanzu.vmware.com $CLAIM_NAME\n</code></pre> <p>It should return something like this:</p> <pre><code>NAME              ADDRESS                                                        LOCATION         VERSION   CONNECTION-DETAILS   SYNCED   READY   CONNECTION-SECRET   AGE\npostgresql-0001   postgresql-0001-cmrpp-84lrw.upbound-system.svc.cluster.local   upbound-system   12.13.0                        True     False                       85s\n</code></pre> <p>You can continue with Verify Package Installation.</p>"},{"location":"usecases/multicloud/packages/psql/#verify-package-installation","title":"Verify Package Installation","text":"<p>Among other resources, this package creates the following:</p> <ul> <li>ProviderConfig: it uses the Terraform provider to generate a unique password. To save you the trouble, it configures the provider for you</li> <li>PostgresqlInstances: the claim type. You can track the end state of your request via this resource</li> <li>XPostgresqlInstances: the workhorse behind the scenes for the claim type. It tracks all the Kubernetes CRs related to the Claim request and is your best resource for debugging</li> </ul> <pre><code>kubectl get providerconfig,xpostgresqlinstances,postgresqlinstances\n</code></pre> <p>We assume our package always works, so run the following command to wait for it to be ready:</p> <pre><code>kubectl wait --for=condition=ready \\\npostgresqlinstances.multi.ref.services.apps.tanzu.vmware.com/${CLAIM_NAME} \\\n--timeout=400s\n</code></pre> <p>Wait times out</p> <p>If the wait command times out, you can debug the cause via the Composite Resource (the X resource from Crossplane). <pre><code>kubectl describe xpostgresqlinstances.multi.ref.services.apps.tanzu.vmware.com/${CLAIM_NAME}\n</code></pre> <p>Cannot render composed resource</p> <p>If you get a warning like this in the Composite Resource:</p> <pre><code>cannot render composed resource from resource template at index 1: cannot use dry-run create to name composed resource: workspaces.tf.crossplane.io is forbidden: User \"system:serviceaccount:upbound-system:crossplane\" cannot create resource \"workspaces\" in API group \"tf.crossplane.io\" at the cluster scope\n</code></pre> <p>Sometimes the service account of Crossplane or a Crossplane Provider does not have enough permissions to create the resources. You can either fix this quickly and dirty (for PoCs) like below:</p> <pre><code>kubectl create clusterrolebinding crossplane-admin-binding --clusterrole cluster-admin --serviceaccount=\"upbound-system:crossplane\" || true   </code></pre> <p>Or find the Service Account for the specific provider (they have generated names) and update its RBAC configuration concisely.</p> <pre><code>SA=$(kubectl -n upbound-system get sa -o name | grep provider-terraform | sed -e 's|serviceaccount\\/|upbound-system:|g')   </code></pre> <p>Success</p> <p>If the wait commands returns with <code>Conditions met</code>, the package is ready.</p> <p>You can now continue with test without TAP or test with TAP and STK.</p>"},{"location":"usecases/multicloud/packages/psql/#test-without-tap","title":"Test Without TAP","text":"<p>Tanzu Application Platform (TAP) and the Services Toolkit (STK) are great tools for usage at scale. It can be rather cumbersome to test a single service package.</p> <p>So below is an example of a Kubernetes deployment that directly consumes the secret created by Crossplane. It uses the Service Bindings specification (Spring library) to translate the secret to the parameters needed by the database API.</p>"},{"location":"usecases/multicloud/packages/psql/#install-the-test-application","title":"Install the Test application","text":"<pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: spring-boot-postgres\n  labels:\n    app.kubernetes.io/name: spring-boot-postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: spring-boot-postgres\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: spring-boot-postgres\n    spec:\n      containers:\n        - name: spring-boot-postgres\n          image: joostvdgtanzu/spring-boot-postgres-01:0.1.0\n          env:\n            - name: SERVICE_BINDING_ROOT\n              value: \"/bindings\"\n          ports:\n            - containerPort: 8080\n              name: http-web-svc\n          livenessProbe:\n            httpGet:\n              port: 8080\n              path: /actuator/health/liveness\n            initialDelaySeconds: 30\n            periodSeconds: 10\n            successThreshold: 1\n          readinessProbe:\n            httpGet:\n              port: 8080\n              path: /actuator/health/readiness\n            initialDelaySeconds: 10\n          volumeMounts:\n            - mountPath: /bindings/db\n              name: secret-volume\n              readOnly: true\n      volumes:\n        - name: secret-volume\n          projected:\n            defaultMode: 420\n            sources:\n              - secret:\n                  name: postgresql-0001\nEOF\n</code></pre>"},{"location":"usecases/multicloud/packages/psql/#verify-test-application-installation","title":"Verify test application installation","text":"<p>Verify the deployment is applied correctly:</p> <pre><code>kubectl get deployment\n</code></pre> <p>If so, then you can wait on the application to be ready:</p> <pre><code>kubectl wait --for=condition=ready pod \\\n-l app.kubernetes.io/name=spring-boot-postgres \\\n--timeout=300s\n</code></pre>"},{"location":"usecases/multicloud/packages/psql/#test-database-connection-via-application","title":"Test database connection via application","text":"<p>As it can be complicated to deal with all the ways a service in Kubernetes can be exposed, we resort to <code>kubectl port-forward</code>.</p> <p>Make sure you have two terminals (or tabs) open. Run this in terminal one:</p> <pre><code>kubectl port-forward deployment/spring-boot-postgres 8080\n</code></pre> <p>And now, run this in terminal two:</p> <pre><code>curl -s \"http://localhost:8080\"\n</code></pre> <pre><code>curl --header \"Content-Type: application/json\" \\\n--request POST --data '{\"name\":\"Piet\"}'\\\nhttp://localhost:8080/create\n</code></pre> <pre><code>curl -s \"http://localhost:8080\"\n</code></pre> <p>You should see an empty result in the first call and a response with the name you specified in the second call in the final curl request.</p>"},{"location":"usecases/multicloud/packages/psql/#delete-deployment","title":"Delete deployment","text":"<pre><code>kubectl delete deployment spring-boot-postgres\n</code></pre>"},{"location":"usecases/multicloud/packages/psql/#test-with-tap-stk","title":"Test with TAP &amp; STK","text":"<p>Once we have the Postgresql instance and the appropriate secret (adhering to Service Bindings), we can create an (STK) Claim.</p> <p>We create the following:</p> <ul> <li>ClusterInstanceClass</li> <li>Reader ClusterRole for Services Toolkit</li> <li>ResourceClaim</li> <li>TAP workload to test the database instance</li> </ul>"},{"location":"usecases/multicloud/packages/psql/#create-a-cluster-instance-class","title":"Create a Cluster Instance Class","text":"<p>We will create a generic class of bindable resources. So applications can be made aware of the specific name of the resources, or their bindable secret, to claim them.</p> <p>The multicloud package gives each composition several labels you can filter on. You can create instance classes o</p> <p>Info</p> <p>The included (at this time of writing) labels are:</p> <ul> <li><code>services.apps.tanzu.vmware.com/class</code>: this is a static label and always says <code>multicloud-psql</code></li> <li><code>services.apps.tanzu.vmware.com/infra</code>: this is a dynamic label [<code>Kubernetes</code>, <code>Azure</code>, <code>AWS</code>]</li> <li><code>services.apps.tanzu.vmware.com/location</code>: this is a dynamic label and depends on the <code>location</code> parameter when creating the Crossplane claim</li> <li><code>services.apps.tanzu.vmware.com/version</code>: this is a dynamic label and depends on the <code>version</code> parameter when creating the Crossplane claim</li> </ul> For any multicloud-psqlLimit to Kubernetes basedLimit to Version 12 <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: services.apps.tanzu.vmware.com/v1alpha1\nkind: ClusterInstanceClass\nmetadata:\n  name: multi-psql\nspec:\n  description:\n    short: multi psql\n  pool:\n    kind: Secret\n    group: \"\"\n    labelSelector:\n      matchLabels:\n        services.apps.tanzu.vmware.com/class: multicloud-psql\nEOF\n</code></pre> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: services.apps.tanzu.vmware.com/v1alpha1\nkind: ClusterInstanceClass\nmetadata:\n  name: multi-psql-kubernetes\nspec:\n  description:\n    short: multicloud psql\n  pool:\n    kind: Secret\n    group: \"\"\n    labelSelector:\n      matchLabels:\n        services.apps.tanzu.vmware.com/class: multicloud-psql\n        services.apps.tanzu.vmware.com/infra: kubernetes\nEOF\n</code></pre> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: services.apps.tanzu.vmware.com/v1alpha1\nkind: ClusterInstanceClass\nmetadata:\n  name: multi-psql-kubernetes-v12\nspec:\n  description:\n    short: multicloud psql\n  pool:\n    kind: Secret\n    group: \"\"\n    labelSelector:\n      matchLabels:\n        services.apps.tanzu.vmware.com/class: multicloud-psql\n        services.apps.tanzu.vmware.com/infra: kubernetes\n        services.apps.tanzu.vmware.com/version: \"12\"\nEOF\n</code></pre>"},{"location":"usecases/multicloud/packages/psql/#services-toolkit-reader-cluster-role","title":"Services Toolkit Reader Cluster Role","text":"<p>We make sure the STK controller can read the secrets anywhere. Feel free to limit this role to specific named resources via the <code>resourceNames</code> property.</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: stk-secret-reader\n  labels:\n    servicebinding.io/controller: \\\"true\\\"\nrules:\n- apiGroups:\n  - \\\"\\\"\n  resources:\n  - secrets\n  verbs:\n  - get\n  - list\n  - watch\nEOF\n</code></pre>"},{"location":"usecases/multicloud/packages/psql/#discover-claim-and-bind-to-an-instance","title":"Discover, Claim, and Bind to an instance","text":"<p>First, confirm we have claimable class instances.</p> <pre><code>export RESOURCE_NAMESPACE=default\nexport RESOURCE_NAME=${CLAIM_NAME}\nexport CLASS_NAME=\"multi-psql\"\n</code></pre> For any multicloud-psqlLimit to Kubernetes basedLimit to Version 12 <pre><code>export CLASS_NAME=\"multi-psql\"\n</code></pre> <pre><code>export CLASS_NAME=\"multi-psql-kubernetes\"\n</code></pre> <pre><code>export CLASS_NAME=\"multi-psql-kubernetes-12\"\n</code></pre> <pre><code>tanzu services claimable list --class ${CLASS_NAME} \\\n-n ${RESOURCE_NAMESPACE}\n</code></pre> <p>Which should yield:</p> <pre><code>NAME             NAMESPACE  KIND    APIVERSION  \npostgresql-0001  default    Secret  v1 </code></pre> <p>We have to create the claim in the namespace of the consuming application. So set this variable to the namespace where you create TAP workloads.</p> <pre><code>export TAP_DEV_NAMESPACE=default\n</code></pre> <p>We then create the associated resource claim:</p> Using Tanzu CLIUsing Kubernetes Manifest <pre><code>tanzu service claim create multi-psql-claim-01 \\\n--namespace ${TAP_DEV_NAMESPACE} \\\n--resource-name ${RESOURCE_NAME} \\\n--resource-namespace ${RESOURCE_NAMESPACE} \\\n--resource-kind Secret \\\n--resource-api-version v1\n</code></pre> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: services.apps.tanzu.vmware.com/v1alpha1\nkind: ResourceClaim\nmetadata:\n  name: multi-psql-claim-01\n  namespace: ${TAP_DEV_NAMESPACE}\nspec:\n  ref:\n    apiVersion: v1\n    kind: Secret\n    name: ${RESOURCE_NAME}\n    namespace: ${RESOURCE_NAMESPACE}\nEOF\n</code></pre> <p>To verify your claim is ready, you run this command:</p> <pre><code>tanzu service claim list -o wide </code></pre> <p>Which should yield the following:</p> <pre><code>NAME                 READY  REASON            CLAIM REF                                                                  \nmulti-psql-claim-01  True   Ready             services.apps.tanzu.vmware.com/v1alpha1:ResourceClaim:multi-psql-claim-01 </code></pre> <p>You can now claim it with a TAP workload.</p>"},{"location":"usecases/multicloud/packages/psql/#test-claim-with-tap-workload","title":"Test Claim With TAP Workload","text":"<p>We can now create a TAP workload that uses our resource claim.</p> <p>The runtime will fail once or twice as it takes time before the service update with the secret mount lands. So wait for  <code>deployment-0002</code> or <code>0003</code>. </p> Tanzu CLIKubernetes Manifest <pre><code>tanzu apps workload create spring-boot-postgres-01 \\\n--namespace ${TAP_DEV_NAMESPACE} \\\n--git-repo https://github.com/joostvdg/spring-boot-postgres.git \\\n--git-branch main \\\n--type web \\\n--label app.kubernetes.io/part-of=spring-boot-postgres-01 \\\n--annotation autoscaling.knative.dev/minScale=1 \\\n--build-env BP_JVM_VERSION=17 \\\n--service-ref db=services.apps.tanzu.vmware.com/v1alpha1:ResourceClaim:multi-psql-claim-01 \\\n--yes\n</code></pre> <pre><code>echo \"apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\n  labels:\n    app.kubernetes.io/part-of: spring-boot-postgres-01\n    apps.tanzu.vmware.com/workload-type: web\n  name: spring-boot-postgres-01\n  namespace: ${TAP_DEV_NAMESPACE}\nspec:\n  build:\n    env:\n    - name: BP_JVM_VERSION\n      value: \\\"17\\\"\n  params:\n  - name: annotations\n    value:\n      autoscaling.knative.dev/minScale: \\\"1\\\"\n  serviceClaims:\n  - name: db\n    ref:\n      apiVersion: services.apps.tanzu.vmware.com/v1alpha1\n      kind: ResourceClaim\n      name: azure-mongodb-claim-01\n  source:\n    git:\n      ref:\n        branch: main\n      url: https://github.com/joostvdg/spring-boot-postgres.git\n\" &gt; workload.yml\n</code></pre> <pre><code>kubectl apply -f workload.yml\n</code></pre> <p>To see the logs: <pre><code>tanzu apps workload tail spring-boot-postgres-01\n</code></pre></p> <p>To get the status:</p> <pre><code>tanzu apps workload get spring-boot-postgres-01\n</code></pre> <p>Tap creates the deployment when the build and config writer workflows are complete.</p> <p>To see their pods:</p> <pre><code>kubectl get pod -l app.kubernetes.io/part-of=spring-boot-postgres-01\n</code></pre> <p>Then you can wait for the application to be ready via the <code>kubectl</code> CLI.</p> <pre><code>kubectl wait --for=condition=ready \\\npod -l app.kubernetes.io/component=run,app.kubernetes.io/part-of=spring-boot-postgres-01 \\\n--timeout=180s \\\n--namespace ${TAP_DEV_NAMESPACE}\n</code></pre> <p>Ensure there is only one deployment active:</p> <pre><code>kubectl get pod --namespace ${TAP_DEV_NAMESPACE} \\\n-l app.kubernetes.io/component=run,app.kubernetes.io/part-of=spring-boot-postgres-01\n</code></pre> <p>Which should list a single deployment with Ready 2/2:</p> <pre><code>NAME                                                       READY   STATUS    RESTARTS   AGE\nspring-boot-postgres-01-00002-deployment-8cd56bdc8-gb44n   2/2     Running   0          6m11s\n</code></pre> <p>We then collect the name of the Pod or copy it from the command-line output.</p> <pre><code>POD_NAME=$(kubectl get pod \\\n--namespace ${TAP_DEV_NAMESPACE} \\\n-l app.kubernetes.io/component=run,app.kubernetes.io/part-of=spring-boot-postgres-01 \\\n-o jsonpath=\"{.items[0].metadata.name}\")\n</code></pre>"},{"location":"usecases/multicloud/packages/psql/#test-application","title":"Test Application","text":"<p>Use the <code>kubectl</code> CLI to create a port forward.</p> <pre><code>kubectl port-forward pod/${POD_NAME} --namespace ${TAP_DEV_NAMESPACE} 8080\n</code></pre> <p>And then open another terminal to test the application:</p> <pre><code>curl -s \"http://localhost:8080\"\n</code></pre> <p>Which should return an empty list <code>[]</code>. Add a value that gets stored in the MongoDB instance.</p> <pre><code>curl --header \"Content-Type: application/json\" \\\n--request POST --data '{\"name\":\"Alice\"}' \\\nhttp://localhost:8080/create\n</code></pre> <p>Making another GET request should return the stored entry:</p> <pre><code>curl -s \"http://localhost:8080\"\n</code></pre> <p>Success</p> <p>We have gone through all the steps. You can now clean up all the resources.</p>"},{"location":"usecases/multicloud/packages/psql/#cleanup","title":"Cleanup","text":"<pre><code>CLAIM_NAME=\"postgresql-0001\"\nCONFIG_NAME=\"trp-multicloud-psql\"\n</code></pre> <p>Delete TAP Workload</p> <pre><code>tanzu apps workload delete spring-boot-postgres-01 || true\n</code></pre> <p>Delete STK Claim</p> <pre><code>tanzu service claim delete multi-psql-claim-01 || true\nkubectl delete ClusterInstanceClass multi-psql multi-psql-kubernetes multi-psql-kubernetes-12 || true\n</code></pre> <p>Delete Crossplane Claim &amp; Package</p> <pre><code>kubectl delete postgresqlinstances ${CLAIM_NAME} || true\n</code></pre> <pre><code>kubectl delete configuration ${CONFIG_NAME} || true\n</code></pre> <p>Delete Crossplane resources</p> <pre><code>kubectl delete providerconfigs.helm.crossplane.io default || true\nkubectl delete providerconfigs.kubernetes.crossplane.io default || true\n\nkubectl delete providers.pkg.crossplane.io crossplane-contrib-provider-helm || true\nkubectl delete providers.pkg.crossplane.io crossplane-contrib-provider-kubernetes || true\nkubectl delete providers.pkg.crossplane.io crossplane-contrib-provider-terraform || true\n</code></pre> <p>This should not be required, but in case the resources for Azure are not cleaned up when deleting the package:</p> <p>Delete Azure Resources</p> <pre><code>kubectl delete flexibleserverconfigurations.dbforpostgresql.azure.upbound.io -l crossplane.io/claim-name=${CLAIM_NAME} --force --grace-period=0 || true\nkubectl delete flexibleserverdatabases.dbforpostgresql.azure.upbound.io -l crossplane.io/claim-name=${CLAIM_NAME} --force --grace-period=0 || true\nkubectl delete flexibleserverfirewallrules.dbforpostgresql.azure.upbound.io -l crossplane.io/claim-name=${CLAIM_NAME} --force --grace-period=0 || true\n\nFLEXIBLE_SERVER_NAME=$(kubectl get flexibleserver.dbforpostgresql.azure.upbound.io -l crossplane.io/claim-name=${CLAIM_NAME} -o name)\nkubectl patch ${FLEXIBLE_SERVER_NAME} -p '{\"metadata\":{\"finalizers\":null}}' --type=merge || true\nkubectl delete flexibleserver.dbforpostgresql.azure.upbound.io -l crossplane.io/claim-name=${CLAIM_NAME} --force --grace-period=0 || true\n</code></pre>"},{"location":"usecases/multicloud/packages/psql/create/","title":"Creating the Postgresql Multicloud Crossplane package","text":""},{"location":"usecases/multicloud/packages/psql/create/#why-multicloud","title":"Why Multicloud","text":"<p>There are various reasons why companies are running applications in multiple clouds. We count data centers as local or private clouds.</p> <p>It is increasingly common for companies to run Kubernetes clusters in multiple locations - some clusters in the data center and others in AWS, for example.</p> <p>When (service) consumers do not care about the specific performance parameters of a database, you can simplify onboarding and relocation pain by providing a generic API.</p> <p>This package is an example of such a generic API. It holds as long as the consumers want a database and there is no need (yet) to fine-tune it.</p> <p>Good examples are PoCs, preview environments, or applications where the performance bottleneck lies elsewhere.</p>"},{"location":"usecases/multicloud/packages/psql/create/#why-this-package","title":"Why This Package","text":"<p>There are several contributing factors as to why this package exists.</p> <ul> <li>Multicloud Messaging: VMware is going big on multicloud. We wanted a package that shows how to leverage the Tanzu Application Platform (TAP) and other technologies (such as Crossplane) across multiple cloud environments.</li> <li>Power of YTT: We are proud of our Carvel tool suite. We want to show how maintaining complex Crossplane Compositions is more manageable using YTT.</li> <li>Compare Crossplane to Cloud Controllers: Each major public cloud provider has their Kubernetes Controller for managing resources in their respective cloud. We wanted to compare the producer side of things between those packages we did before.</li> </ul> <p>We stuck to the classic example of a PostgreSQL database. Something you can run anywhere in a myriad of ways. So the attention can be on using Crossplane and managing it with YTT.</p>"},{"location":"usecases/multicloud/packages/psql/create/#crossplane-xrd-compositions","title":"Crossplane XRD &amp; Compositions","text":"<p>At the time of writing, the (Crossplane) package has four implementations, or Compositions.</p> <ul> <li>Helm</li> <li>FlexibleServer for Azure</li> <li>RDS Private for AWS, only privately (within VPC) available</li> <li>RDS Public for AWS, public available</li> </ul> <p>While there is significant overlap between these packages, there are also a lot of differences.</p> <p>This means the case of \"I want to run a PostgreSQL database; I don't care where or how\" is easily supported. The package ensures sane defaults for every runtime environment for trivial (or hello world) installations.</p> <p>Unfortunately, for anything more complex, the configuration of the public clouds shatters the illusion of a single API (the Crossplane XRD). You cannot use this package for managing databases for non-trivial applications running in production.</p> <p>Still, it serves its purpose of showing the Multicloud potential, the power of YTT, and how Crossplane compares to the cloud-specific controllers.</p>"},{"location":"usecases/multicloud/packages/psql/create/#solving-for-random-password","title":"Solving For Random Password","text":"<p>An interesting problem encountered early was the need for a secure admin password.</p> <p>There are various ways of generating a password in Kubernetes, for instance, our own SecretGen Controller.</p> <p>The challenge is that we need the following:</p> <ul> <li>deterministic name of the secret the password is stored in</li> <li>password is stored in the ConnectionDetails secret</li> </ul> <p>Most alternatives, such as the SecretGen Controller, do not expose the password so we can get it into the ConnectionDetails.</p> <p>Our current solution is the use of Terraform. There is an official Terraform Provider, and Terraform can generate a random password.</p> <p>The Terraform Provider also supports outputs, which the Crossplane uses as values for the ConnectionSecret. We can then promote this value to the ConnectionDetails.</p> <p>Below is the snippet showing the example. The Patches part is cut; we'll dive into that next.</p> Terraform Password snippet <pre><code>base:\napiVersion: tf.upbound.io/v1beta1\nkind: Workspace\nspec:\nforProvider:\nmodule: |\nresource \"random_password\" \"password\" {\nlength  = 64\nspecial = false\n}\n\noutput \"password\" {\nvalue     = random_password.password.result\nsensitive = true\n}\nsource: Inline\nwriteConnectionSecretToRef:\nnamespace: #@ crossplaneNamespace\nconnectionDetails:\n- fromConnectionSecretKey: password\n</code></pre>"},{"location":"usecases/multicloud/packages/psql/create/#creating-a-complete-connectiondetails-secret","title":"Creating a complete ConnectionDetails secret","text":"<p>We use Crossplane to create and manage data stores in public cloud infrastructure. To consume these data stores with Tanzu Application Platform (TAP), we use the ServiceBinding and a mapping solution with the ServicesToolkit (STK).</p> <p>The ServiceBinding spec gives us a clear goal of what we need to generate. We need a secret with specific keys depending on the data store type, e.g., Postgres.</p> <p>Postgres required keys</p> <p>The Service Binding specification defines how a service such as PostgreSQL can be bound to an application.</p> <p>For a list of keys that is required, one can also look at the Spring Cloud Bindings (Java) library.</p> <p>You can view the list for PostgreSQL here.</p> <p>For STK, we also need some labels on this (Kubernetes) secret so it can create a ClusterInstanceClass.</p> <p>When using Compositions, Crossplane generates at least two secrets. The data for those secrets depends on the <code>spec.connectionSecretKeys</code> entries in the CompositeResourceDefinition (XRD). I will name the secrets to explain this. Both of these secrets represent the ConnectionDetails conceptual secret.</p> <ol> <li>Composition Secret: It creates a \"placeholder\" secret for collecting the expected entries from the various Composition Resources</li> <li>Claim Secret: When the Composition has collected all entries, it creates a secret in the namespace of the (Crossplane) Claim (e.g., <code>PostgreSQLInstance</code> CR)</li> </ol> <p>Instruct Crossplane to create Composition Secret</p> <p>We specify the <code>spec.writeConnectionSecretsToNamespace</code> to instruct Crossplane to create the CompositionSecret and in which namespace.</p> <pre><code>apiVersion: apiextensions.crossplane.io/v1\nkind: Composition\nmetadata:\n...\nspec:\nwriteConnectionSecretsToNamespace: #@ data.values.crossplane.namespace\n...\nresources:\n</code></pre> <p>Some resources defined in the Composition have their own ConnectionSecret, from where it copies the values to the ConnectionDetails secret. An example to illustrate this:</p> <p>Terraform Password - Connection Secret</p> <p>The Terraform provider exposes the <code>output</code> in the Module as something Crossplane can write to a ConnectionSecret. We tell Crossplane to create this secret by setting <code>writeConnectionSecretToRef.namespace</code> and <code>writeConnectionSecretToRef.name</code>.</p> <p>We then instruct Crossplane to copy the key <code>password</code> from the ConnectionSecret to the ConnectionDetails. Again, the ConnectionDetails repre</p> <pre><code>name: password\nbase:\napiVersion: tf.upbound.io/v1beta1\nkind: Workspace\nspec:\nforProvider:\nmodule: |\nresource \"random_password\" \"password\" {\nlength  = 64\nspecial = false\n}\n\noutput \"password\" {\nvalue     = random_password.password.result\nsensitive = true\n}\nsource: Inline\nwriteConnectionSecretToRef:\nnamespace: #@ crossplaneNamespace \n# we set the name in a patch, excluded from the example for brevity\npatches:\n...\nconnectionDetails:\n- fromConnectionSecretKey: password\n</code></pre> <p>Assuming that this Composition succeeds, we end up with three secrets:</p> <ol> <li>Composition Secret</li> <li>Composition Resource Secret (Terraform Password)</li> <li>Claim Secret</li> </ol> <p>Depending on the number of Composition Resources that have secrets, we can end up with a whole bunch of them.</p> <p>We decided to use Crossplane's Patch system to reduce the number of secrets. We are reducing the proliferation of possibly sensitive data.</p> <p>To re-iterate our main goal: we need a (Kubernetes) Secret with all the data keys for the ServiceBinding specification and labels for the STK mapping. Any other secret is a potential problem, so the fewer, the better.</p> <p>The most straightforward secret to manipulate is the Composition Secret. This secret is a placeholder, so it doesn't impact anything.</p> <p>Terraform Password - Modifying Composition Secret</p> <p>We use Crossplane's patches mechanism to \"merge\" the Composition Secret into the Claim Secret.</p> <p>This is done by taking the Claim's name and namespace and overriding the <code>spec.writeConnectionSecretToRef</code> fields of the Composition. You access these fields via <code>- type: ToCompositeFieldPath</code>.</p> <pre><code>patches:\n- type: ToCompositeFieldPath\nfromFieldPath: metadata.labels[crossplane.io/claim-name]\ntoFieldPath: spec.writeConnectionSecretToRef.name\n- type: ToCompositeFieldPath\nfromFieldPath: metadata.labels[crossplane.io/claim-namespace]\ntoFieldPath: spec.writeConnectionSecretToRef.namespace\n</code></pre> <p>The Composition does not own all Secrets. It can only merge secrets it owns. If you try to merge a secret it doesn't own; you will get an error (in the resource's events) saying it cannot update the secret because it doesn't own it.</p> <p>An example of this is the Connection Secret of the Terraform Password resource. To help understand what this secret is, we patch it.</p> <p>Terraform Password - Rename Connection Secret</p> <p>Here we patch the name of the Connection Secret to use the UID of the Composition.</p> <p>In addition, we use the <code>transforms</code> option to add further information to the name. This helps us understand what this Secret is and to what resource it belongs.</p> <pre><code>patches:\n- type: FromCompositeFieldPath\nfromFieldPath: metadata.uid\ntoFieldPath: spec.writeConnectionSecretToRef.name\ntransforms:\n- type: string\nstring:\ntype: Format\nfmt: '%s-postgresql-admin'\n</code></pre>"},{"location":"usecases/multicloud/packages/psql/create/#solve-for-stk-metadata","title":"Solve For STK Metadata","text":"<p>The Claim Secret we end up with has all the data we write to the Connection Details. It has the name of the Claim and exists in the Claim namespace.</p> <p>This is enough for any application to use and leverage with a service binding library (e.g., Spring applications). Unfortunately, this (Kubernetes) secret is relatively sterile, containing no Annotations or Labels.</p> <p>The Services Toolkit (STK) needs specific fields or labels on the object to map it to a ClusterInstanceClass. We need ClusterInstanceClasses, to create STK Claims which TAP can consume.</p> <p>There are several ways we can tackle this.</p> <ul> <li>Use a SecretGen Controller's SecretTemplate to create a copy with the desired labels</li> <li>Specify the labels in the Crossplane Claim (see below)</li> <li>Create a Crossplane Managed Resource that merges with the Claim Secret, adding the desired labels</li> </ul> <p>The first solution is out, as we want fewer secrets, not more.</p> <p>The second solution is not nice for our users, as it requires additional work and understanding. We prefer to use our packages to reduce our users' burden (and cognitive load), not increase it.</p> <p>Add labels via Crossplane Claim</p> <p>Here's an example of adding additional metadata to the Claim Secret.</p> <pre><code>publishConnectionDetailsTo:\nname: trp-cosmosdb-mongo-bindable-08\nconfigRef:\nname: default\nmetadata:\nlabels:\nservices.apps.tanzu.vmware.com/class: azure-mongodb\n</code></pre> <p>Consequently, there is only one option we like. We create another Crossplane Composition Resource.</p> <p>Secret with labels resource</p> <p>As you can see, we use the Crossplane Kubernetes Provider to create a Kubernetes Secret resource.</p> <p>This secret is an empty shell, as we set <code>spec: {}</code>. We do add our labels and patch the name and namespace.</p> <p>We set the name and namespace to the values of the Claim. Which guarantees it has the same values as the Claim Secret.</p> <p>Crossplane merges this empty secret with the Claim Secret. And thus, our Claim Secret ends up having our desired labels.</p> <pre><code>name: connectionSecret\nbase:\napiVersion: kubernetes.crossplane.io/v1alpha1\nkind: Object\nspec:\nforProvider:\nmanifest:\napiVersion: v1\nkind: Secret\nspec: {}\nmetadata:\nlabels:\nservices.apps.tanzu.vmware.com/class: multicloud-psql\npatches:\n- type: FromCompositeFieldPath\nfromFieldPath: metadata.labels[crossplane.io/claim-name]\ntoFieldPath: spec.forProvider.manifest.metadata.name\n- type: FromCompositeFieldPath\nfromFieldPath: metadata.labels[crossplane.io/claim-namespace]\ntoFieldPath: spec.forProvider.manifest.metadata.namespace\n</code></pre> <p>More than a single label is required to achieve what we want with the STK. Ideally, we have several dynamic labels to differentiate between different implementations.</p> <p>For example, we would like an infrastructure label to create ClusterInstanceClasses for Azure and AWS. This leads us to the next topic we want to explain: how do we manage multiple Compositions with significant overlap?</p>"},{"location":"usecases/multicloud/packages/psql/create/#manage-compositions-with-ytt","title":"Manage Compositions with YTT","text":"<p>The four Compositions share several resources. Some are the same (e.g., the Terraform Password), while others have minor variations.</p> <p>Let's explain how we tackled the shared values with YTT templating.</p>"},{"location":"usecases/multicloud/packages/psql/create/#templating","title":"Templating","text":"<p>For more information on YTT, read the docs or look at the Playground.</p> <p>In contrast to tools such as Helm and Kustomize, YTT works with the YAML structure. This means we can create a schema and validate the data we are working with (more on that in the next paragraph).</p> <p>We'll stick to the base templating in this package (and later the Library feature).</p> <p>For the most part, we stick to templating from direct data values. These data values come from input into the templating process.</p> <p>You can supply these in several ways:</p> <ul> <li>data values file</li> <li>raw YAML file</li> <li>a schema file's default values (see next paragraph)</li> <li>input flags when executing the YTT CLI's template command</li> </ul> <p>We supply all our values with a schema file. When we run the YTT templating, we use the values from the Schema file and apply the same value to all the templates.</p> <p>The templates we have are the XRD and the Compositions, each in their respective file.</p> <p>Below are two snippets as examples.</p> <p>XRD template snippet</p> <p>The CompositeResourceDefinition or XRD. It defines our custom resources and the API for all our compositions.</p> <pre><code>#@ load(\"@ytt:data\", \"data\")\n---\napiVersion: apiextensions.crossplane.io/v1\nkind: CompositeResourceDefinition\nmetadata:\nname: #@ data.values.xrd.names.plural + \".\" + data.values.xrd.group\nspec:\ngroup: #@ data.values.xrd.group\nnames: #@ data.values.xrd.names\nclaimNames: #@ data.values.xrd.claimNames\n</code></pre> <p>Composition (Helm) template snippet</p> <p>The example below is the top section of the Helm Composition.</p> <p>It implements the API defined by the XRD. Via the reuse of the same data values, we guarantee it is always correct. And due to the reuse, we specify these fundamental values only once.</p> <pre><code>#@ load(\"@ytt:data\", \"data\")\n---\napiVersion: apiextensions.crossplane.io/v1\nkind: Composition\nmetadata:\nname: #@ data.values.providers.helm.name + \"-\" + data.values.cloudServiceBindingType\nlabels:\ncrossplane.io/xrd: #@ data.values.xrd.names.plural + \".\" + data.values.xrd.group\nprovider: #@ data.values.providers.helm.name\ndatabase: #@ data.values.cloudServiceBindingType\nspec:\nwriteConnectionSecretsToNamespace: #@ data.values.crossplane.namespace\ncompositeTypeRef:\napiVersion: #@ data.values.xrd.group + \"/\" + data.values.xrd.version\nkind: #@ data.values.xrd.names.kind\n</code></pre> <p>Now that you've seen how we reuse the data values, it is good to see where we define the structure and default values.</p> <p>We do so in a Values Schema.</p>"},{"location":"usecases/multicloud/packages/psql/create/#schema","title":"Schema","text":"<p>In this Values Schema, we define the structure of the data values used by the YTT templating.</p> <p>We need to specify the keys; the values are optional. There are two reasons to specify the values:</p> <ol> <li>to clarify the data type, although for Strings, <code>\"\"</code> will do</li> <li>to set a default value</li> </ol> <p>values-schema.ytt.yml</p> <p>We limited the values for brevity, though this is enough to show what you can do with it.</p> <p>First, we have to let YTT know this is a values schema. We do so with <code>#@data/values-schema</code>.</p> <p>Then we provide YAML keys in our desired structure.</p> <p>In our case, we only have String values, and because these are only used to generate the Crossplane source files, we only need a little documentation.  For all the things you can do with the schema, read the docs.</p> <pre><code>#@data/values-schema\n---\nxrd:\ngroup: multi.ref.services.apps.tanzu.vmware.com\nnames:\nkind: XPostgreSQLInstance\nplural: xpostgresqlinstances\nclaimNames:\nkind: PostgreSQLInstance\nplural: postgresqlinstances\nversion: v1alpha1\n\nproviders:\nhelm:\nname: helm\nimage: xpkg.upbound.io/crossplane-contrib/provider-helm\nversion: \"&gt;=v0.12.0\"\n\ncrossplane:\n#@schema/title \"CrossplaneNamespace\"\n#@schema/desc \"The namespace where crossplane controller is installed\"\nnamespace: upbound-system\nversion: '^v1.10'\n\n#@schema/title \"StoreConfig\"\n#@schema/desc \"Details of the StoreConfig\"\nstoreConfig:\n#@schema/title \"StoreConfig Name\"\n#@schema/desc \"The name of the StoreConfig\"\nname: \"default\"\n</code></pre>"},{"location":"usecases/multicloud/packages/psql/create/#solving-for-re-usable-snippets","title":"Solving For Re-usable Snippets","text":"<p>In the templating section, you can see how we reuse values from the schema to avoid duplication and misconfiguration.</p> <p>Even so, the number of shared data structures between the Compositions is significant. The solution for the ConnetionDetails, for example, is something each Composition requires.</p> <p>So there is a need for even more reuse. Not just the data values, but entire data structures, for example, the Composition Resources.</p> <p>For that purpose, we use the YTT features Load and Functions. It lets you create functions in separate files you import into other YTT files.</p> <p>YTT Library</p> <p>We could also have opted to use a YTT Library.</p> <p>Which does practically the same thing but requires more steps to use. So we opted for the solution below.</p> <p>To use this, we do the following:</p> <ol> <li>Create a file named <code>&lt;name&gt;.lib.yml</code></li> <li>Add Functions in this file that generate the Crossplane Composition Resource snippets</li> <li>Load the file and the desired functions in the YTT data file (a) file starting with <code>#@ load(\"@ytt:data\", \"data\")</code>)</li> <li>Use the functions as if they are defined in this YTT data file</li> </ol> <p>Let's look at some examples to clarify what we did. First, it's a function in a library module.</p> <p>shared.lib.yml</p> <p>For clarity, we removed some lines and highlighted the noteworthy lines.</p> <p>We start a function with <code>#@ def &lt;name&gt;(&lt;input parameters&gt;)</code>.</p> <p>We end a function with <code>#@ end</code>.</p> <p>We can use any defined input parameters directly by name. We determine the value by the order in which the caller supplies them.</p> <p>In this case, we have one parameter, <code>infra</code>. This lets us create a dynamic label on the secret based on the type of infrastructure the Composition implements (e.g., <code>azure</code>, <code>aws</code>, <code>kubernetes</code>).</p> shared.lib.yml<pre><code>#@ def labelsForSecret(infra):\nname: connectionSecret\nbase:\napiVersion: kubernetes.crossplane.io/v1alpha1\nkind: Object\nspec:\nforProvider:\nmanifest:\napiVersion: v1\nkind: Secret\nspec: {}\nmetadata:\nlabels:\nservices.apps.tanzu.vmware.com/class: multicloud-psql\nservices.apps.tanzu.vmware.com/infra: #@ infra\npatches:\n...\n#@ end\n</code></pre> <p>We use the library as follows:</p> <ol> <li>We import it using the <code>#@ load()</code> feature. Using the relative path of the file and the functions we want to use.     <pre><code>#@ load(\"shared.lib.yml\", \"labelsForSecret\")\n</code></pre></li> <li>Then, we call the function where we want its output to be.     <pre><code>resources:\n- #@ labelsForSecret(\"aws\")\n</code></pre></li> </ol> <p>Important</p> <p>By default, a function returns the data structure that it defines.</p> <p>If you want to return a single value, you can do so via the <code>return</code> statement like this:</p> <pre><code>#@ def TLSSecretName(domain):\n#@ return str(domain).replace(\".\", \"-\") + \"-tls\"\n#@ end\n</code></pre> <p>The functions can use everything YTT has to offer.</p> <p>This makes it an excellent place to handle specific logic for Crossplane Compositions. For example, in the case of the AWS RDS instance, we have private and public variants.</p> <p>This means that while most values for the RDS instance definition are the same, some change if it needs to be public.</p> <p>AWS Composition Public/Private Switch</p> <p>When we make the RDS instance publicly available, we need to set <code>spec.forProvider.publiclyAccessible</code> to true.</p> <p>We also need to tell Crossplan which SubnetGroup it has to use. When we make the RDS instance public, we create the SubnetGroup ourselves and let Crossplane manage the reference:</p> <p><pre><code>#@ if/end publiclyAccessible: \ndbSubnetGroupNameSelector:\nmatchControllerRef: true\n</code></pre> ps. <code>#@ if/end</code> means it does an if/else/end expression for a single line only</p> <p>If private, we expect you to supply the name of an existing one. We can use the <code>not</code> keyword to reverse the <code>if/end</code>, so we end up with two variations of the same snippet.</p> <pre><code>#@ if/end not publiclyAccessible: \n- type: FromCompositeFieldPath\nfromFieldPath: spec.parameters.aws.dbSubnetGroupName\ntoFieldPath: spec.forProvider.dbSubnetGroupName\n</code></pre> aws-composition.lib.yml<pre><code>#@ def rdsInstance(crossplaneNamespace, providerConfigRef, publiclyAccessible):\nname: rdsinstance\nbase:\napiVersion: rds.aws.upbound.io/v1beta1\nkind: Instance\nspec:\nforProvider:\nengine: postgres\ninstanceClass: db.t3.micro\npasswordSecretRef: key: password\nnamespace: #@ crossplaneNamespace\npubliclyAccessible: #@ publiclyAccessible\nskipFinalSnapshot: true\nstorageEncrypted: false\nallocatedStorage: 10\nvpcSecurityGroupIdSelector:\nmatchControllerRef: true\n#@ if/end publiclyAccessible: \ndbSubnetGroupNameSelector:\nmatchControllerRef: true\nproviderConfigRef:\nname: #@ providerConfigRef\npatches:\n#@ if/end not publiclyAccessible: \n- type: FromCompositeFieldPath\nfromFieldPath: spec.parameters.aws.dbSubnetGroupName\ntoFieldPath: spec.forProvider.dbSubnetGroupName\n...\n#@ end\n</code></pre> <p>Let's look at the two AWS examples (private and public) to see how this works out for the Composition files.</p>"},{"location":"usecases/multicloud/packages/psql/create/#aws-private-example","title":"AWS Private Example","text":"aws-composition-private.ytt.yml<pre><code>#@ load(\"@ytt:data\", \"data\")\n---\n#@ load(\"aws-composition.lib.yml\", \"rdsInstance\", \"securityGroup\", \"securityGroupRule\")\n#@ load(\"shared.lib.yml\", \"labelsForSecret\", \"tfProviderConfig\", \"tfWorkspace\")\n\napiVersion: apiextensions.crossplane.io/v1\nkind: Composition\nmetadata:\nname: #@ data.values.providers.aws.name + \"-\" + data.values.cloudServiceBindingType + \"-private\"\nlabels:\ncrossplane.io/xrd: #@ data.values.xrd.names.plural + \".\" + data.values.xrd.group\nprovider: #@ data.values.providers.aws.name\ndatabase: #@ data.values.cloudServiceBindingType\nconnectivity: \"private\"\nspec:\nwriteConnectionSecretsToNamespace: #@ data.values.crossplane.namespace\ncompositeTypeRef:\napiVersion: #@ data.values.xrd.group + \"/\" + data.values.xrd.version\nkind: #@ data.values.xrd.names.kind\nresources:\n- #@ labelsForSecret(\"aws\")\n- #@ tfProviderConfig(data.values.crossplane.namespace)\n- #@ tfWorkspace(data.values.crossplane.namespace)\n- #@ rdsInstance(data.values.crossplane.namespace, data.values.providers.aws.configRef, False)\n- #@ securityGroup()\n- #@ securityGroupRule()\n</code></pre>"},{"location":"usecases/multicloud/packages/psql/create/#aws-public-example","title":"AWS Public Example","text":"<p>Convenience Variables</p> <p>YTT supports the use of convenience variables.</p> <p>Below you can see we make ample use of them to make the subnet parameters clearer to understand and separate.</p> aws-composition-public.ytt.yml<pre><code>#@ load(\"@ytt:data\", \"data\")\n---\n#@ load(\"aws-composition.lib.yml\",  \"rdsInstance\", \"securityGroup\", \"securityGroupRule\", \"subnet\", \"routeTableAssociation\", \"routeTable\", \"route\", \"subnetGroup\")\n#@ load(\"shared.lib.yml\", \"labelsForSecret\", \"tfProviderConfig\", \"tfWorkspace\")\n\n#@ subnetASuffix = '-a'\n#@ subnetAFormat = 'subnet-a-%s'\n#@ availabilityZoneAFormat = '%sa'\n#@ cidrBlockFieldA = 'spec.parameters.aws.public.subnetACidrBlock'\n\n#@ subnetBSuffix = '-b'\n#@ subnetBFormat = 'subnet-b-%s'\n#@ availabilityZoneBFormat = '%sb'\n#@ cidrBlockFieldB = 'spec.parameters.aws.public.subnetBCidrBlock'\n\napiVersion: apiextensions.crossplane.io/v1\nkind: Composition\nmetadata:\nname: #@ data.values.providers.aws.name + \"-\" + data.values.cloudServiceBindingType + \"-public\"\nlabels:\ncrossplane.io/xrd: #@ data.values.xrd.names.plural + \".\" + data.values.xrd.group\nprovider: #@ data.values.providers.aws.name \ndatabase: #@ data.values.cloudServiceBindingType\nconnectivity: \"public\"\nspec:\nwriteConnectionSecretsToNamespace: #@ data.values.crossplane.namespace\ncompositeTypeRef:\napiVersion: #@ data.values.xrd.group + \"/\" + data.values.xrd.version\nkind: #@ data.values.xrd.names.kind\nresources:\n- #@ labelsForSecret(\"aws\")\n- #@ tfProviderConfig(data.values.crossplane.namespace)\n- #@ tfWorkspace(data.values.crossplane.namespace)\n- #@ rdsInstance(data.values.crossplane.namespace, data.values.providers.aws.configRef, True)\n- #@ securityGroup()\n- #@ securityGroupRule()\n- #@ routeTable()\n- #@ subnetGroup()\n- #@ subnet(subnetASuffix, subnetAFormat, availabilityZoneAFormat, cidrBlockFieldA)\n- #@ routeTableAssociation(subnetASuffix, subnetAFormat)\n- #@ subnet(subnetBSuffix, subnetBFormat, availabilityZoneBFormat, cidrBlockFieldB)\n- #@ routeTableAssociation(subnetBSuffix, subnetBFormat)\n- #@ route('route-%s')\n</code></pre>"}]}